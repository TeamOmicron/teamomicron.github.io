{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BBC Robotics - Team Omicron Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys\u2019 College in Queensland, Australia, who would have been competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot\u2019s hardware, electrical and software design. Please feel free to browse around, and don\u2019t hesitate to contact us by visiting the About page. We\u2019re more than happy to answer any questions you may have about any aspect of our robots! Due to the COVID-19 global pandemic, our team sadly never got a chance to compete in competition. Instead, we have released all of our source code for every project, hardware designs (mechanical and electrical), documentation and notes under permissive open source licences! Check it out here: Open source release","title":"Home"},{"location":"#bbc-robotics-team-omicron","text":"Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys\u2019 College in Queensland, Australia, who would have been competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot\u2019s hardware, electrical and software design. Please feel free to browse around, and don\u2019t hesitate to contact us by visiting the About page. We\u2019re more than happy to answer any questions you may have about any aspect of our robots! Due to the COVID-19 global pandemic, our team sadly never got a chance to compete in competition. Instead, we have released all of our source code for every project, hardware designs (mechanical and electrical), documentation and notes under permissive open source licences! Check it out here: Open source release","title":"BBC Robotics - Team Omicron"},{"location":"about/","text":"About us About Team Omicron Team Omicron is a robotics team from Brisbane Boys\u2019 College, competing in RoboCup Jr Open Soccer. We are from Brisbane, Queensland, Australia. Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical & electrical deign, strategy code, docs Omicron ethanlo2010@gmail.com James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron matt.young.1@outlook.com Our members are experienced, having competed in 2-3 previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. COVID-19 Our competition was scheduled for the 2020 RoboCup Internationals in Bordeaux, France. Unfortunately, due to the COVID-19 world pandemic, our competition has been cancelled. In addition, our current knowledge indicates that the 2021 competition (the time that the 2020 one was postponed to) will either be cancelled, or we would not be able to attend for other reasons. Thus, we have decided to release absolutely everything we have produced: code, designs, PCBs, documentation, as open source to give back to the community. See this section here for more information. After we were made aware of COVID, and consequently the 2020 Internationals were \u201cpostponed\u201d, development on the robot was significantly slowed down. This means that some announced features may not be fully complete or bug-free. However, we believe our product is still of value to the larger RoboCup community. Robot components This section contains the overview of the components on our robots. Microcontrollers: - Main: ESP32 DEVKIT-C - Secondary: Teensy 3.5, ATMega328P SBC: - LattePanda Delta 432 Camera module: - e-con Systems Hyperyon Motors: - 4x Maxon DCX19 New features & refinements This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : Our advanced vision and localisation application, capable of 70 FPS ball, goal and line detection at 1280x720 resolution. It\u2019s also capable of 1.5cm accurate localisation by using a using a novel hybrid sensor-fusion/non-linear optimisation algorithm. Omicontrol : Our custom, WiFi/Ethernet robot control software. Written in Kotlin, this program is used to tune Omicam, move robots around and visualise sensor data. Mouse sensor and velocity control : Our robots use PixArt PWM3360 mouse sensor, which we use to accurately estimate and control our robot\u2019s velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : Using the above projects, our movement software developers have developed interesting and complex ball-manipulation strategies that we use to a competitive advantage on the field such as flick-shots and \u201cline running\u201d. Double dribbler and kicker: (TODO hardware team write info) Wheels(?) Some technologies we build upon this year include: FSM : In 2019, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \u201cswitched\u201d through. This year, we continue to build upon this technology, introducing more states to our FSM. Protocol Buffers: In 2019, we unveiled Protocol Buffers as a an easy and intuitive way of passing complex data between devices. This year, we continue to use the Google-developed technology, expanding upon its usage scope significantly. PCBs (modular?) Wheels","title":"About"},{"location":"about/#about-us","text":"","title":"About us"},{"location":"about/#about-team-omicron","text":"Team Omicron is a robotics team from Brisbane Boys\u2019 College, competing in RoboCup Jr Open Soccer. We are from Brisbane, Queensland, Australia. Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical & electrical deign, strategy code, docs Omicron ethanlo2010@gmail.com James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron matt.young.1@outlook.com Our members are experienced, having competed in 2-3 previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions.","title":"About Team Omicron"},{"location":"about/#covid-19","text":"Our competition was scheduled for the 2020 RoboCup Internationals in Bordeaux, France. Unfortunately, due to the COVID-19 world pandemic, our competition has been cancelled. In addition, our current knowledge indicates that the 2021 competition (the time that the 2020 one was postponed to) will either be cancelled, or we would not be able to attend for other reasons. Thus, we have decided to release absolutely everything we have produced: code, designs, PCBs, documentation, as open source to give back to the community. See this section here for more information. After we were made aware of COVID, and consequently the 2020 Internationals were \u201cpostponed\u201d, development on the robot was significantly slowed down. This means that some announced features may not be fully complete or bug-free. However, we believe our product is still of value to the larger RoboCup community.","title":"COVID-19"},{"location":"about/#robot-components","text":"This section contains the overview of the components on our robots. Microcontrollers: - Main: ESP32 DEVKIT-C - Secondary: Teensy 3.5, ATMega328P SBC: - LattePanda Delta 432 Camera module: - e-con Systems Hyperyon Motors: - 4x Maxon DCX19","title":"Robot components"},{"location":"about/#new-features-refinements","text":"This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : Our advanced vision and localisation application, capable of 70 FPS ball, goal and line detection at 1280x720 resolution. It\u2019s also capable of 1.5cm accurate localisation by using a using a novel hybrid sensor-fusion/non-linear optimisation algorithm. Omicontrol : Our custom, WiFi/Ethernet robot control software. Written in Kotlin, this program is used to tune Omicam, move robots around and visualise sensor data. Mouse sensor and velocity control : Our robots use PixArt PWM3360 mouse sensor, which we use to accurately estimate and control our robot\u2019s velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : Using the above projects, our movement software developers have developed interesting and complex ball-manipulation strategies that we use to a competitive advantage on the field such as flick-shots and \u201cline running\u201d. Double dribbler and kicker: (TODO hardware team write info) Wheels(?) Some technologies we build upon this year include: FSM : In 2019, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \u201cswitched\u201d through. This year, we continue to build upon this technology, introducing more states to our FSM. Protocol Buffers: In 2019, we unveiled Protocol Buffers as a an easy and intuitive way of passing complex data between devices. This year, we continue to use the Google-developed technology, expanding upon its usage scope significantly. PCBs (modular?) Wheels","title":"New features &amp; refinements"},{"location":"communication/","text":"Communication Page author: Matt Young, vision systems & low-level developer Inter-robot communication is an extremely important aspect of RoboCup Jr. Using wireless protocols such as Zigbee or Bluetooth allows robots to exchange game strategy information with each other, switch FSM states and much more. This year, building on our codebase from last year, we have Bluetooth Classic communication between the two robots using the ESP32\u2019s built in wireless module and it\u2019s implementation in the IDF API. Likewise, intra-robot communication is an essential aspect of a multi-micro-controller setup like the one we have. This year, we extend our previous Protocol Buffer, UART-backed communication protocol to include a CRC8 checksum and better error handling to improve reliability. Note: Some of this documentation is borrowed from the RoboCup 2019 Sydney submission by our previous team (Deus Vult). Bluetooth communication Our Bluetooth inter-robot comms remain largely unchanged from last year, minus a few bug fixes and more testing. Background Bluetooth is a low-power wireless communications protocol used in many devices. Our ESP32s support both Bluetooth Classic v4 Enhanced Data Rate (EDR) and Bluetooth Low Energy (LE). We opted to use the BT Classic as its simpler and has faster speeds. On the ESP32, Bluetooth, like most other things, is highly parallel and runs in separate threads on its own core (core 0). This means that our code, which runs on core 1, can run totally asynchronously to Bluetooth. Implementation in Omicron Bluetooth is an extremely complicated protocol with many layers and quirks, but in essence, we use a Simple Port Profile (SPP) to connect to the other robot using Simple Secure Pairing (SSP). We have two robots, Robot 0 and Robot 1. Robot 0 is the \u201chost\u201d, who establishes an SPP server. Robot 1 is the \u201cclient\u201d, who discovers Robot 0 and connects to it. Our Bluetooth aims to be very reliable, and on the event of a disconnect, the client will continuously attempt to reconnect to the host. Similarly, the host will always be listening for a new client on disconnect. If too many Protobuf decode errors occur, the server will disconnect the client. We also use whitelisting to make sure no foreign devices can interrupt our Bluetooth connectivity by connecting during a game. Our Bluetooth communication uses Protocol Buffers. Each robot runs a receive and send task. The receive tasks reads Protobuf byte streams and decodes them, while send task encodes Protobuf byte streams and sends them. The default Bluetooth timeout is about 3 seconds, which is too slow for our purposes. Instead, we programmed a custom timer for 800ms where if a packet is not received within this time, the other robot is considered off for damage. When this is detected, the other robot destroys the Bluetooth connection and enters into defence. We also have some complex conflict resolution code. Both robots send each other\u2019s FSM states, and if both are in attack or both are in defence, an algorithm is run to resolve the conflict, which works by having the robot closest to the goal become the defender, as well as some code to deal with special circumstances such as if a robot can\u2019t see the goal. One of the most important parts of Bluetooth is switching. Inside the FSM, each robot will set a boolean true/false if they themselves are willing to switch. The attacker is willing to switch almost all of the time, except when its doing something important like shooting or dribbling to goal. The defender will only switch in much more specific circumstances like if it\u2019s surging (the ball is in front of it and its close to the goal). One robot, robot 0, is the designated \u201cswitch observer\u201d. It will observe if it\u2019s willing to switch and the other robot is willing to switch, and if so, broadcast a special message over Bluetooth that causes each robot to invert its state. To prevent fast-paced switching that\u2019s detrimental to gameplay, a switch timer is also used which means the robot is only permitted to switch every 1.5 seconds. UART communication Last year, we improved our communication infrastructure significantly by switching from I2C to UART and using Protocol Buffers instead of an arbitrary, bit-shifted binary format. For our purposes, we found I2C to be too error-prone and complex, with small payload sizes. Instead, through testing we observed that UART on 115200 baud rate was simpler to setup and use, more reliable and fast enough for our situation. This year, we continue our tradition of using Protocol Buffers for everything imaginable in our comms protocol, but also extend our communication protocol (nicknamed JimBus after a member on our team) to improve reliability. A standard JimBus message sent over UART has the following structure: Byte 0x0B to indicate message start Message ID Message length Protobuf data (message contents) CRC8 checksum of protobuf data Essentially, our messages have a 3 byte header and a 1 byte checksum. JimBus, our custom UART comms protocol, runs on the ESP32, the Teensy 3.5 and even the LattePanda using POSIX termios, so it\u2019s highly cross-platform and easy to decode. We introduced a CRC8 checksum this year to ensure that, when a message is corrupted due to electrical noise, the Protobuf decoding doesn\u2019t spectacuarly fail. Instead, if the received CRC8 checksum doesn\u2019t match the received checksum (calculated before sending by the other device), we reject the message, log an error and attempt to resync the UART stream. In future, we would like to add an extra byte to the message size field so we can transmit messages longer than 255 bytes, and also use a CRC32 checksum instead of CRC8 for better accuracy.","title":"Communication"},{"location":"communication/#communication","text":"Page author: Matt Young, vision systems & low-level developer Inter-robot communication is an extremely important aspect of RoboCup Jr. Using wireless protocols such as Zigbee or Bluetooth allows robots to exchange game strategy information with each other, switch FSM states and much more. This year, building on our codebase from last year, we have Bluetooth Classic communication between the two robots using the ESP32\u2019s built in wireless module and it\u2019s implementation in the IDF API. Likewise, intra-robot communication is an essential aspect of a multi-micro-controller setup like the one we have. This year, we extend our previous Protocol Buffer, UART-backed communication protocol to include a CRC8 checksum and better error handling to improve reliability. Note: Some of this documentation is borrowed from the RoboCup 2019 Sydney submission by our previous team (Deus Vult).","title":"Communication"},{"location":"communication/#bluetooth-communication","text":"Our Bluetooth inter-robot comms remain largely unchanged from last year, minus a few bug fixes and more testing.","title":"Bluetooth communication"},{"location":"communication/#background","text":"Bluetooth is a low-power wireless communications protocol used in many devices. Our ESP32s support both Bluetooth Classic v4 Enhanced Data Rate (EDR) and Bluetooth Low Energy (LE). We opted to use the BT Classic as its simpler and has faster speeds. On the ESP32, Bluetooth, like most other things, is highly parallel and runs in separate threads on its own core (core 0). This means that our code, which runs on core 1, can run totally asynchronously to Bluetooth.","title":"Background"},{"location":"communication/#implementation-in-omicron","text":"Bluetooth is an extremely complicated protocol with many layers and quirks, but in essence, we use a Simple Port Profile (SPP) to connect to the other robot using Simple Secure Pairing (SSP). We have two robots, Robot 0 and Robot 1. Robot 0 is the \u201chost\u201d, who establishes an SPP server. Robot 1 is the \u201cclient\u201d, who discovers Robot 0 and connects to it. Our Bluetooth aims to be very reliable, and on the event of a disconnect, the client will continuously attempt to reconnect to the host. Similarly, the host will always be listening for a new client on disconnect. If too many Protobuf decode errors occur, the server will disconnect the client. We also use whitelisting to make sure no foreign devices can interrupt our Bluetooth connectivity by connecting during a game. Our Bluetooth communication uses Protocol Buffers. Each robot runs a receive and send task. The receive tasks reads Protobuf byte streams and decodes them, while send task encodes Protobuf byte streams and sends them. The default Bluetooth timeout is about 3 seconds, which is too slow for our purposes. Instead, we programmed a custom timer for 800ms where if a packet is not received within this time, the other robot is considered off for damage. When this is detected, the other robot destroys the Bluetooth connection and enters into defence. We also have some complex conflict resolution code. Both robots send each other\u2019s FSM states, and if both are in attack or both are in defence, an algorithm is run to resolve the conflict, which works by having the robot closest to the goal become the defender, as well as some code to deal with special circumstances such as if a robot can\u2019t see the goal. One of the most important parts of Bluetooth is switching. Inside the FSM, each robot will set a boolean true/false if they themselves are willing to switch. The attacker is willing to switch almost all of the time, except when its doing something important like shooting or dribbling to goal. The defender will only switch in much more specific circumstances like if it\u2019s surging (the ball is in front of it and its close to the goal). One robot, robot 0, is the designated \u201cswitch observer\u201d. It will observe if it\u2019s willing to switch and the other robot is willing to switch, and if so, broadcast a special message over Bluetooth that causes each robot to invert its state. To prevent fast-paced switching that\u2019s detrimental to gameplay, a switch timer is also used which means the robot is only permitted to switch every 1.5 seconds.","title":"Implementation in Omicron"},{"location":"communication/#uart-communication","text":"Last year, we improved our communication infrastructure significantly by switching from I2C to UART and using Protocol Buffers instead of an arbitrary, bit-shifted binary format. For our purposes, we found I2C to be too error-prone and complex, with small payload sizes. Instead, through testing we observed that UART on 115200 baud rate was simpler to setup and use, more reliable and fast enough for our situation. This year, we continue our tradition of using Protocol Buffers for everything imaginable in our comms protocol, but also extend our communication protocol (nicknamed JimBus after a member on our team) to improve reliability. A standard JimBus message sent over UART has the following structure: Byte 0x0B to indicate message start Message ID Message length Protobuf data (message contents) CRC8 checksum of protobuf data Essentially, our messages have a 3 byte header and a 1 byte checksum. JimBus, our custom UART comms protocol, runs on the ESP32, the Teensy 3.5 and even the LattePanda using POSIX termios, so it\u2019s highly cross-platform and easy to decode. We introduced a CRC8 checksum this year to ensure that, when a message is corrupted due to electrical noise, the Protobuf decoding doesn\u2019t spectacuarly fail. Instead, if the received CRC8 checksum doesn\u2019t match the received checksum (calculated before sending by the other device), we reject the message, log an error and attempt to resync the UART stream. In future, we would like to add an extra byte to the message size field so we can transmit messages longer than 255 bytes, and also use a CRC32 checksum instead of CRC8 for better accuracy.","title":"UART communication"},{"location":"electrical_design/","text":"Electrical Design Page author: Ethan Lo, mechanical & electrical design PCBs were used for our electronics to mitigate wiring, as well us enable us to use components that would otherwise be unusable. We use 4 custom-made PCBs, which we label as the \u201cMain\u201d, \u201cBase\u201d, \u201cKicker\u201d and \u201cDebug\u201d boards. TODO: WE NEED IMAGES Main Board The main board is the central board of our electrical design. It contains the power supply, master microcontroller, test points and connections to all the other external electronics. Of these, the most noteworthy development is the use of test points. These are regions of exposed copper designed for convenient probing during the debug stage. The Main Board contains: Power supply and safety circuitry Chip-on-Board ESP32 microcontroller Teensy 4.0 microcontroller Micro-USB port for uploading 2 position slide switching for choosing upload target BNO055 9-Axis IMU Laser Rangefinders Buzzers for audio debug LEDs for visual debug Various test points for easy probing Base Board At the bottom of our robot is the base board. It holds our light sensors, light gate outputs, mouse sensor, and motor controllers. To drive these a slave microcontroller (namely the ATMEGA328p) was soldered directly onto the board. This is to save money and vertical clearance, and to decrease the chance of a short occurring due to the board\u2019s proximity to the motors. Kicker Board On the kicker board is the necessary electronics to drive two solenoids. This is not new to RoboCup, however we have made major modifications to the tried and tested design. Instead of directly driving the power MOSFET, we use an optocoupler to isolate the sensitive signal lines from the high voltages running through the circuit in an effort to protect the microcontroller in the event of a failure. Furthermore, we have moved away from relays and instead use MOSFETS to switch the load. We found that relays wore out after many frequent uses, and therefore we decided to use a non-mechanical solution. Debug Board A problem we have previously faced is the lack of debug tools for our robot. Whilst debug messages are adequate for software design, when the robot is running it is often difficult to connect a computer to it. Hence, we developed a dedicated debug board that allows us to manually interface with our robot for debug purposes.","title":"Electrical design"},{"location":"electrical_design/#electrical-design","text":"Page author: Ethan Lo, mechanical & electrical design PCBs were used for our electronics to mitigate wiring, as well us enable us to use components that would otherwise be unusable. We use 4 custom-made PCBs, which we label as the \u201cMain\u201d, \u201cBase\u201d, \u201cKicker\u201d and \u201cDebug\u201d boards. TODO: WE NEED IMAGES","title":"Electrical Design"},{"location":"electrical_design/#main-board","text":"The main board is the central board of our electrical design. It contains the power supply, master microcontroller, test points and connections to all the other external electronics. Of these, the most noteworthy development is the use of test points. These are regions of exposed copper designed for convenient probing during the debug stage. The Main Board contains: Power supply and safety circuitry Chip-on-Board ESP32 microcontroller Teensy 4.0 microcontroller Micro-USB port for uploading 2 position slide switching for choosing upload target BNO055 9-Axis IMU Laser Rangefinders Buzzers for audio debug LEDs for visual debug Various test points for easy probing","title":"Main Board"},{"location":"electrical_design/#base-board","text":"At the bottom of our robot is the base board. It holds our light sensors, light gate outputs, mouse sensor, and motor controllers. To drive these a slave microcontroller (namely the ATMEGA328p) was soldered directly onto the board. This is to save money and vertical clearance, and to decrease the chance of a short occurring due to the board\u2019s proximity to the motors.","title":"Base Board"},{"location":"electrical_design/#kicker-board","text":"On the kicker board is the necessary electronics to drive two solenoids. This is not new to RoboCup, however we have made major modifications to the tried and tested design. Instead of directly driving the power MOSFET, we use an optocoupler to isolate the sensitive signal lines from the high voltages running through the circuit in an effort to protect the microcontroller in the event of a failure. Furthermore, we have moved away from relays and instead use MOSFETS to switch the load. We found that relays wore out after many frequent uses, and therefore we decided to use a non-mechanical solution.","title":"Kicker Board"},{"location":"electrical_design/#debug-board","text":"A problem we have previously faced is the lack of debug tools for our robot. Whilst debug messages are adequate for software design, when the robot is running it is often difficult to connect a computer to it. Hence, we developed a dedicated debug board that allows us to manually interface with our robot for debug purposes.","title":"Debug Board"},{"location":"fsm/","text":"Finite State Machine We inherit our Hierarchical Finite State Machine (HFSM), implemented by us in C on the ESP32, from last year. Note: Due to COVID and development delays, our FSM implementation remains unchanged from last year (with a few minor bug fixes). Hence, most of this documentation is borrowed from the RoboCup 2019 Sydney submission by our previous team (Deus Vult). Introduction In the simplest possible terms, a Finite State Machine is a model of computation whereby a system (in this case, the robot\u2019s software), can be in exactly one of finite states at any given time. FSMs are commonly used to manage AI in video games and to manage complex system software (for example, our ESP32 uses an FSM internally in its I2C stack). Each state is designed to handle a very specific set of criteria. Should a state not meet its very specific criteria, it will revert to a previous state or change states to a new state. In this way, each state function has only the minimal amount of code required to perform one specific action, leading to an efficient and clean codebase. Implementation Our FSM implementation is entirely our own custom implementation written in pure C11 on the ESP32. It stores its state history to a stack data structure, meaning that it\u2019s possible to revert back through states, just like you can undo and redo in certain programs. Each state is represented as a set of functions: state_enter, state_update and state_exit. Enter and exit are called respectively when the given state is entered into or exited from. Update is called each time the main task\u2019s loop is executed for the state the robot is in. Our FSM implementation can be considered to be \u201chierarchical\u201d, meaning that we have nested state machines. Our global state machine has three states: attack, defence and general. Each sub-state-machine has another series of states, listed below. Benefits The main benefit of the FSM is the improved debugging as a result of the various states. Since the robot must be in one state at all times, issues can be rapidly identified and pinpointed from the terminal output. This has greatly improved our efficiency with regards to solving problems and debugging. It also allows for greater flexibility when programming the robot. New actions can simply be added by adding a new state to the finite state machine. Furthermore, various additions such as timers can be added quickly and efficiently. Another benefit is the code cleanliness. Using the FSM we have found, avoids much of the dreaded \u201cspaghetti code\u201d a lot of teams have. Drawbacks The main drawback of an FSM is the possibility of rapid alternating between two states caused by logic errors or noise. At its worse, this can cause the robot get stuck swapping between two states and not make any progress. However, these instances are not only rare, but solvable, with tuning. Furthermore, the FSM itself makes catching and rectifying these errors easier. States Attacker states Idle: entered into when the robot cannot see the ball for 5 seconds. Manoeuvres into the centre of the field. Pursue: entered into when the robot needs to move quickly towards the ball when it is far away. Orbit: our exponential orbit, entered into when the robot is behind the ball and needs to be in front of it. Dribble: once the robot has the ball in its capture zone, this state is entered into. Accelerates the robot linearly towards the goal. Defender states Reverse: entered into when the goal is not visible. Reverses and aligns itself to the back wall with the LRF Idle: entered into when the ball is not visible. Manoeuvres in front of the goal. Defend: entered into when the robot is visible. Positions itself between the ball and the goal. Surge: entered into when the robot is in possession of the ball. Rushes forward to put distance between the ball and the goal. If the other robot is active, it will switch to attacker state. General states (shared) Shoot: entered into when the robot is has a clear shot of the goal. Activates solenoid kicker. Timers Timers are an essential part of our robots behaviour, and are particularly relevant for our finite state machine. We use FreeRTOS\u2019 highly efficient software timers, which use zero CPU time and add zero overhead to the tick unless they\u2019re activated. We extended FreeRTOS\u2019 timer system using a custom add-on layer that we developed, to assist in creating timers, stopping them and starting them if they\u2019re not already running. List of timers Bluetooth packet timer: if a BT packet is not received in this time, the other robot must be off for damage Cooldown timer: after a switch, this timer is started, and another switch is not permitted until it expires Idle timer: if the ball is not visible for this time, switch into idle state Dribble timer: if the ball is in the capture zone and close enough for this time, switch into dribble state Surge timer: same as the above for defender, but switch into surge state Shoot timer: if the ball is in the capture zone for long enough, kick","title":"Finite State Machine"},{"location":"fsm/#finite-state-machine","text":"We inherit our Hierarchical Finite State Machine (HFSM), implemented by us in C on the ESP32, from last year. Note: Due to COVID and development delays, our FSM implementation remains unchanged from last year (with a few minor bug fixes). Hence, most of this documentation is borrowed from the RoboCup 2019 Sydney submission by our previous team (Deus Vult).","title":"Finite State Machine"},{"location":"fsm/#introduction","text":"In the simplest possible terms, a Finite State Machine is a model of computation whereby a system (in this case, the robot\u2019s software), can be in exactly one of finite states at any given time. FSMs are commonly used to manage AI in video games and to manage complex system software (for example, our ESP32 uses an FSM internally in its I2C stack). Each state is designed to handle a very specific set of criteria. Should a state not meet its very specific criteria, it will revert to a previous state or change states to a new state. In this way, each state function has only the minimal amount of code required to perform one specific action, leading to an efficient and clean codebase.","title":"Introduction"},{"location":"fsm/#implementation","text":"Our FSM implementation is entirely our own custom implementation written in pure C11 on the ESP32. It stores its state history to a stack data structure, meaning that it\u2019s possible to revert back through states, just like you can undo and redo in certain programs. Each state is represented as a set of functions: state_enter, state_update and state_exit. Enter and exit are called respectively when the given state is entered into or exited from. Update is called each time the main task\u2019s loop is executed for the state the robot is in. Our FSM implementation can be considered to be \u201chierarchical\u201d, meaning that we have nested state machines. Our global state machine has three states: attack, defence and general. Each sub-state-machine has another series of states, listed below.","title":"Implementation"},{"location":"fsm/#benefits","text":"The main benefit of the FSM is the improved debugging as a result of the various states. Since the robot must be in one state at all times, issues can be rapidly identified and pinpointed from the terminal output. This has greatly improved our efficiency with regards to solving problems and debugging. It also allows for greater flexibility when programming the robot. New actions can simply be added by adding a new state to the finite state machine. Furthermore, various additions such as timers can be added quickly and efficiently. Another benefit is the code cleanliness. Using the FSM we have found, avoids much of the dreaded \u201cspaghetti code\u201d a lot of teams have.","title":"Benefits"},{"location":"fsm/#drawbacks","text":"The main drawback of an FSM is the possibility of rapid alternating between two states caused by logic errors or noise. At its worse, this can cause the robot get stuck swapping between two states and not make any progress. However, these instances are not only rare, but solvable, with tuning. Furthermore, the FSM itself makes catching and rectifying these errors easier.","title":"Drawbacks"},{"location":"fsm/#states","text":"","title":"States"},{"location":"fsm/#attacker-states","text":"Idle: entered into when the robot cannot see the ball for 5 seconds. Manoeuvres into the centre of the field. Pursue: entered into when the robot needs to move quickly towards the ball when it is far away. Orbit: our exponential orbit, entered into when the robot is behind the ball and needs to be in front of it. Dribble: once the robot has the ball in its capture zone, this state is entered into. Accelerates the robot linearly towards the goal.","title":"Attacker states"},{"location":"fsm/#defender-states","text":"Reverse: entered into when the goal is not visible. Reverses and aligns itself to the back wall with the LRF Idle: entered into when the ball is not visible. Manoeuvres in front of the goal. Defend: entered into when the robot is visible. Positions itself between the ball and the goal. Surge: entered into when the robot is in possession of the ball. Rushes forward to put distance between the ball and the goal. If the other robot is active, it will switch to attacker state.","title":"Defender states"},{"location":"fsm/#general-states-shared","text":"Shoot: entered into when the robot is has a clear shot of the goal. Activates solenoid kicker.","title":"General states (shared)"},{"location":"fsm/#timers","text":"Timers are an essential part of our robots behaviour, and are particularly relevant for our finite state machine. We use FreeRTOS\u2019 highly efficient software timers, which use zero CPU time and add zero overhead to the tick unless they\u2019re activated. We extended FreeRTOS\u2019 timer system using a custom add-on layer that we developed, to assist in creating timers, stopping them and starting them if they\u2019re not already running.","title":"Timers"},{"location":"fsm/#list-of-timers","text":"Bluetooth packet timer: if a BT packet is not received in this time, the other robot must be off for damage Cooldown timer: after a switch, this timer is started, and another switch is not permitted until it expires Idle timer: if the ball is not visible for this time, switch into idle state Dribble timer: if the ball is in the capture zone and close enough for this time, switch into dribble state Surge timer: same as the above for defender, but switch into surge state Shoot timer: if the ball is in the capture zone for long enough, kick","title":"List of timers"},{"location":"future_work/","text":"Future ideas Although this was the last year of our competition, we nonetheless have a few remaining ideas we would have liked to research: A method for automatic camera threshold calibration using the same Subplex algorithm as used in the localiser (model as 6 dimensional optimisation problem). Properly implement and test strategies Finish a few unfinished/untested Omicam and Omicontrol features Fix all remaining bugs Implement a better communication protocol with more reliability Predict the velocity of the ball and supply it to the goalie for interception Better inter-robot communication, transmit more information, test more","title":"Future ideas"},{"location":"future_work/#future-ideas","text":"Although this was the last year of our competition, we nonetheless have a few remaining ideas we would have liked to research: A method for automatic camera threshold calibration using the same Subplex algorithm as used in the localiser (model as 6 dimensional optimisation problem). Properly implement and test strategies Finish a few unfinished/untested Omicam and Omicontrol features Fix all remaining bugs Implement a better communication protocol with more reliability Predict the velocity of the ball and supply it to the goalie for interception Better inter-robot communication, transmit more information, test more","title":"Future ideas"},{"location":"lowlevel/","text":"Low-level code Page author: Matt Young, vision systems & low-level developer Team Omicron\u2019s code is written at a much lower level (we believe) than many other teams in the RoboCup Jr competition. For example, most of our gameplay code runs on the ESP32 and is written in plain C11 using Espressif\u2019s IoT Development Framework (as opposed to Arduino). At last year\u2019s competition, only one other team was using the ESP32 and they were using the Arduino runtime instead of the IDF. We believe that working at a lower level allows more flexibility, performance and knowledge to be gained. Note: Due to COVID and development delays, a lot of our low-level code is the same as last year (with a few minor bug fixes). Hence, a lot of this documentation is borrowed from the RoboCup 2019 Sydney submission by our previous team (Deus Vult). ESP32-IDF In our previous team, Deus Vult, we migrated from using a Teensy 3.5 as the main device to using an ESP32 last year. This was due to REASONS . This year we again use the ESP32 as our main device, working with last year\u2019s codebase. Espressif (ESP32 manufacturers) provides two options for software development: the Arduino core, which provides a familiar C++ environment with ported Arduino functions, and secondly the IoT Development Framework (IDF), which is a completely ESP32 specific platform created by Espressif. The IDF is the recommended option to program advanced devices in, and we consider our robot to be an advanced device. Espressif recommends programming projects in the IDF using C (rather than C++). Ethan and I (Matt) already knew C from last year\u2019s competition, and all of the ESP32 code was already mostly written after being ported from Arduino. That only left Lachlan to learn a little bit of C to write his orbit strategy. Like last year, we selected the IDF over the Arduino core due to its higher quality codebase, greater performance, the fact that Espressif provides more official support to the IDF, dissatisfaction with Arduino functions and most importantly: access to advanced features. We document some important ESP32-IDF features below. Core dumps When an ESP32 encounters an irrecoverable error (a crash), it can be configured to produce what\u2019s known as a core dump, and save it to flash memory. This core dump contains highly technical information that our software engineers can use to debug the root cause of the problem. Such information includes the name of the error, the memory locations where it happened, a register dump, as well as a backtrace containing all function calls to the problematic code. Due to the fact that this code is saved to flash memory, if the robot crashes on the field, we can perform a virtual autopsy of the ESP32, by retrieving and analysing its core dump. This has been invaluable in debugging many errors the robot has encountered. When an error happens on the robot, unlike most other microcontrollers, the ESP32 is actually capable of rebooting itself. This way, when our robot crashes, it can safely reboot and continue on with normal play, without us having to take it off for damage. Task watchdog timer A frequent problem many teams encounter is the robot stopping responding, also known as hanging. This can be caused by a multitude of reasons, but is often very challenging to debug effectively. As is covered more in the FreeRTOS section, our robot\u2019s code runs in parallel tasks, meaning hanging is less of an issue: however, these tasks themselves can still stop responding without bringing down the whole robot, which is still an issue! Using the IDF, we can make these issues much easier to resolve using a feature known as the Task Watchdog Timer (TWDT). When a task stops responding for about 5 seconds, the TWDT will print error messages containing the name of the task which crashed and can optionally reboot the robot. Logging library One of the most critical part of debugging is understanding what your robot is doing, and why it\u2019s doing it. Traditionally, this has been a difficult challenge due the lack of an official logging library for Teensies (and Arduino). However, the IDF includes a built-in high performance logging library with support for coloured output and tags. This makes determining what the robot was doing (in our case, what state it\u2019s in and why it\u2019s switching states) a trivial task. The ESP\u2019s logging library has solved countless bugs in our robot and we\u2019re thankful that Espressif created it! An issue sometimes happens on Windows based computers, where the computer can\u2019t handle continuous outputs of logging, causing us to end in a back-buffer of old data. To work around this, one of our programmers created a log once function, which calculates the unique Jenkins hash of a string and stores it into a database, so that the same message won\u2019t be logged twice, leading to cleaner outputs. FreeRTOS Building on last year\u2019s work, we once again use FreeRTOS as the real-time operating system of the ESP32. FreeRTOS is an industry standard real time operating system kernel (RTOS). It\u2019s open source, released by the developers under the permissive MIT license. It has been created over a 15 year period and is widely renowned by millions of users as being stable, safe and secure. Whilst teams using an Arduino or a Teensy will be stuck with a cyclic-executive runtime in which only one action can be run at once, we can run multiple tasks at the same time, giving us performance and design benefits. An RTOS is an operating system that is optimised for use in embedded/real time applications, such as our robot. The main component of FreeRTOS is the task scheduler, which has the responsibility of deciding which task at any given point in time should run on the CPU. It decides this by choosing the task with the highest priority, and when two tasks have the same priority, it uses an algorithm known as round-robin scheduling where each task is given equal CPU time. Changing which task runs on the CPU happens through a process known as context switching. Essentially, it involves saving and later restoring the entire execution state of the microcontroller, such as the location of the instruction pointer and values of all registers. FreeRTOS\u2019s scheduler is preemptive, meaning that tasks are not told when a switch will occur and the kernel is entitled to context switch at any state of execution. The ESP32-IDF heavily revolves around FreeRTOS, and most drivers are written using highly efficient, parallel thread-aware code. This is further aided by the fact that the ESP32 is dual core. As an example of this, we run our Bluetooth stack on core 0, which can be running at the same time as core 1 processes Protocol Buffer data and moves the motors. FreeRTOS has a tick rate, meaning not how fast the processor itself runs, but instead how often the RTOS updates or ticks. We have FreeRTOS running at 128 Hz (overclocked from the default 100 Hz). Our ESP32 itself runs at 240 MHz, the fastest speed possible. This is an ideal clock rate for FreeRTOS as clocking it too high can actually lead to performance degradation due to most CPU time being spent in the RTOS rather than user code. How it\u2019s used As mentioned previously, FreeRTOS works by running multiple tasks in parallel on both the ESP32\u2019s cores. Most of the IDF\u2019s drivers, such as the entire Bluetooth stack, run on core 0 (the \u201cprotocol core\u201d). All of our tasks, by default, run on core 1 (the \u201capplication core\u201d), meaning Bluetooth can run with absolutely zero overhead to the rest of our robot. In ESP32 (technically the Xtensa) port of FreeRTOS, a task can float between cores, meaning it will run on whichever core is least busy, or it can be pinned to core meaning it is locked down to that one. However, upon accessing the floating point unit (FPU) embedded into the ESP32 to perform fast decimal calculations, due to some complicated hardware specifics that are out of the scope of our documentation, a task must be pinned to the core it started on. This means in practice most of our tasks run on core 1, and most of the IDF\u2019s tasks run on core 0. List of RTOS tasks: MasterTask: main task which runs when the ESP32 is in master mode. Initialises and receives data from other tasks, updates FSM and motors. CameraReceiveTask: receives data and calculates goal distance and angles from the camera over UART. I2CReceiveTask: receives data from the slave over I2C and decodes Protocol Buffers BTLogicTask: Bluetooth logic task, created when a Bluetooth connection is established. Manages sending and receiving Bluetooth data, switching and other logic associated with that ar Benefits Increased performance and efficiency due to asynchronous tasks and utilising dual-core architecture Increased debugging resources as FreeRTOS is widely used in the real world Tighter integration with ESP32-IDF as it also uses FreeRTOS multi-tasking Drawbacks Multi-tasking brings with it lots of complexities including notoriously hard to debug race conditions, deadlocks, etc. Less lightweight than cyclic-executive runtime, meaning some elements (e.g. IMU reading) may be slower leading to IMU drift","title":"Low-level code"},{"location":"lowlevel/#low-level-code","text":"Page author: Matt Young, vision systems & low-level developer Team Omicron\u2019s code is written at a much lower level (we believe) than many other teams in the RoboCup Jr competition. For example, most of our gameplay code runs on the ESP32 and is written in plain C11 using Espressif\u2019s IoT Development Framework (as opposed to Arduino). At last year\u2019s competition, only one other team was using the ESP32 and they were using the Arduino runtime instead of the IDF. We believe that working at a lower level allows more flexibility, performance and knowledge to be gained. Note: Due to COVID and development delays, a lot of our low-level code is the same as last year (with a few minor bug fixes). Hence, a lot of this documentation is borrowed from the RoboCup 2019 Sydney submission by our previous team (Deus Vult).","title":"Low-level code"},{"location":"lowlevel/#esp32-idf","text":"In our previous team, Deus Vult, we migrated from using a Teensy 3.5 as the main device to using an ESP32 last year. This was due to REASONS . This year we again use the ESP32 as our main device, working with last year\u2019s codebase. Espressif (ESP32 manufacturers) provides two options for software development: the Arduino core, which provides a familiar C++ environment with ported Arduino functions, and secondly the IoT Development Framework (IDF), which is a completely ESP32 specific platform created by Espressif. The IDF is the recommended option to program advanced devices in, and we consider our robot to be an advanced device. Espressif recommends programming projects in the IDF using C (rather than C++). Ethan and I (Matt) already knew C from last year\u2019s competition, and all of the ESP32 code was already mostly written after being ported from Arduino. That only left Lachlan to learn a little bit of C to write his orbit strategy. Like last year, we selected the IDF over the Arduino core due to its higher quality codebase, greater performance, the fact that Espressif provides more official support to the IDF, dissatisfaction with Arduino functions and most importantly: access to advanced features. We document some important ESP32-IDF features below.","title":"ESP32-IDF"},{"location":"lowlevel/#core-dumps","text":"When an ESP32 encounters an irrecoverable error (a crash), it can be configured to produce what\u2019s known as a core dump, and save it to flash memory. This core dump contains highly technical information that our software engineers can use to debug the root cause of the problem. Such information includes the name of the error, the memory locations where it happened, a register dump, as well as a backtrace containing all function calls to the problematic code. Due to the fact that this code is saved to flash memory, if the robot crashes on the field, we can perform a virtual autopsy of the ESP32, by retrieving and analysing its core dump. This has been invaluable in debugging many errors the robot has encountered. When an error happens on the robot, unlike most other microcontrollers, the ESP32 is actually capable of rebooting itself. This way, when our robot crashes, it can safely reboot and continue on with normal play, without us having to take it off for damage.","title":"Core dumps"},{"location":"lowlevel/#task-watchdog-timer","text":"A frequent problem many teams encounter is the robot stopping responding, also known as hanging. This can be caused by a multitude of reasons, but is often very challenging to debug effectively. As is covered more in the FreeRTOS section, our robot\u2019s code runs in parallel tasks, meaning hanging is less of an issue: however, these tasks themselves can still stop responding without bringing down the whole robot, which is still an issue! Using the IDF, we can make these issues much easier to resolve using a feature known as the Task Watchdog Timer (TWDT). When a task stops responding for about 5 seconds, the TWDT will print error messages containing the name of the task which crashed and can optionally reboot the robot.","title":"Task watchdog timer"},{"location":"lowlevel/#logging-library","text":"One of the most critical part of debugging is understanding what your robot is doing, and why it\u2019s doing it. Traditionally, this has been a difficult challenge due the lack of an official logging library for Teensies (and Arduino). However, the IDF includes a built-in high performance logging library with support for coloured output and tags. This makes determining what the robot was doing (in our case, what state it\u2019s in and why it\u2019s switching states) a trivial task. The ESP\u2019s logging library has solved countless bugs in our robot and we\u2019re thankful that Espressif created it! An issue sometimes happens on Windows based computers, where the computer can\u2019t handle continuous outputs of logging, causing us to end in a back-buffer of old data. To work around this, one of our programmers created a log once function, which calculates the unique Jenkins hash of a string and stores it into a database, so that the same message won\u2019t be logged twice, leading to cleaner outputs.","title":"Logging library"},{"location":"lowlevel/#freertos","text":"Building on last year\u2019s work, we once again use FreeRTOS as the real-time operating system of the ESP32. FreeRTOS is an industry standard real time operating system kernel (RTOS). It\u2019s open source, released by the developers under the permissive MIT license. It has been created over a 15 year period and is widely renowned by millions of users as being stable, safe and secure. Whilst teams using an Arduino or a Teensy will be stuck with a cyclic-executive runtime in which only one action can be run at once, we can run multiple tasks at the same time, giving us performance and design benefits. An RTOS is an operating system that is optimised for use in embedded/real time applications, such as our robot. The main component of FreeRTOS is the task scheduler, which has the responsibility of deciding which task at any given point in time should run on the CPU. It decides this by choosing the task with the highest priority, and when two tasks have the same priority, it uses an algorithm known as round-robin scheduling where each task is given equal CPU time. Changing which task runs on the CPU happens through a process known as context switching. Essentially, it involves saving and later restoring the entire execution state of the microcontroller, such as the location of the instruction pointer and values of all registers. FreeRTOS\u2019s scheduler is preemptive, meaning that tasks are not told when a switch will occur and the kernel is entitled to context switch at any state of execution. The ESP32-IDF heavily revolves around FreeRTOS, and most drivers are written using highly efficient, parallel thread-aware code. This is further aided by the fact that the ESP32 is dual core. As an example of this, we run our Bluetooth stack on core 0, which can be running at the same time as core 1 processes Protocol Buffer data and moves the motors. FreeRTOS has a tick rate, meaning not how fast the processor itself runs, but instead how often the RTOS updates or ticks. We have FreeRTOS running at 128 Hz (overclocked from the default 100 Hz). Our ESP32 itself runs at 240 MHz, the fastest speed possible. This is an ideal clock rate for FreeRTOS as clocking it too high can actually lead to performance degradation due to most CPU time being spent in the RTOS rather than user code.","title":"FreeRTOS"},{"location":"lowlevel/#how-its-used","text":"As mentioned previously, FreeRTOS works by running multiple tasks in parallel on both the ESP32\u2019s cores. Most of the IDF\u2019s drivers, such as the entire Bluetooth stack, run on core 0 (the \u201cprotocol core\u201d). All of our tasks, by default, run on core 1 (the \u201capplication core\u201d), meaning Bluetooth can run with absolutely zero overhead to the rest of our robot. In ESP32 (technically the Xtensa) port of FreeRTOS, a task can float between cores, meaning it will run on whichever core is least busy, or it can be pinned to core meaning it is locked down to that one. However, upon accessing the floating point unit (FPU) embedded into the ESP32 to perform fast decimal calculations, due to some complicated hardware specifics that are out of the scope of our documentation, a task must be pinned to the core it started on. This means in practice most of our tasks run on core 1, and most of the IDF\u2019s tasks run on core 0. List of RTOS tasks: MasterTask: main task which runs when the ESP32 is in master mode. Initialises and receives data from other tasks, updates FSM and motors. CameraReceiveTask: receives data and calculates goal distance and angles from the camera over UART. I2CReceiveTask: receives data from the slave over I2C and decodes Protocol Buffers BTLogicTask: Bluetooth logic task, created when a Bluetooth connection is established. Manages sending and receiving Bluetooth data, switching and other logic associated with that ar","title":"How it's used"},{"location":"lowlevel/#benefits","text":"Increased performance and efficiency due to asynchronous tasks and utilising dual-core architecture Increased debugging resources as FreeRTOS is widely used in the real world Tighter integration with ESP32-IDF as it also uses FreeRTOS multi-tasking","title":"Benefits"},{"location":"lowlevel/#drawbacks","text":"Multi-tasking brings with it lots of complexities including notoriously hard to debug race conditions, deadlocks, etc. Less lightweight than cyclic-executive runtime, meaning some elements (e.g. IMU reading) may be slower leading to IMU drift","title":"Drawbacks"},{"location":"omicam/","text":"Page author: Matt Young, vision systems & low-level developer One of the biggest innovation Team Omicron brings this year is our high-performance, custom vision and localisation application (i.e. camera software) called Omicam . Omicam handles the complex process of detecting the ball and goals, determining the robot\u2019s position on the field as well as encoding/transmitting this information, all in one highly optimised codebase. It uses a novel method involving non-linear optimisation to calculate the robot\u2019s position. The application is developed mostly in C, with some C++ code to interface with OpenCV. It runs on the LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS. We are proud to report that Omicam is a significant improvement compared to previous vision applications in use at BBC Robotics, as explained in the \u201cPerformance and results\u201d section. Background and previous methods Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer. With the introduction of the orange ball and advanced teams\u2019 application of \u201cball-hiding\u201d strategies, high resolution yet performant computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. Advanced teams now also need to accurately estimate their position on the field (localise) in order to execute strategies and gain the upper hand in the competition. Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the best solution was to develop a custom vision application running on a single board computer (SBC) . In terms of localisation, in the past we managed to get away with not using any, or using a low-fidelity approach based on detecting the goals in the image. With the introduction of strategies this year, however, team members needed more accurate position data, which requires more complex localisation. Performance and results Omicam is capable of detecting the ball, goals and lines at 60-70fps at 720p (1280x720) resolution. Compared to the previous OpenMV H7, this is 23x higher resolution at 3x the framerate. 1 In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the robot\u2019s position to around 1.5cm accuracy in less than 2ms. This is over 5x/25x more accurate 2 than any previous methods used at BBC Robotics, most likely much faster, and has been shown to be much more reliable and stable in all conditions. Using the e-con Systems Hyperyon camera based around the ultra low-light performance IMX290 sensor, Omicam is robust against lighting conditions ranging from near pitch darkness to direct LED light. 1: previous results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution. 2: depending on whether LRF based/goal based localisation was used. Hardware Omicam is capable of running on any x86 computer running Linux. In our case, we use a LattePanda Delta 432 SBC with a 2.4 GHz quad-core Intel Celeron N4100 and 4GB of RAM. All computer vision tasks run on the CPU, we do not use the GPU. The camera module we use is the e-con Systems Hyperon which uses the IMX290 ultra-low light image sensor. For more information, please see this page . Field object detection One of the responsibilities of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. This is accomplished by using the open-source computer vision library OpenCV. We use a Gstreamer pipeline to decode our camera\u2019s MJPEG stream at our target resolution of 1280x720 pixels, which is read in through an OpenCV VideoCapture object. Pre-processing With the camera frame in OpenCV memory, we then apply the following pre-processing steps to the image: Crop the frame to only leave a rectangle around the mirror visible Flip the image, if necessary, to account for the mounting of the camera on the robot Apply a circular mask to the image to mask out any pixels that aren\u2019t on the mirror Downscale the frame for use when processing the blue and yellow goals to process faster at the cost of accuracy Mask out the robot from the centre of the mirror (as it has reflective plastic on it which comes up as white) Thresholding and component labelling After pre-processing, we then use OpenCV\u2019s inRange inside a parallel_for to threshold all objects at once using multiple threads. This produces a 1-bit binary mask of the image, where each pixel is 255 (true) if it\u2019s inside the RGB range specified, and 0 (false) if it\u2019s not. For the lines, the vision processing step is finished here as we only need a binary mask. Unfortunately, the localisation process can\u2019t begin until the goals have finished processing due to the localiser relying on the goals in its initial estimate calculations (see below). Finally, we use OpenCV\u2019s Grana 1 parallel connected component labeller to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. OpenCV automatically calculates the bounding box and centroid for each of these connected regions. Coordinate transforms and dispatch Considering the centroid for each field object as a Cartesian vector in pixels coordinates, we convert this vector to polar form and run the magnitude through our mirror dewarping function (see below) to convert it to a length in centimetres. This leaves us with a polar vector for each field object relative to the robot\u2019s centre. We then convert back to Cartesian and use the last localiser position to infer the field object\u2019s absolute position in centimetres. Finally, for the goals we also calculate the relative Cartesian coordinates (convert polar to Cartesian but don\u2019t add localiser position) for use in the localiser\u2019s initial estimate calculations. This information is encoded into a Protobuf format with nanopb, and is sent over UART to the ESP32 using POSIX termios at 115200 baud. Unfortunately, the UART bus is owned by an ATMega 32U4, so a sketch on that device forwards it to the ESP32. TODO images and/or video of thresholded field TODO more detail on dewarp function here (maybe move stuff up from localisation section) Localisation Previous methods and background Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball. Currently, teams use three main methods of localisation. Firstly, the most common approach uses the detected goal blobs in the camera to triangulate the robot\u2019s position. This approach can be very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in SuperTeam). Using this approach on an OpenMV, we found accuracy of about 15 cm, but running on Omicam at 720p we found accuracy of around 4-6 cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using a few of these sensors on a robot, the position of the robot can be inferred with trigonometry by measuring the distance to the walls. The drawback of this approach is that it\u2019s impossible to reliably distinguish between interfering objects, such as a hand or another robot, compared to a wall. This means that although this approach can be accurate on an empty field, it is very difficult to use reliably in actual games. On an empty field, teams have found accuracies of 5-10 cm, but data is often invalid and can\u2019t be used reliably in real games. The third approach in use by some teams is one based on 360 degree LiDARS. This approach, being similar to the second approach, has similar accuracy and drawbacks. One additional drawback is the expensive cost and heavy weight of LiDARS. Another approach that can be used is summing displacement values from a mouse sensor, or integrating acceleration data from an IMU. While these approaches have high levels of accuracy initially, the repeated integration of the data gradually builds error over time, and if the robot is on for a particularly long time, the localised position may be several centimetres inaccurate by the end. In addition, these approaches all return relative displacement to the initial position, not an absolute position on the field - meaning a method for calculating the robot\u2019s initial position accurately is still required. Some teams, including us in the past, do not localise at all. Strictly speaking, with many gameplay behaviours, knowing the robot\u2019s position is not necessary. However, with our advanced strategies, localisation is a necessity. Our approach This year, Team Omicron presents a novel approach to robot localisation based on a hybrid sensor-fusion/non-linear optimisation algorithm. Our approach builds an initial estimate of the robot\u2019s position using faster, more inaccurate methods like goal triangulation and mouse sensor displacement. It then refines this estimate to a much higher accuracy by solving a 2D non-linear optimisation problem in realtime. The addition of the optimisation stage to the algorithm increases accuracy by about 4.6x, to be as little as 1.5cm error. Our optimisation algorithm works by comparing observed field line geometry from the camera (sampled via raycasting), to a known model of the field. By trying to optimise the robot\u2019s unknown (x,y) position such that it minimises the error between the observed lines and expected lines at each position, we can infer the robot\u2019s coordinates to a very high level of accuracy. The optimisation stage of our sensor fusion algorithm is based on a paper by Lauer, Lange and Redmiller (2006) 2 , as covered in the paper by Lu, Li, Hu and Zheng (2013) 3 . However, our approaches are different in certain areas, with us making some concessions and improvements to better suit the RoboCup Junior league (as the paper was written for middle-size league). Lauer et al\u2019s approach casts rays and marks points where these rays intersect the lines, whereas we work with these rays and their lengths directly. Their approach also calculates the orientation of the robot, while we trust the value of the IMU for orientation to simplify our algorithm. Their optimisation stage uses the RPROP algorithm to minimise error, while we use Subplex (based on the Nelder-Mead simplex algorithm). Finally, Lauer et al\u2019s approach has more complex global localisation and smoothing that can handle unstable localisations with multiple local minima, while our approach assumes only one minimum is present in the objective function. Our approach has the following 5 main steps: Estimate calculation Image analysis Camera normalisation Coordinate optimisation and interpolation (Optional) Robot detection Estimate calculation The localiser first constructs an initial estimate of the robot\u2019s position using faster but more inaccurate methods. The preferred method is to sum the displacement data from our PWM3360 mouse sensor to determine our current position since last localisation. However, in certain cases such as the initial localisation after the robot is powered on, we also use the blue and yellow goals to triangulate our position since there\u2019s no initial position for the mouse sensor to use. Once we have an initial localisation though, we can go back to using the mouse sensor data. Figure 1: method for triangulating position using goals coordinates and vectors Once the initial estimate is calculated, which is usually accurate to about 6-10cm, we constrain the optimiser\u2019s bounds to a certain sized rectangle around the centre of the estimated position. We also change the optimiser\u2019s initial estimate position to the estimated position, and scale down its step size significantly. We assume that the estimate is close to the actual position, so a smaller step size means that it won\u2019t overshoot the minimum and will hopefully converge faster. The combination of the estimate bounds with changing localiser settings means that we can often stably converge on the real position of the robot with around 40 objective function evaluations. If the localiser isn\u2019t constrained to the initial estimate bounds, especially on the internationals field we found it could be very unstable and would often get stuck in nonsensical positions like the corners of the field or inside the goals. Constraining the optimiser to the estimate bounds is very helpful to reduce these problems. Image analysis The localiser\u2019s input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding for the colour white, which is handled by the vision pipeline described earlier. With the input provided, a certain number of rays (usually 64) are emitted from the centre of the line image. A ray terminates when it touches a line, reaches the edge of the image or reaches the edge of the mirror (as it would be a waste of time to check outside the mirror). Currently we use simple trigonometry to project this ray, although we could use the Bresenham line algorithm for more performance. Rays are stored as only a length in a regular C array, as we can infer the angle between each ray as: 2pi / n_rays This step can be summarised as essentially \u201csampling\u201d the line around us on the field. Figure 2: example of ray casting on field, with a position near to the centre Camera normalisation These rays are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by measuring the pixels between points along evenly spaced tape placed on the real field, via Omicontrol. Using regression software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances. In our case, we simply apply the dewarp function to each ray length instead, leaving us with each ray essentially in field coordinates (or field lengths) rather than camera coordinates. This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres. Figure 3: example of applying the dewarp function to an entire image, on the low resolution OpenMV H7. TODO update to new images on the above The second phase of the camera normalisation is to rotate the rays relative to the robot\u2019s heading, using a rotation matrix. The robot\u2019s heading value, which is relative to when it was powered on, is transmitted by the ESP32 (again using Protocol Buffers). For information about how this value is calculated using the IMU, see the ESP32 and movement code page. Coordinate optimisation and interpolation The coordinate optimisation stage is achieved by using the Subplex local derivative-free non-linear optimisation algorithm 4 , implemented as part of the NLopt package 5 . This algorithm essentially acts as an efficiency and stability improvement over the well-known Nelder-Mead Simplex algorithm. Subplex is capable of quickly solving systems that have no calculable derivatives with hundreds of variables, so in our case solving a two dimensional problem is trivial for it. Due to the fact that our system is overdetermined, it is not easy to solve and hence why we have to generate an approximate solution using an optimisation algorithm like Subplex (although, as mentioned later, it should be possible to generate a least squares solution). Our problem description is as follows: ( TODO not correct ) The most important part of this process is the objective function , which is a function that takes an N-dimensional vector (in our case, an estimated 2D position) and calculates a \u201cscore\u201d of how accurate the value is. Figure 4: map of objective function for a robot placed at the centre of the field. White pixels indicate high accuracy areas and black pixels indicate less accurate areas. This takes up to 30 seconds to calculate for all 44,226 positions (this shows why a brute force search would not work in competition). Figure 5: another visualisation of the objective function, instead shown by treating it as a heightmap in Blender. The known line layout/geometry of the RoboCup field is encoded into a \u201cfield file\u201d. This is a binary Protocol Buffer file written to disk, and is essentially a bitmap where 1 pixel = 1cm, white if line and black if no line. The reason for recording it as a Protobuf instead of a bitmap is because the format is theoretically capable of storing grids of any resolution, such as 0.5cm if more accuracy was desired. We use a 1cm grid, which stores 44,226 cells and is 42 KiB on disk. This takes about 2 seconds to generate on a fast desktop computer using a Python script that can be modified to support any field geometry such as SuperTeam or our regional Australian field. Unfortunately, due to the expensive raycasting, calculating this objective function is very expensive. However, with smart use of caching and memoisation, we were able to achieve enormous performance improvements (in the magnitude of something like 100x faster). This works by having Omicam store the results of each set of raycasts it does to the static field file to an in-memory cache. Since the field file never changes, when the optimisation algorithm needs to look up that grid cell in future, it can simply pull the results from the cache instead of needlessly recalculating them. This means the only raycasting we do is on the incoming camera frame, saving a heap of time. Since the robot is often still, we have extremely low cache miss rates most of the time. In theory, it would also be possible to generate this in-memory cache completely offline for more performance, although we currently do not. Figure 6: visualisation of the field data component of the field file. In the original 243x182 image, 1 pixel = 1cm In theory, our optimisation problem could actually be represented as a non-linear least squares probblem (rather than a non-linear derivative free problem), and could thus be solved using standard algorithms like the Gauss-Newton or Levenberg-Marquardt algorithms. However, due to a lack of mathematical knowledge I decided to just use the simpler derivative-free algorithm instead. With the robot\u2019s position now determined to the highest accuracy possible, the final step in the localisation process is to smooth and interpolate this coordinate. The optimisation algorithm can produce unstable results, especially in environments where the line is not very visible or mistaken for other objects on the field. Smoothing is currently accomplished by using a simple moving average. A moving median was also tried but led to more unstable results. In future, we would like to investigate more complex digital signal processing such as a low-pass filter or Kalman filter to improve our accuracy. Robot detection We also experimented with a novel robot detection algorithm, although it was not completed in time for release. Initially testing shows it could be pretty accurate, especially with LiDAR instead of video. The algorithm works under the assumption that since most robots are white, they will in turn be picked up by our line detector. We can analyse all the lines we detect and detect outliers, or \u201csuspicious\u201d rays, which will bselong to robots. We do this using the standard method of checking if a ray lies 1.5x outside the interquartile range (IQR). Once that is done, we cluster groups of suspicious rays one after the other into unbroken blobs that are robots. With this, we essentially have a triangle of where the robot could be in (the back face of the triangle is set to be along the nearest line). To check if a robot does in fact lie in this triangle, we check to see if a circle the size of the largest legal robot diameter could fit into the triangle. If the circle fits anywhere, we have a detection - otherwise, it\u2019s a false positive. Although it\u2019s impossible to know exactly where the robot is located in this triangle, we assume that it will be located the closest possible distance to the robot without clipping outside the bounds (we essentially \u201cinscribe\u201d the circle within the triangle this way). With that done, we have successfully detected the (x,y) position of a potential opposition robot! TODO image This algorithm has its limitations, because it makes many assumptions that are not necessarily true such as robots being white or fitting in the inscribed circle. Nonetheless, we believe it would be useful enough in competition to be another helpful feature of Omicam. Limitations and refinements Although a lot of effort has been spent improving the performance of our approach, some issues with it remain: Accuracy can still be improved. Although 1.5cm is good, it\u2019s still a margin of error that may make some more precise movements difficult. The algorithm can be unstable, especially on internationals fields where lines are harder to see. The initial estimate stage and optimiser bounds helps to correct this, but a better algorithm for detecting lines (perhaps using a Sobel filter) should be looked into. On a related note, the optimiser probably needs too many lines visible to generate a stable position. In fact, this limitation is what led us to develop the hybrid sensor-fusion/optimisation algorithm instead of just optimising directly. In the real world, just optimising was too unstable when lines weren\u2019t visible, compared to idealistic simulation renders. The optimisation stage could be solved more intelligently by representing it as a least-squares problem instead of a derivative-free problem, as mentioned above. The algorithm is difficult to debug. Despite programming tools for Omicontrol, it\u2019s still difficult to determine exactly why the algorithm won\u2019t localise sometimes, which could be a problem at competitions. Although this is expected from a complex approach, more debugging tools should be programmed to assist with this. To generate a better final position, a low pass filter or even Kalman filter could be used instead of a simple moving average. Extra/miscellaneous features Omicam also has many additional features outside of vision! Interfacing with Omicontrol To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as the temperature of the SBC. The TCP listen thread receives messages from the Omicontrol client, decodes them and executes the specified action ID (for example, save data to disk) if one is specified. The TCP send thread receives data from the vision and localisation pipelines, and encodes them an sends them over network. One important feature of the Omicam and Omicontrol connection is its ability to operate at high framerates, while not using a ridiculous amount of bandwidth (even though it\u2019s meant to be connected locally, we wanted to keep bandwidth under 2 MB/s). To achieve this, we use the SIMD optimised libjpeg-turbo library to efficiently encode camera frames to JPEG, so as to not waste performance on the remote debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit masks, we found that zlib could compress them more efficiently (around about 460,800x reduction in size compared to uncompressed data, likely due to this being a great use case for zlib\u2019s run-length encoding). Configuration Usually, we embed configuration in a \u201cdefines.h\u201d file. Config includes information like the bounding box of the crop rectangle, the radius of the mirror and the dewarp function. Because this is embedded in a header, the project would have to be recompiled and relaunched every time a setting is updated which is not ideal. For Omicam, we used an INI file stored on the SBC\u2019s disk that is parsed and loaded on every startup. In addition, the config file can also be dynamically reloaded by an Omicontrol command, making even relaunching Omicam un-necessary. Because of this, we have much more flexibility and faster prototyping abilities when tuning to different venues. Video recording Omicam can record its vision output to disk as an MP4 file. When this is enabled in the configuration, a thread is started that writes video frames to an OpenCV VideoWriter at the correct framerate to produce a stable 30 fps video. These generated videos can be loaded as Omicam input (instead of using a live camera), which is useful for offline testing. In this way, games can be recorded for later analysis of Omicam\u2019s performance and accuracy, or for entertainment value. Replay system Similar to the above, we also added a \u201creplay\u201d system (although it wasn\u2019t fully finished). When enabled, Omicam records the data it calculates about the robot\u2019s position and orientation to a binary Protobuf file on disk at a rate of 30 Hz. This format is compact and fault tolerant. The file is periodically re-written every 5 seconds (in case of the app crashing), and we roughly estimate it should be able to encode ~45 minutes of gameplay data in only 16MB. See our Omicontrol section for more details on how this would have looked. Debugging and diagnostics Low-level compiled languages such as C and C++ can be difficult to debug when memory corruption or undefined behaviour issues occur. In addition, many latent bugs can go undetected in code written in these languages. In order to improve the stability of Omicam and fix bugs, we used Google\u2019s Address Sanitizer (ASan) and Undefined Behaviour Sanitizer (UBSan) to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. This (usually) works in parallel with gdb. To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application. Conclusion This year, Team Omicron presents a high-performance, custom vision and localisation application called Omicam. It is written mainly in C, and runs on a LattePanda Delta 432. It is capable of processing vision at orders of magnitude faster than our previous solution, the OpenMV H7. We also present a novel approach to robot localisation, based on sensor-fusion and 2D non-linear optimisation. This approach is significantly more accurate and robust than previous methods. Finally, we introduce support for communication to our visualisation and management application, Omicontrol (covered separately) using Protocol Buffers. In addition to all this, we also include many interesting features (though, untested) features such as replays, dynamic config reloading and video recording. Thank you for reading this! If you have any questions, please don\u2019t hesitate to contact us. References C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-Based Connected Components Labeling With Decision Trees,\u201d IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596\u20131609, 2010, doi: 10.1109/TIP.2010.2044963. \u21a9 Lauer, M., Lange, S. and Riedmiller, M., 2006. Calculating the Perfect Match: An Efficient and Accurate Approach for Robot Self-localization. RoboCup 2005: Robot Soccer World Cup IX, pp.142-153. \u21a9 H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, \u201cRobust and real-time self-localization based on omnidirectional vision for soccer robots,\u201d Adv. Robot., vol. 27, no. 10, pp. 799\u2013811, Jul. 2013, doi: 10.1080/01691864.2013.785473. \u21a9 T. H. Rowan, \u201cFunctional stability analysis of numerical algorithms,\u201d Unpuplished Diss., p. 218, 1990. \u21a9 Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt \u21a9","title":"Omicam"},{"location":"omicam/#background-and-previous-methods","text":"Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer. With the introduction of the orange ball and advanced teams\u2019 application of \u201cball-hiding\u201d strategies, high resolution yet performant computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. Advanced teams now also need to accurately estimate their position on the field (localise) in order to execute strategies and gain the upper hand in the competition. Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the best solution was to develop a custom vision application running on a single board computer (SBC) . In terms of localisation, in the past we managed to get away with not using any, or using a low-fidelity approach based on detecting the goals in the image. With the introduction of strategies this year, however, team members needed more accurate position data, which requires more complex localisation.","title":"Background and previous methods"},{"location":"omicam/#performance-and-results","text":"Omicam is capable of detecting the ball, goals and lines at 60-70fps at 720p (1280x720) resolution. Compared to the previous OpenMV H7, this is 23x higher resolution at 3x the framerate. 1 In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the robot\u2019s position to around 1.5cm accuracy in less than 2ms. This is over 5x/25x more accurate 2 than any previous methods used at BBC Robotics, most likely much faster, and has been shown to be much more reliable and stable in all conditions. Using the e-con Systems Hyperyon camera based around the ultra low-light performance IMX290 sensor, Omicam is robust against lighting conditions ranging from near pitch darkness to direct LED light. 1: previous results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution. 2: depending on whether LRF based/goal based localisation was used.","title":"Performance and results"},{"location":"omicam/#hardware","text":"Omicam is capable of running on any x86 computer running Linux. In our case, we use a LattePanda Delta 432 SBC with a 2.4 GHz quad-core Intel Celeron N4100 and 4GB of RAM. All computer vision tasks run on the CPU, we do not use the GPU. The camera module we use is the e-con Systems Hyperon which uses the IMX290 ultra-low light image sensor. For more information, please see this page .","title":"Hardware"},{"location":"omicam/#field-object-detection","text":"One of the responsibilities of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. This is accomplished by using the open-source computer vision library OpenCV. We use a Gstreamer pipeline to decode our camera\u2019s MJPEG stream at our target resolution of 1280x720 pixels, which is read in through an OpenCV VideoCapture object.","title":"Field object detection"},{"location":"omicam/#pre-processing","text":"With the camera frame in OpenCV memory, we then apply the following pre-processing steps to the image: Crop the frame to only leave a rectangle around the mirror visible Flip the image, if necessary, to account for the mounting of the camera on the robot Apply a circular mask to the image to mask out any pixels that aren\u2019t on the mirror Downscale the frame for use when processing the blue and yellow goals to process faster at the cost of accuracy Mask out the robot from the centre of the mirror (as it has reflective plastic on it which comes up as white)","title":"Pre-processing"},{"location":"omicam/#thresholding-and-component-labelling","text":"After pre-processing, we then use OpenCV\u2019s inRange inside a parallel_for to threshold all objects at once using multiple threads. This produces a 1-bit binary mask of the image, where each pixel is 255 (true) if it\u2019s inside the RGB range specified, and 0 (false) if it\u2019s not. For the lines, the vision processing step is finished here as we only need a binary mask. Unfortunately, the localisation process can\u2019t begin until the goals have finished processing due to the localiser relying on the goals in its initial estimate calculations (see below). Finally, we use OpenCV\u2019s Grana 1 parallel connected component labeller to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. OpenCV automatically calculates the bounding box and centroid for each of these connected regions.","title":"Thresholding and component labelling"},{"location":"omicam/#coordinate-transforms-and-dispatch","text":"Considering the centroid for each field object as a Cartesian vector in pixels coordinates, we convert this vector to polar form and run the magnitude through our mirror dewarping function (see below) to convert it to a length in centimetres. This leaves us with a polar vector for each field object relative to the robot\u2019s centre. We then convert back to Cartesian and use the last localiser position to infer the field object\u2019s absolute position in centimetres. Finally, for the goals we also calculate the relative Cartesian coordinates (convert polar to Cartesian but don\u2019t add localiser position) for use in the localiser\u2019s initial estimate calculations. This information is encoded into a Protobuf format with nanopb, and is sent over UART to the ESP32 using POSIX termios at 115200 baud. Unfortunately, the UART bus is owned by an ATMega 32U4, so a sketch on that device forwards it to the ESP32. TODO images and/or video of thresholded field TODO more detail on dewarp function here (maybe move stuff up from localisation section)","title":"Coordinate transforms and dispatch"},{"location":"omicam/#localisation","text":"","title":"Localisation"},{"location":"omicam/#previous-methods-and-background","text":"Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball. Currently, teams use three main methods of localisation. Firstly, the most common approach uses the detected goal blobs in the camera to triangulate the robot\u2019s position. This approach can be very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in SuperTeam). Using this approach on an OpenMV, we found accuracy of about 15 cm, but running on Omicam at 720p we found accuracy of around 4-6 cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using a few of these sensors on a robot, the position of the robot can be inferred with trigonometry by measuring the distance to the walls. The drawback of this approach is that it\u2019s impossible to reliably distinguish between interfering objects, such as a hand or another robot, compared to a wall. This means that although this approach can be accurate on an empty field, it is very difficult to use reliably in actual games. On an empty field, teams have found accuracies of 5-10 cm, but data is often invalid and can\u2019t be used reliably in real games. The third approach in use by some teams is one based on 360 degree LiDARS. This approach, being similar to the second approach, has similar accuracy and drawbacks. One additional drawback is the expensive cost and heavy weight of LiDARS. Another approach that can be used is summing displacement values from a mouse sensor, or integrating acceleration data from an IMU. While these approaches have high levels of accuracy initially, the repeated integration of the data gradually builds error over time, and if the robot is on for a particularly long time, the localised position may be several centimetres inaccurate by the end. In addition, these approaches all return relative displacement to the initial position, not an absolute position on the field - meaning a method for calculating the robot\u2019s initial position accurately is still required. Some teams, including us in the past, do not localise at all. Strictly speaking, with many gameplay behaviours, knowing the robot\u2019s position is not necessary. However, with our advanced strategies, localisation is a necessity.","title":"Previous methods and background"},{"location":"omicam/#our-approach","text":"This year, Team Omicron presents a novel approach to robot localisation based on a hybrid sensor-fusion/non-linear optimisation algorithm. Our approach builds an initial estimate of the robot\u2019s position using faster, more inaccurate methods like goal triangulation and mouse sensor displacement. It then refines this estimate to a much higher accuracy by solving a 2D non-linear optimisation problem in realtime. The addition of the optimisation stage to the algorithm increases accuracy by about 4.6x, to be as little as 1.5cm error. Our optimisation algorithm works by comparing observed field line geometry from the camera (sampled via raycasting), to a known model of the field. By trying to optimise the robot\u2019s unknown (x,y) position such that it minimises the error between the observed lines and expected lines at each position, we can infer the robot\u2019s coordinates to a very high level of accuracy. The optimisation stage of our sensor fusion algorithm is based on a paper by Lauer, Lange and Redmiller (2006) 2 , as covered in the paper by Lu, Li, Hu and Zheng (2013) 3 . However, our approaches are different in certain areas, with us making some concessions and improvements to better suit the RoboCup Junior league (as the paper was written for middle-size league). Lauer et al\u2019s approach casts rays and marks points where these rays intersect the lines, whereas we work with these rays and their lengths directly. Their approach also calculates the orientation of the robot, while we trust the value of the IMU for orientation to simplify our algorithm. Their optimisation stage uses the RPROP algorithm to minimise error, while we use Subplex (based on the Nelder-Mead simplex algorithm). Finally, Lauer et al\u2019s approach has more complex global localisation and smoothing that can handle unstable localisations with multiple local minima, while our approach assumes only one minimum is present in the objective function. Our approach has the following 5 main steps: Estimate calculation Image analysis Camera normalisation Coordinate optimisation and interpolation (Optional) Robot detection","title":"Our approach"},{"location":"omicam/#estimate-calculation","text":"The localiser first constructs an initial estimate of the robot\u2019s position using faster but more inaccurate methods. The preferred method is to sum the displacement data from our PWM3360 mouse sensor to determine our current position since last localisation. However, in certain cases such as the initial localisation after the robot is powered on, we also use the blue and yellow goals to triangulate our position since there\u2019s no initial position for the mouse sensor to use. Once we have an initial localisation though, we can go back to using the mouse sensor data. Figure 1: method for triangulating position using goals coordinates and vectors Once the initial estimate is calculated, which is usually accurate to about 6-10cm, we constrain the optimiser\u2019s bounds to a certain sized rectangle around the centre of the estimated position. We also change the optimiser\u2019s initial estimate position to the estimated position, and scale down its step size significantly. We assume that the estimate is close to the actual position, so a smaller step size means that it won\u2019t overshoot the minimum and will hopefully converge faster. The combination of the estimate bounds with changing localiser settings means that we can often stably converge on the real position of the robot with around 40 objective function evaluations. If the localiser isn\u2019t constrained to the initial estimate bounds, especially on the internationals field we found it could be very unstable and would often get stuck in nonsensical positions like the corners of the field or inside the goals. Constraining the optimiser to the estimate bounds is very helpful to reduce these problems.","title":"Estimate calculation"},{"location":"omicam/#image-analysis","text":"The localiser\u2019s input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding for the colour white, which is handled by the vision pipeline described earlier. With the input provided, a certain number of rays (usually 64) are emitted from the centre of the line image. A ray terminates when it touches a line, reaches the edge of the image or reaches the edge of the mirror (as it would be a waste of time to check outside the mirror). Currently we use simple trigonometry to project this ray, although we could use the Bresenham line algorithm for more performance. Rays are stored as only a length in a regular C array, as we can infer the angle between each ray as: 2pi / n_rays This step can be summarised as essentially \u201csampling\u201d the line around us on the field. Figure 2: example of ray casting on field, with a position near to the centre","title":"Image analysis"},{"location":"omicam/#camera-normalisation","text":"These rays are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by measuring the pixels between points along evenly spaced tape placed on the real field, via Omicontrol. Using regression software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances. In our case, we simply apply the dewarp function to each ray length instead, leaving us with each ray essentially in field coordinates (or field lengths) rather than camera coordinates. This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres. Figure 3: example of applying the dewarp function to an entire image, on the low resolution OpenMV H7. TODO update to new images on the above The second phase of the camera normalisation is to rotate the rays relative to the robot\u2019s heading, using a rotation matrix. The robot\u2019s heading value, which is relative to when it was powered on, is transmitted by the ESP32 (again using Protocol Buffers). For information about how this value is calculated using the IMU, see the ESP32 and movement code page.","title":"Camera normalisation"},{"location":"omicam/#coordinate-optimisation-and-interpolation","text":"The coordinate optimisation stage is achieved by using the Subplex local derivative-free non-linear optimisation algorithm 4 , implemented as part of the NLopt package 5 . This algorithm essentially acts as an efficiency and stability improvement over the well-known Nelder-Mead Simplex algorithm. Subplex is capable of quickly solving systems that have no calculable derivatives with hundreds of variables, so in our case solving a two dimensional problem is trivial for it. Due to the fact that our system is overdetermined, it is not easy to solve and hence why we have to generate an approximate solution using an optimisation algorithm like Subplex (although, as mentioned later, it should be possible to generate a least squares solution). Our problem description is as follows: ( TODO not correct ) The most important part of this process is the objective function , which is a function that takes an N-dimensional vector (in our case, an estimated 2D position) and calculates a \u201cscore\u201d of how accurate the value is. Figure 4: map of objective function for a robot placed at the centre of the field. White pixels indicate high accuracy areas and black pixels indicate less accurate areas. This takes up to 30 seconds to calculate for all 44,226 positions (this shows why a brute force search would not work in competition). Figure 5: another visualisation of the objective function, instead shown by treating it as a heightmap in Blender. The known line layout/geometry of the RoboCup field is encoded into a \u201cfield file\u201d. This is a binary Protocol Buffer file written to disk, and is essentially a bitmap where 1 pixel = 1cm, white if line and black if no line. The reason for recording it as a Protobuf instead of a bitmap is because the format is theoretically capable of storing grids of any resolution, such as 0.5cm if more accuracy was desired. We use a 1cm grid, which stores 44,226 cells and is 42 KiB on disk. This takes about 2 seconds to generate on a fast desktop computer using a Python script that can be modified to support any field geometry such as SuperTeam or our regional Australian field. Unfortunately, due to the expensive raycasting, calculating this objective function is very expensive. However, with smart use of caching and memoisation, we were able to achieve enormous performance improvements (in the magnitude of something like 100x faster). This works by having Omicam store the results of each set of raycasts it does to the static field file to an in-memory cache. Since the field file never changes, when the optimisation algorithm needs to look up that grid cell in future, it can simply pull the results from the cache instead of needlessly recalculating them. This means the only raycasting we do is on the incoming camera frame, saving a heap of time. Since the robot is often still, we have extremely low cache miss rates most of the time. In theory, it would also be possible to generate this in-memory cache completely offline for more performance, although we currently do not. Figure 6: visualisation of the field data component of the field file. In the original 243x182 image, 1 pixel = 1cm In theory, our optimisation problem could actually be represented as a non-linear least squares probblem (rather than a non-linear derivative free problem), and could thus be solved using standard algorithms like the Gauss-Newton or Levenberg-Marquardt algorithms. However, due to a lack of mathematical knowledge I decided to just use the simpler derivative-free algorithm instead. With the robot\u2019s position now determined to the highest accuracy possible, the final step in the localisation process is to smooth and interpolate this coordinate. The optimisation algorithm can produce unstable results, especially in environments where the line is not very visible or mistaken for other objects on the field. Smoothing is currently accomplished by using a simple moving average. A moving median was also tried but led to more unstable results. In future, we would like to investigate more complex digital signal processing such as a low-pass filter or Kalman filter to improve our accuracy.","title":"Coordinate optimisation and interpolation"},{"location":"omicam/#robot-detection","text":"We also experimented with a novel robot detection algorithm, although it was not completed in time for release. Initially testing shows it could be pretty accurate, especially with LiDAR instead of video. The algorithm works under the assumption that since most robots are white, they will in turn be picked up by our line detector. We can analyse all the lines we detect and detect outliers, or \u201csuspicious\u201d rays, which will bselong to robots. We do this using the standard method of checking if a ray lies 1.5x outside the interquartile range (IQR). Once that is done, we cluster groups of suspicious rays one after the other into unbroken blobs that are robots. With this, we essentially have a triangle of where the robot could be in (the back face of the triangle is set to be along the nearest line). To check if a robot does in fact lie in this triangle, we check to see if a circle the size of the largest legal robot diameter could fit into the triangle. If the circle fits anywhere, we have a detection - otherwise, it\u2019s a false positive. Although it\u2019s impossible to know exactly where the robot is located in this triangle, we assume that it will be located the closest possible distance to the robot without clipping outside the bounds (we essentially \u201cinscribe\u201d the circle within the triangle this way). With that done, we have successfully detected the (x,y) position of a potential opposition robot! TODO image This algorithm has its limitations, because it makes many assumptions that are not necessarily true such as robots being white or fitting in the inscribed circle. Nonetheless, we believe it would be useful enough in competition to be another helpful feature of Omicam.","title":"Robot detection"},{"location":"omicam/#limitations-and-refinements","text":"Although a lot of effort has been spent improving the performance of our approach, some issues with it remain: Accuracy can still be improved. Although 1.5cm is good, it\u2019s still a margin of error that may make some more precise movements difficult. The algorithm can be unstable, especially on internationals fields where lines are harder to see. The initial estimate stage and optimiser bounds helps to correct this, but a better algorithm for detecting lines (perhaps using a Sobel filter) should be looked into. On a related note, the optimiser probably needs too many lines visible to generate a stable position. In fact, this limitation is what led us to develop the hybrid sensor-fusion/optimisation algorithm instead of just optimising directly. In the real world, just optimising was too unstable when lines weren\u2019t visible, compared to idealistic simulation renders. The optimisation stage could be solved more intelligently by representing it as a least-squares problem instead of a derivative-free problem, as mentioned above. The algorithm is difficult to debug. Despite programming tools for Omicontrol, it\u2019s still difficult to determine exactly why the algorithm won\u2019t localise sometimes, which could be a problem at competitions. Although this is expected from a complex approach, more debugging tools should be programmed to assist with this. To generate a better final position, a low pass filter or even Kalman filter could be used instead of a simple moving average.","title":"Limitations and refinements"},{"location":"omicam/#extramiscellaneous-features","text":"Omicam also has many additional features outside of vision!","title":"Extra/miscellaneous features"},{"location":"omicam/#interfacing-with-omicontrol","text":"To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as the temperature of the SBC. The TCP listen thread receives messages from the Omicontrol client, decodes them and executes the specified action ID (for example, save data to disk) if one is specified. The TCP send thread receives data from the vision and localisation pipelines, and encodes them an sends them over network. One important feature of the Omicam and Omicontrol connection is its ability to operate at high framerates, while not using a ridiculous amount of bandwidth (even though it\u2019s meant to be connected locally, we wanted to keep bandwidth under 2 MB/s). To achieve this, we use the SIMD optimised libjpeg-turbo library to efficiently encode camera frames to JPEG, so as to not waste performance on the remote debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit masks, we found that zlib could compress them more efficiently (around about 460,800x reduction in size compared to uncompressed data, likely due to this being a great use case for zlib\u2019s run-length encoding).","title":"Interfacing with Omicontrol"},{"location":"omicam/#configuration","text":"Usually, we embed configuration in a \u201cdefines.h\u201d file. Config includes information like the bounding box of the crop rectangle, the radius of the mirror and the dewarp function. Because this is embedded in a header, the project would have to be recompiled and relaunched every time a setting is updated which is not ideal. For Omicam, we used an INI file stored on the SBC\u2019s disk that is parsed and loaded on every startup. In addition, the config file can also be dynamically reloaded by an Omicontrol command, making even relaunching Omicam un-necessary. Because of this, we have much more flexibility and faster prototyping abilities when tuning to different venues.","title":"Configuration"},{"location":"omicam/#video-recording","text":"Omicam can record its vision output to disk as an MP4 file. When this is enabled in the configuration, a thread is started that writes video frames to an OpenCV VideoWriter at the correct framerate to produce a stable 30 fps video. These generated videos can be loaded as Omicam input (instead of using a live camera), which is useful for offline testing. In this way, games can be recorded for later analysis of Omicam\u2019s performance and accuracy, or for entertainment value.","title":"Video recording"},{"location":"omicam/#replay-system","text":"Similar to the above, we also added a \u201creplay\u201d system (although it wasn\u2019t fully finished). When enabled, Omicam records the data it calculates about the robot\u2019s position and orientation to a binary Protobuf file on disk at a rate of 30 Hz. This format is compact and fault tolerant. The file is periodically re-written every 5 seconds (in case of the app crashing), and we roughly estimate it should be able to encode ~45 minutes of gameplay data in only 16MB. See our Omicontrol section for more details on how this would have looked.","title":"Replay system"},{"location":"omicam/#debugging-and-diagnostics","text":"Low-level compiled languages such as C and C++ can be difficult to debug when memory corruption or undefined behaviour issues occur. In addition, many latent bugs can go undetected in code written in these languages. In order to improve the stability of Omicam and fix bugs, we used Google\u2019s Address Sanitizer (ASan) and Undefined Behaviour Sanitizer (UBSan) to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. This (usually) works in parallel with gdb. To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application.","title":"Debugging and diagnostics"},{"location":"omicam/#conclusion","text":"This year, Team Omicron presents a high-performance, custom vision and localisation application called Omicam. It is written mainly in C, and runs on a LattePanda Delta 432. It is capable of processing vision at orders of magnitude faster than our previous solution, the OpenMV H7. We also present a novel approach to robot localisation, based on sensor-fusion and 2D non-linear optimisation. This approach is significantly more accurate and robust than previous methods. Finally, we introduce support for communication to our visualisation and management application, Omicontrol (covered separately) using Protocol Buffers. In addition to all this, we also include many interesting features (though, untested) features such as replays, dynamic config reloading and video recording. Thank you for reading this! If you have any questions, please don\u2019t hesitate to contact us.","title":"Conclusion"},{"location":"omicam/#references","text":"C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-Based Connected Components Labeling With Decision Trees,\u201d IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596\u20131609, 2010, doi: 10.1109/TIP.2010.2044963. \u21a9 Lauer, M., Lange, S. and Riedmiller, M., 2006. Calculating the Perfect Match: An Efficient and Accurate Approach for Robot Self-localization. RoboCup 2005: Robot Soccer World Cup IX, pp.142-153. \u21a9 H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, \u201cRobust and real-time self-localization based on omnidirectional vision for soccer robots,\u201d Adv. Robot., vol. 27, no. 10, pp. 799\u2013811, Jul. 2013, doi: 10.1080/01691864.2013.785473. \u21a9 T. H. Rowan, \u201cFunctional stability analysis of numerical algorithms,\u201d Unpuplished Diss., p. 218, 1990. \u21a9 Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt \u21a9","title":"References"},{"location":"omicontrol/","text":"Page author: Matt Young, vision systems & low-level developer Omicontrol is Team Omicron\u2019s wireless robot/camera debugging and management application. With the creation of Omicam, we needed a way of visualising camera output, managing the SBC and editing thresholds. In the past, we used the OpenMV IDE, but given that Omicam is entirely custom developed, this meant we also had to custom-develop our own camera application. In addition, with our new advanced strategies and localisation, as well as the potential auto referee future rule addition, we felt like this year was a good time to add a remote robot management feature in our future app with the ability to reposition, reorient and automatically reset robots on the field wirelessly, as well as visualise their positions. We added both of these features into Omicontrol, to make it essentially our all-in-one robot management application. Because the application will be used frequently in debugging as well as in the high-pressure mid-game interval, Omicontrol\u2019s primary design goals are to be easy to use and reliable. Omicontrol is a cross-platform application that works on Windows, Mac and Linux. It\u2019s developed in Kotlin and uses JavaFX for the GUI, via the TornadoFX framework. Figure 1: Omicontrol main window running on KDE neon TODO: VIDEO Views Omicontrol functions through the concepts of \u201cviews\u201d, which provide different information about different aspects of our robot(s). Camera view The Camera View can to view camera output and tune threshold parameters. This view receives a JPEG-compressed 720p video stream from Omicam and can be configured to display both the bounding box and individual pixels selected by the thresholds. The camera view supports the editing and live preview of thresholds for different objects (balls, yellow goal, blue goal, lines). This has greatly sped up development because thresholds can be previewed in realtime instead of having to wait for the app to restart (as was the case in OpenMV). The camera view supports thresholds specified in RGB, HSV, YUV and LAB colour spaces (although only RGB is sent to the robot). When changing colour spaces, the current threshold values are automatically converted. At the time of writing, this was a little bugged so we mostly just used RGB for thresholding. Field view Similar to teams in the major league, we implement a Field View to display the robots\u2019 localised positions and enable control of them. The field view allows us to get a full picture of the robot\u2019s world model (how it sees the world currently), and has been immensely useful in evaluating the performance of the localiser (as well as just being a cool thing to show off!). This view displays a top-down model of the field with the following features: sprites for each of our robots\u2019 localised position (drawn to the correct orientation from the IMU as well) sprite for the ball\u2019s estimated position displays for detected robots and their estimated position many debug features for looking at he localisation algorithm and goal detection bandwidth display, localisation optimiser performance display of each robot\u2019s FSM state The sidebar also contains a panel for controlling the behaviour of the robots. Due to COVID-19 we didn\u2019t have a working robot as explained in other sections, so this feature wasn\u2019t fully implemented. Although the commands are forwarded from Omicontrol, to Omicam, to the ESP32, the ESP32 currently ignores the command. In a future release, users would be able to click to move robots to a specific position, pause robots, orient them and even have them return to their starting position automatically. This would have proved very useful for testing robot behaviour. Calibration view This view is used to calibrate the mirror dewarp model, which translates pixel distances from the centre of the mirror into centimetre distances on the real field. In the past, this was complex and could take upwards of half an hour. We decided this process could be greatly streamlined, so we added the calibration view to Omicontrol. The general flow of generating a mirror model is this process: Lay out a ruler, with clear tape every 5cm Start Omicam and Omicontrol, and open the Calibration View Click on each of the 5cm strips in order to generate data points Select the regression type (exponential or polynomial) Click \u201cCalculate model\u201d Copy the results into the mirrorModel field in Omicam and refresh the config file This approach streamlines mirror model calculation from being a painstaking 30 minute task, into one which can be done in a few button presses in under 5 minutes. Model calculation process Once the user has selected a set of points, and the pixel and corresponding centimetre distances have been determined, we now have an interesting problem to solve. Essentially, we have to interpolate this dataset so that for any pixel distance we have, we can generate a corresponding centimetre distance. Ideally, this interpolation method would be accurate for data points that were not observed in the dataset as well (for example, they\u2019re larger than the largest sampled point distance). This means there\u2019s two categories of algorithms we can use: curve fitting and polynomial interpolation. Curve fitting, or a regression, performs a mathematical optimisation routine to optimise the coefficients for an actual function. Polynomial interpolation uses a series of polynomials to interpolate between data points. Examples of polynomial interpolation methods include Lagrange polynomials, Newton polynomials and also spline-based methods like cubic splines. The trouble with polynomial interpolation is that it doesn\u2019t generalise to points outside the sampled dataset, whereas a regression does. Because a regression generates coefficients for a mathematical model, in theory, it can extend with some accuracy outside the dataset, whereas polynomial interpolation just interpolates inside the dataset. Because of this, we decided to use a regression to generate our mirror model. The two models we chose to use for our regression were an N-th order polynomial and an exponential in the form f(x) = ae^(bx) . As both of these models are non-linear, we need to do a non-linear least squares regression to calculate the coefficients. In the polynomial this is N unknowns, and in the exponential it\u2019s just a and b . Traditionally, we used to export the data to Excel and have it calculated in there, but we decided instead to implement the regression ourselves to increase the speed of the mirror calculation process. To perform our least squares regression, we use the Apache Commons Math library and its AbstractCurveFitter . This internally uses the Levenberg-Marquardt algorithm, an improvement over the Gauss-Newton algorithm, to converge on the most optimal values for the unknown coefficients. Although Commons Math has a built in PolynomialCurveFitter which can fit a PolynomialFunction , there is no exponential curve fitter or exponential function provided, so we built our own ExponentialCurveFitter and ExpFunction with value and gradient vector. Finally, we also calculate the coefficient of determination (R 2 value) to analyse the accuracy of the model. Commons Math has no built-in way of finding the R 2 value from the result of an AbstractCurveFitter implementation, so instead we implemented it ourselves by calculating the residual sum of squares and total sum of squares as explained here . Replay view Unfortunately, the replay view wasn\u2019t completed in time for release. In a future release, the replay view will be very similar to the field view, except it would allow playing back of recorded Omicam Protobuf replay files (*.omirec). The UI would essentially be the same, just minus the online features like movement control and the addition of a video-player-like scrub bar and play/pause/time warp controls. Wireless connection Being able to connect to any robot wirelessly and manage it can be a great quality of life improvement when debugging fast moving robots. There are a variety of wireless protocols in existence such as Bluetooth Classic, Bluetooth LE, Zigbee and others. However, we decided to use TCP/IP over WiFi because it\u2019s easy to setup and reliable, compared to Bluetooth which has issues on many platforms. To accomplish this, the LattePanda SBC creates a WiFi access point (with no internet connection) that the client computer then connects to, establishing a direct link between the two computers. As shown in Figure 1, the user simply types the local IP to connect and everything else is handled for them. Due to the nature of TCP/IP, we interestingly also have Omicontrol across the Internet. In this setup, an Internet-connected router hosts the LattePanda device on its network via Ethernet or WiFi. The router must port forward port 42708 which is used for Omicam to Omicontrol communications. Then, the client computer in an entirely different location, even a different country, connects to the Omicam router\u2019s public IP address and can interact with the camera and robots as normal. While this is interesting, nonetheless it\u2019s not used in practice due to lack of security, lack of need and Omicam\u2019s bandwidth and latency being too high (as its designed for high-speed local connections). Wired connection Although wireless connection is very flexible, on the LattePanda it unfortunately comes at the cost of a very poor connection quality. Thus, we decided to use gigabit Ethernet as an alternative connection type. This also acts as a backup in case the venue bans WiFi hotspots (as has happened before), or there\u2019s too much signal noise to connect properly. This works by using a special Ethernet crossover cable and creating a simple network through assigning static IPs. The user can simply connect to the static IP they assigned the SBC. Just like the WiFi setup, this creates an offline connection between the two devices at a very high bandwidth and stability. We experience no connection issues at all over Ethernet (although there are some occasional issues with SSH). Communication protocol The backbone of bidrectional Omicam to Omicontrol communication is a simple TCP socket on port 42708, with Omicam as the host and Omicontrol as the client. This works both for wireless and wired connections. Communication between the two devices makes heavy use of Protocol Buffers, on the Omicam side via nanopb, and on the Omicontrol end in Kotlin via the official Java API from Google (which also works with Kotlin). One of the benefits of Protocol Buffers is that both Omicam with nanopb and Omicontrol with the Java API share the exact same Protobuf definition file ( RemoteDebug.proto ), they just are the result of different code generators. This means that as long as both the Java and C code is generated at the same time, they are guaranteed to be compatible with each other. Internally, Omicontrol creates a thread to manage the TCP connection in the ConnectionManager class which handles encoding and transmitting Protocol Buffer data over the socket, as well as detecting errors in the connection. All of the code that awaits a response from the remote runs in a separate thread to the JavaFX UI thread so that the UI isn\u2019t locked up every time a response is queued. When a button in Omicontrol is clicked, this thread encodes and transmits a Protocol Buffer message to Omicam and waits for a response asynchronously, notifying the calling thread once one is received. Design considerations The Omicontrol UI was designed to look professional, be simple to use, friendly to use as a tablet and work cross-platform. Since Omicontrol is written in Kotlin using JavaFX (via TornadoFX), the app runs on Windows, Mac and Linux with minimal porting required (mainly for changing display types).","title":"Omicontrol"},{"location":"omicontrol/#views","text":"Omicontrol functions through the concepts of \u201cviews\u201d, which provide different information about different aspects of our robot(s).","title":"Views"},{"location":"omicontrol/#camera-view","text":"The Camera View can to view camera output and tune threshold parameters. This view receives a JPEG-compressed 720p video stream from Omicam and can be configured to display both the bounding box and individual pixels selected by the thresholds. The camera view supports the editing and live preview of thresholds for different objects (balls, yellow goal, blue goal, lines). This has greatly sped up development because thresholds can be previewed in realtime instead of having to wait for the app to restart (as was the case in OpenMV). The camera view supports thresholds specified in RGB, HSV, YUV and LAB colour spaces (although only RGB is sent to the robot). When changing colour spaces, the current threshold values are automatically converted. At the time of writing, this was a little bugged so we mostly just used RGB for thresholding.","title":"Camera view"},{"location":"omicontrol/#field-view","text":"Similar to teams in the major league, we implement a Field View to display the robots\u2019 localised positions and enable control of them. The field view allows us to get a full picture of the robot\u2019s world model (how it sees the world currently), and has been immensely useful in evaluating the performance of the localiser (as well as just being a cool thing to show off!). This view displays a top-down model of the field with the following features: sprites for each of our robots\u2019 localised position (drawn to the correct orientation from the IMU as well) sprite for the ball\u2019s estimated position displays for detected robots and their estimated position many debug features for looking at he localisation algorithm and goal detection bandwidth display, localisation optimiser performance display of each robot\u2019s FSM state The sidebar also contains a panel for controlling the behaviour of the robots. Due to COVID-19 we didn\u2019t have a working robot as explained in other sections, so this feature wasn\u2019t fully implemented. Although the commands are forwarded from Omicontrol, to Omicam, to the ESP32, the ESP32 currently ignores the command. In a future release, users would be able to click to move robots to a specific position, pause robots, orient them and even have them return to their starting position automatically. This would have proved very useful for testing robot behaviour.","title":"Field view"},{"location":"omicontrol/#calibration-view","text":"This view is used to calibrate the mirror dewarp model, which translates pixel distances from the centre of the mirror into centimetre distances on the real field. In the past, this was complex and could take upwards of half an hour. We decided this process could be greatly streamlined, so we added the calibration view to Omicontrol. The general flow of generating a mirror model is this process: Lay out a ruler, with clear tape every 5cm Start Omicam and Omicontrol, and open the Calibration View Click on each of the 5cm strips in order to generate data points Select the regression type (exponential or polynomial) Click \u201cCalculate model\u201d Copy the results into the mirrorModel field in Omicam and refresh the config file This approach streamlines mirror model calculation from being a painstaking 30 minute task, into one which can be done in a few button presses in under 5 minutes.","title":"Calibration view"},{"location":"omicontrol/#model-calculation-process","text":"Once the user has selected a set of points, and the pixel and corresponding centimetre distances have been determined, we now have an interesting problem to solve. Essentially, we have to interpolate this dataset so that for any pixel distance we have, we can generate a corresponding centimetre distance. Ideally, this interpolation method would be accurate for data points that were not observed in the dataset as well (for example, they\u2019re larger than the largest sampled point distance). This means there\u2019s two categories of algorithms we can use: curve fitting and polynomial interpolation. Curve fitting, or a regression, performs a mathematical optimisation routine to optimise the coefficients for an actual function. Polynomial interpolation uses a series of polynomials to interpolate between data points. Examples of polynomial interpolation methods include Lagrange polynomials, Newton polynomials and also spline-based methods like cubic splines. The trouble with polynomial interpolation is that it doesn\u2019t generalise to points outside the sampled dataset, whereas a regression does. Because a regression generates coefficients for a mathematical model, in theory, it can extend with some accuracy outside the dataset, whereas polynomial interpolation just interpolates inside the dataset. Because of this, we decided to use a regression to generate our mirror model. The two models we chose to use for our regression were an N-th order polynomial and an exponential in the form f(x) = ae^(bx) . As both of these models are non-linear, we need to do a non-linear least squares regression to calculate the coefficients. In the polynomial this is N unknowns, and in the exponential it\u2019s just a and b . Traditionally, we used to export the data to Excel and have it calculated in there, but we decided instead to implement the regression ourselves to increase the speed of the mirror calculation process. To perform our least squares regression, we use the Apache Commons Math library and its AbstractCurveFitter . This internally uses the Levenberg-Marquardt algorithm, an improvement over the Gauss-Newton algorithm, to converge on the most optimal values for the unknown coefficients. Although Commons Math has a built in PolynomialCurveFitter which can fit a PolynomialFunction , there is no exponential curve fitter or exponential function provided, so we built our own ExponentialCurveFitter and ExpFunction with value and gradient vector. Finally, we also calculate the coefficient of determination (R 2 value) to analyse the accuracy of the model. Commons Math has no built-in way of finding the R 2 value from the result of an AbstractCurveFitter implementation, so instead we implemented it ourselves by calculating the residual sum of squares and total sum of squares as explained here .","title":"Model calculation process"},{"location":"omicontrol/#replay-view","text":"Unfortunately, the replay view wasn\u2019t completed in time for release. In a future release, the replay view will be very similar to the field view, except it would allow playing back of recorded Omicam Protobuf replay files (*.omirec). The UI would essentially be the same, just minus the online features like movement control and the addition of a video-player-like scrub bar and play/pause/time warp controls.","title":"Replay view"},{"location":"omicontrol/#wireless-connection","text":"Being able to connect to any robot wirelessly and manage it can be a great quality of life improvement when debugging fast moving robots. There are a variety of wireless protocols in existence such as Bluetooth Classic, Bluetooth LE, Zigbee and others. However, we decided to use TCP/IP over WiFi because it\u2019s easy to setup and reliable, compared to Bluetooth which has issues on many platforms. To accomplish this, the LattePanda SBC creates a WiFi access point (with no internet connection) that the client computer then connects to, establishing a direct link between the two computers. As shown in Figure 1, the user simply types the local IP to connect and everything else is handled for them. Due to the nature of TCP/IP, we interestingly also have Omicontrol across the Internet. In this setup, an Internet-connected router hosts the LattePanda device on its network via Ethernet or WiFi. The router must port forward port 42708 which is used for Omicam to Omicontrol communications. Then, the client computer in an entirely different location, even a different country, connects to the Omicam router\u2019s public IP address and can interact with the camera and robots as normal. While this is interesting, nonetheless it\u2019s not used in practice due to lack of security, lack of need and Omicam\u2019s bandwidth and latency being too high (as its designed for high-speed local connections).","title":"Wireless connection"},{"location":"omicontrol/#wired-connection","text":"Although wireless connection is very flexible, on the LattePanda it unfortunately comes at the cost of a very poor connection quality. Thus, we decided to use gigabit Ethernet as an alternative connection type. This also acts as a backup in case the venue bans WiFi hotspots (as has happened before), or there\u2019s too much signal noise to connect properly. This works by using a special Ethernet crossover cable and creating a simple network through assigning static IPs. The user can simply connect to the static IP they assigned the SBC. Just like the WiFi setup, this creates an offline connection between the two devices at a very high bandwidth and stability. We experience no connection issues at all over Ethernet (although there are some occasional issues with SSH).","title":"Wired connection"},{"location":"omicontrol/#communication-protocol","text":"The backbone of bidrectional Omicam to Omicontrol communication is a simple TCP socket on port 42708, with Omicam as the host and Omicontrol as the client. This works both for wireless and wired connections. Communication between the two devices makes heavy use of Protocol Buffers, on the Omicam side via nanopb, and on the Omicontrol end in Kotlin via the official Java API from Google (which also works with Kotlin). One of the benefits of Protocol Buffers is that both Omicam with nanopb and Omicontrol with the Java API share the exact same Protobuf definition file ( RemoteDebug.proto ), they just are the result of different code generators. This means that as long as both the Java and C code is generated at the same time, they are guaranteed to be compatible with each other. Internally, Omicontrol creates a thread to manage the TCP connection in the ConnectionManager class which handles encoding and transmitting Protocol Buffer data over the socket, as well as detecting errors in the connection. All of the code that awaits a response from the remote runs in a separate thread to the JavaFX UI thread so that the UI isn\u2019t locked up every time a response is queued. When a button in Omicontrol is clicked, this thread encodes and transmits a Protocol Buffer message to Omicam and waits for a response asynchronously, notifying the calling thread once one is received.","title":"Communication protocol"},{"location":"omicontrol/#design-considerations","text":"The Omicontrol UI was designed to look professional, be simple to use, friendly to use as a tablet and work cross-platform. Since Omicontrol is written in Kotlin using JavaFX (via TornadoFX), the app runs on Windows, Mac and Linux with minimal porting required (mainly for changing display types).","title":"Design considerations"},{"location":"open_source/","text":"Open source acknowledgements Team Omicron is happy to acknowledge our use of many open-source projects in developing our robots software, which we acknowledge here: Omicam log.c : MIT licence iniparser : MIT licence nanopb : Zlib licence DG_dynarr : Public Domain rpa_queue : Apache 2 licence libjpeg-turbo : BSD 3-Clause, IJG and Zlib licences NLopt : MIT licence OpenCV : BSD 3-Clause licence mathc : Zlib licence qdbmp : MIT licence Omicontrol Protocol Buffers : BSD 3-clause licence TornadoFX : Apache 2 licence Apache Commons Math, Lang, IO : Apache 2 licence GreenRobot EventBus : Apache 2 licence exp4j : Apache 2 licence colormath : Apache 2 licence ESP32 firmware ESP-IDF : Apache 2 licence DG_dynarr : Public domain HandmadeMath : Public domain Nanopb : zlib licence BNO055_driver : BSD 3-Clause licence esp32-button : MIT licence ATMega/Teensy firmware LICENCES HERE","title":"Third party library notices"},{"location":"open_source/#open-source-acknowledgements","text":"Team Omicron is happy to acknowledge our use of many open-source projects in developing our robots software, which we acknowledge here:","title":"Open source acknowledgements"},{"location":"open_source/#omicam","text":"log.c : MIT licence iniparser : MIT licence nanopb : Zlib licence DG_dynarr : Public Domain rpa_queue : Apache 2 licence libjpeg-turbo : BSD 3-Clause, IJG and Zlib licences NLopt : MIT licence OpenCV : BSD 3-Clause licence mathc : Zlib licence qdbmp : MIT licence","title":"Omicam"},{"location":"open_source/#omicontrol","text":"Protocol Buffers : BSD 3-clause licence TornadoFX : Apache 2 licence Apache Commons Math, Lang, IO : Apache 2 licence GreenRobot EventBus : Apache 2 licence exp4j : Apache 2 licence colormath : Apache 2 licence","title":"Omicontrol"},{"location":"open_source/#esp32-firmware","text":"ESP-IDF : Apache 2 licence DG_dynarr : Public domain HandmadeMath : Public domain Nanopb : zlib licence BNO055_driver : BSD 3-Clause licence esp32-button : MIT licence","title":"ESP32 firmware"},{"location":"open_source/#atmegateensy-firmware","text":"LICENCES HERE","title":"ATMega/Teensy firmware"},{"location":"open_source_release/","text":"Open source release Introduction by Matt Young. Introduction Hi, and thanks for reading this! This year, 2020, has been a long one for everyone, RoboCup Jr teams included. Regardless of where you\u2019re from, COVID-19 is bound to have affected you in one way or another. At Team Omicron, it effected us quite significantly. The 2019 Internationals competition had left all team members with a lot to be desired, with many bugs to fix and new things to add. Thus, we all set our sights to the 2020 competition. We undertook many new research & development projects in the hope of finally bringing something (hopefully) game-changing to soccer. Our plan was, of course, to deploy this all at the 2020 RoboCup Jr Internationals in Bordeaux, France. Unfortunately, however, this competition was tragically cancelled due to the COVID-19 restrictions, and it\u2019s unlikely we\u2019ll will have the chance to go back next year in 2021 (should it even go ahead). This left us feeling extremely disappointed, and thus development and testing of the robot slowed down quite a lot. Although we never got a chance to make our difference in the official competitions, we at Team Omicron still want to give back to the community somehow, or do something with the hundreds of hours we\u2019ve individually spent designing, programming, testing and writing. As such, we\u2019ve decided to open source our entire project! This means that every single last bit of code, hardware (both structural & electrical), all formal documentation and even internal testing documents/notes will be released under permissive open source licences. We provide documentation below and on GitHub for how to get started. We\u2019re really excited about doing this, because we believe it\u2019s the largest scale open-source release of a relatively successful and long-running RoboCup Jr team. We hope that it\u2019s really beneficial to the community and allows teams new and old to grow, adapt and improve on their current setups. Although Omicron ourselves will never have the chance to deploy our new technology at a competition, we hope that others will enjoy the same honour, and all of our team members look forward to seeing what people come up with. If you have any questions or comments, shoot them our way - see the About page for contact details. For myself and everyone else on Team Omicron, it\u2019s been an amazing experience participating in RoboCup Jr over the years, even despite this year\u2019s many disasters. Anyway, that\u2019s it from me. Thanks, and have fun! Getting started To get started, simply install Git and clone our GitHub repo . This repo contains all our projects and heaps of documentation to get you started. Please read the README.md file all the way through, and branch out to individual projects from there. License Code TODO mention poster credit All code written by Team Omicron is released under the Mozilla Public License 2.0 (see LICENSE.txt in each directory). You will be able to tell which code is ours due to the presence of Omicron copyright notices at the top of the file. For information on this licence, please read this FAQ , and this question . Simply put, if you are building a robot based on Team Omicron\u2019s code, the MPL requires that you disclose that your robot uses open-source software from Team Omicron, and where anyone can obtain it (this repo). A great place to do so would be in your poster and presentation. If you modify any files with the MPL licence header, the licence requires that you release your improvements under the MPL 2.0 as well. New files that you create can be under any licence. We have decided to use the MPL because we believe it balances freedom of usage, while making sure that any improvements are released back to the RoboCup community for future teams to benefit from. Designs TODO figure out designs licence. CC-BY-SA most likely. Some final thoughts While it\u2019s certainly possible to use, say, our camera project on your robot by simply downloading and compiling it, I personally think it\u2019s much more educational to use our design ideas and code as a baseline for your own projects . For example, we think that our novel localisation algorithm works pretty well. While it\u2019s certainly possible to use our implementation in Omicam on your robot directly (and there are instructions for doing so), I think it\u2019s much more impressive and educational to use that code as a method for creating your own implementation of our algorithms - or creating better algorithms that run faster and more accurately! That\u2019s the spirit of RoboCup, after all :)","title":"Our open-source release!"},{"location":"open_source_release/#open-source-release","text":"Introduction by Matt Young.","title":"Open source release"},{"location":"open_source_release/#introduction","text":"Hi, and thanks for reading this! This year, 2020, has been a long one for everyone, RoboCup Jr teams included. Regardless of where you\u2019re from, COVID-19 is bound to have affected you in one way or another. At Team Omicron, it effected us quite significantly. The 2019 Internationals competition had left all team members with a lot to be desired, with many bugs to fix and new things to add. Thus, we all set our sights to the 2020 competition. We undertook many new research & development projects in the hope of finally bringing something (hopefully) game-changing to soccer. Our plan was, of course, to deploy this all at the 2020 RoboCup Jr Internationals in Bordeaux, France. Unfortunately, however, this competition was tragically cancelled due to the COVID-19 restrictions, and it\u2019s unlikely we\u2019ll will have the chance to go back next year in 2021 (should it even go ahead). This left us feeling extremely disappointed, and thus development and testing of the robot slowed down quite a lot. Although we never got a chance to make our difference in the official competitions, we at Team Omicron still want to give back to the community somehow, or do something with the hundreds of hours we\u2019ve individually spent designing, programming, testing and writing. As such, we\u2019ve decided to open source our entire project! This means that every single last bit of code, hardware (both structural & electrical), all formal documentation and even internal testing documents/notes will be released under permissive open source licences. We provide documentation below and on GitHub for how to get started. We\u2019re really excited about doing this, because we believe it\u2019s the largest scale open-source release of a relatively successful and long-running RoboCup Jr team. We hope that it\u2019s really beneficial to the community and allows teams new and old to grow, adapt and improve on their current setups. Although Omicron ourselves will never have the chance to deploy our new technology at a competition, we hope that others will enjoy the same honour, and all of our team members look forward to seeing what people come up with. If you have any questions or comments, shoot them our way - see the About page for contact details. For myself and everyone else on Team Omicron, it\u2019s been an amazing experience participating in RoboCup Jr over the years, even despite this year\u2019s many disasters. Anyway, that\u2019s it from me. Thanks, and have fun!","title":"Introduction"},{"location":"open_source_release/#getting-started","text":"To get started, simply install Git and clone our GitHub repo . This repo contains all our projects and heaps of documentation to get you started. Please read the README.md file all the way through, and branch out to individual projects from there.","title":"Getting started"},{"location":"open_source_release/#license","text":"","title":"License"},{"location":"open_source_release/#code","text":"TODO mention poster credit All code written by Team Omicron is released under the Mozilla Public License 2.0 (see LICENSE.txt in each directory). You will be able to tell which code is ours due to the presence of Omicron copyright notices at the top of the file. For information on this licence, please read this FAQ , and this question . Simply put, if you are building a robot based on Team Omicron\u2019s code, the MPL requires that you disclose that your robot uses open-source software from Team Omicron, and where anyone can obtain it (this repo). A great place to do so would be in your poster and presentation. If you modify any files with the MPL licence header, the licence requires that you release your improvements under the MPL 2.0 as well. New files that you create can be under any licence. We have decided to use the MPL because we believe it balances freedom of usage, while making sure that any improvements are released back to the RoboCup community for future teams to benefit from.","title":"Code"},{"location":"open_source_release/#designs","text":"TODO figure out designs licence. CC-BY-SA most likely.","title":"Designs"},{"location":"open_source_release/#some-final-thoughts","text":"While it\u2019s certainly possible to use, say, our camera project on your robot by simply downloading and compiling it, I personally think it\u2019s much more educational to use our design ideas and code as a baseline for your own projects . For example, we think that our novel localisation algorithm works pretty well. While it\u2019s certainly possible to use our implementation in Omicam on your robot directly (and there are instructions for doing so), I think it\u2019s much more impressive and educational to use that code as a method for creating your own implementation of our algorithms - or creating better algorithms that run faster and more accurately! That\u2019s the spirit of RoboCup, after all :)","title":"Some final thoughts"},{"location":"protobuf/","text":"Protocol Buffers Page author: Matt Young, vision systems & low-level developer Team Omicron inherits our use of Protocol Buffers, a Google-developed technology, from our team last year (Team Deus Vult, RoboCup Internationals 2019). However, we have greatly expanded on our usage of this technology in our project, as it\u2019s now used in almost all communications we do everywhere, as well as non-communication files such as replays and field files. Note: Some of this documentation is borrowed from the RoboCup 2019 Sydney submission by our previous team (Deus Vult). Introduction Low-level communication protocols such as UART and I2C operate on the byte level. This is a problem, because much of the data we work with is larger than one byte (max range of 0-255). Most teams resolve this issue by inventing a custom binary protocol, for example by turning a 16-bit integer into two 8-bit integers for transmisson which are then pieced back together on the receiving end. However, this is a complicated, difficult to program approach which is difficult to add new fields to in the future and only supports an extremely limited amount of data types that can be easily added by the programmers (usually 16-bit ints, 8-bit ints and booleans). Without documentation about the custom protocol used, future engineers on the project will struggle to identify what each byte in a byte stream means. Protocol Buffers (Protobuf) is a Google developed technology that allows easy and fast encoding of data (known as serialisation) to an arbitrary byte stream (in our case, a UART bus), and finally decoded (or deserialised) into the same useable information as before. As a cross-platform, cross-language serialisation framework, Protocol Buffers allows us to transmit many types of data such as floating points, up to 64 bit integers, strings and more. This can all be done simply by specifying the packet structure in a protocol definition file, importing the appropriate encoding/decoding libraries for the language in use and generating some boilerplate code using the Protobuf compiler. Thus, our intra-robot (i.e. between microcontroller) communications are reliable, efficient and easier to manage than ever before. Implementation in Omicron On the ESP32, LattePanda (via Omicam) and in the Teensy 3.5, we use Nanopb, which is an implementation of Protocol Buffers v3 for C. We selected this library compared to the alternatives because it\u2019s easy to use, space efficient and fast (about 1ms to encode a message, and a little bit more to decode it). This library, licensed under the Zlib license, is used to encode data to a byte stream (file, UART bus, etc) and decode it a later date. Data is stored in memory using a standard C struct. This struct is then fed to the Nanopb encoder which turns it into a byte stream. Usually, this byte stream is either written to disk (replays, field files) or transmitted over UART. At a later date, and possibly on a different device, the decoder then reads these bytes in, and uses them to reconstruct the same C struct that was sent. Compared to last year, after ditching the OpenMV we added Protocol Buffer support to Omicam, which it uses both in transmitting to the ESP32 over UART, and to Omicontrol over Wi-Fi. Likewise, Omicontrol uses Google\u2019s official Java Protobuf library to talk back to Omicam. Despite being two different libraries, due to the Protobuf spec being standardised, they can communicate successfully. Drawbacks As the encoding/decoding processes are long and complex, our weaker ATMega328P slave device doesn\u2019t use Protcol Buffers since it would be too slow and resource intensive.","title":"Protocol Buffers"},{"location":"protobuf/#protocol-buffers","text":"Page author: Matt Young, vision systems & low-level developer Team Omicron inherits our use of Protocol Buffers, a Google-developed technology, from our team last year (Team Deus Vult, RoboCup Internationals 2019). However, we have greatly expanded on our usage of this technology in our project, as it\u2019s now used in almost all communications we do everywhere, as well as non-communication files such as replays and field files. Note: Some of this documentation is borrowed from the RoboCup 2019 Sydney submission by our previous team (Deus Vult).","title":"Protocol Buffers"},{"location":"protobuf/#introduction","text":"Low-level communication protocols such as UART and I2C operate on the byte level. This is a problem, because much of the data we work with is larger than one byte (max range of 0-255). Most teams resolve this issue by inventing a custom binary protocol, for example by turning a 16-bit integer into two 8-bit integers for transmisson which are then pieced back together on the receiving end. However, this is a complicated, difficult to program approach which is difficult to add new fields to in the future and only supports an extremely limited amount of data types that can be easily added by the programmers (usually 16-bit ints, 8-bit ints and booleans). Without documentation about the custom protocol used, future engineers on the project will struggle to identify what each byte in a byte stream means. Protocol Buffers (Protobuf) is a Google developed technology that allows easy and fast encoding of data (known as serialisation) to an arbitrary byte stream (in our case, a UART bus), and finally decoded (or deserialised) into the same useable information as before. As a cross-platform, cross-language serialisation framework, Protocol Buffers allows us to transmit many types of data such as floating points, up to 64 bit integers, strings and more. This can all be done simply by specifying the packet structure in a protocol definition file, importing the appropriate encoding/decoding libraries for the language in use and generating some boilerplate code using the Protobuf compiler. Thus, our intra-robot (i.e. between microcontroller) communications are reliable, efficient and easier to manage than ever before.","title":"Introduction"},{"location":"protobuf/#implementation-in-omicron","text":"On the ESP32, LattePanda (via Omicam) and in the Teensy 3.5, we use Nanopb, which is an implementation of Protocol Buffers v3 for C. We selected this library compared to the alternatives because it\u2019s easy to use, space efficient and fast (about 1ms to encode a message, and a little bit more to decode it). This library, licensed under the Zlib license, is used to encode data to a byte stream (file, UART bus, etc) and decode it a later date. Data is stored in memory using a standard C struct. This struct is then fed to the Nanopb encoder which turns it into a byte stream. Usually, this byte stream is either written to disk (replays, field files) or transmitted over UART. At a later date, and possibly on a different device, the decoder then reads these bytes in, and uses them to reconstruct the same C struct that was sent. Compared to last year, after ditching the OpenMV we added Protocol Buffer support to Omicam, which it uses both in transmitting to the ESP32 over UART, and to Omicontrol over Wi-Fi. Likewise, Omicontrol uses Google\u2019s official Java Protobuf library to talk back to Omicam. Despite being two different libraries, due to the Protobuf spec being standardised, they can communicate successfully.","title":"Implementation in Omicron"},{"location":"protobuf/#drawbacks","text":"As the encoding/decoding processes are long and complex, our weaker ATMega328P slave device doesn\u2019t use Protcol Buffers since it would be too slow and resource intensive.","title":"Drawbacks"},{"location":"sbc/","text":"SBC & Camera Introduction Single board computers (SBCs) are becoming increasingly more popular in the RoboCup Jr Open league as a solution to the vision problem. While most teams use devices powered by microcontrollers such as the OpenMV or Pixy, due to their limited resources these cameras can be very slow and usually run at extremely low resolutions. This year, Team Omicron investigated using an SBC for our robot\u2019s vision system. This replaces our old vision system, the OpenMV H7. For more info, please see the Omicam page. Although in the past we had considered replacing all microcontrollers with an SBC, we decided that this would be too complicated. Instead, using an SBC in addition to a microcontroller (the ESP32) running an RTOS (FreeRTOS) worked the best for us. Instead, we focused effort on finding an SBC that would power our vision pipeline. It would need to achieve excellent performance, while not being too expensive. Our setup Currently, Team Omicron uses a LattePanda Delta 432 with a 2.4 GHz quad-core Intel Celeron N4100, 4GB RAM, 32GB of storage, WiFi, Bluetooth, gigabit Ethernet and a UART bus. The current camera we use is an e-con Systems Hyperyon , using the Sony Starvis IMX290 ultra low-light sensor capable of seeing in almost pitch black at high framerates. This is a USB 2.0 camera module, since the LattePanda has no MIPI port. SBC iterations The current iteration of Omicam\u2019s SBC setup is the cumulation of around 2 years of prototyping iterations in both hardware and software approaches. Prototype 1 (December 2018-January 2019): This consisted of a Raspberry Pi Zero W, with a 1 GHz single-core CPU. It was our initial prototype for a single-board computer, however, we quickly found it was far too weak to do any vision, and our inexperience at the time didn\u2019t help. Thus, we canned the SBC project for around a year. Prototype 2 (December 2019): After resurrecting the SBC R&D project for our 2020 Internationals, we started development with the Raspberry Pi 4. This has a 1.5 GHz quad-core ARM Cortex-A72 CPU. We began developing a custom computer vision library tailored specifically to our task, using the Pi\u2019s MMAL API for GPU-accelerated camera decoding. Initial results showed we could threshold images successfully, but we believed it would be too slow to localise and run a connected-component labeller in real time. Prototype 3 (January 2020): Next, we moved onto the NVIDIA Jetson Nano, containing a 1.43 GHz quad-core Cortex-A57, but far more importantly a 128-core Maxwell GPU. At this time we also switched to using OpenCV 4 for our computer vision. In theory, using the GPU for processing would lead to a huge performance boost due to the parallelism, however, in practice we observed the GPU was significantly slower than even the weaker Cortex-A43 CPU, (presumably) due to copying times. We were unable to optimise this to standard after weeks of development, thus we decided to move on from this prototype. Prototype 4 (January-February 2020): After much research, we decided to use the LattePanda Delta 432. The OpenCV pipeline is now entirely CPU bound, and despite not using the GPU at all, we are able to achieve good performance. Camera module iterations In addition to the SBC, the camera module has undergone many iterations, as it\u2019s also an important element in the vision pipeline. Pi Camera: The initial camera we used in hardware prototypes 1-3, was the Raspberry Pi Camera. In prototype 1, we used the Pi Cam v1.3 which is an OV5647 connected via MIPI, and in prototypes 2 and 3 we used the Pi Cam v2 which is an IMX219 again connected via MIPI. We had to drop this camera in later iterations because the LattePanda doesn\u2019t have a MIPI port. Both of these cameras were capable of around 720p at 60 fps. OV4689-based module: We experimented with a generic OV4689 USB 2.0 camera module from Amazon, which is capable of streaming JPEGs (aka an MJPEG stream) at 720p at 120 fps (we could get around 90 fps in practice with no processing). While this framerate was useful, the camera module suffered from noise and flickering in relatively good lighting conditions, so it was dropped. e-con Hyperyon: After these two failures, we began looking into industrial-grade cameras to use on our robot. While most of these, from companies like FLIR, are out of our price range, we found e-con Systems as a relatively affordable vendor of high quality cameras. We narrowed down our selection to two devices: the See3CAM_CU30 USB 2/USB 3 2MP camera module, which is fairly standard and affordable, as well as the more expensive Hyperyon described above. After testing both, we decided the Hyperon fit our needs better, despite its higher latency, due to its extremely good low light performance.","title":"SBC & Camera"},{"location":"sbc/#sbc-camera","text":"","title":"SBC &amp; Camera"},{"location":"sbc/#introduction","text":"Single board computers (SBCs) are becoming increasingly more popular in the RoboCup Jr Open league as a solution to the vision problem. While most teams use devices powered by microcontrollers such as the OpenMV or Pixy, due to their limited resources these cameras can be very slow and usually run at extremely low resolutions. This year, Team Omicron investigated using an SBC for our robot\u2019s vision system. This replaces our old vision system, the OpenMV H7. For more info, please see the Omicam page. Although in the past we had considered replacing all microcontrollers with an SBC, we decided that this would be too complicated. Instead, using an SBC in addition to a microcontroller (the ESP32) running an RTOS (FreeRTOS) worked the best for us. Instead, we focused effort on finding an SBC that would power our vision pipeline. It would need to achieve excellent performance, while not being too expensive.","title":"Introduction"},{"location":"sbc/#our-setup","text":"Currently, Team Omicron uses a LattePanda Delta 432 with a 2.4 GHz quad-core Intel Celeron N4100, 4GB RAM, 32GB of storage, WiFi, Bluetooth, gigabit Ethernet and a UART bus. The current camera we use is an e-con Systems Hyperyon , using the Sony Starvis IMX290 ultra low-light sensor capable of seeing in almost pitch black at high framerates. This is a USB 2.0 camera module, since the LattePanda has no MIPI port.","title":"Our setup"},{"location":"sbc/#sbc-iterations","text":"The current iteration of Omicam\u2019s SBC setup is the cumulation of around 2 years of prototyping iterations in both hardware and software approaches. Prototype 1 (December 2018-January 2019): This consisted of a Raspberry Pi Zero W, with a 1 GHz single-core CPU. It was our initial prototype for a single-board computer, however, we quickly found it was far too weak to do any vision, and our inexperience at the time didn\u2019t help. Thus, we canned the SBC project for around a year. Prototype 2 (December 2019): After resurrecting the SBC R&D project for our 2020 Internationals, we started development with the Raspberry Pi 4. This has a 1.5 GHz quad-core ARM Cortex-A72 CPU. We began developing a custom computer vision library tailored specifically to our task, using the Pi\u2019s MMAL API for GPU-accelerated camera decoding. Initial results showed we could threshold images successfully, but we believed it would be too slow to localise and run a connected-component labeller in real time. Prototype 3 (January 2020): Next, we moved onto the NVIDIA Jetson Nano, containing a 1.43 GHz quad-core Cortex-A57, but far more importantly a 128-core Maxwell GPU. At this time we also switched to using OpenCV 4 for our computer vision. In theory, using the GPU for processing would lead to a huge performance boost due to the parallelism, however, in practice we observed the GPU was significantly slower than even the weaker Cortex-A43 CPU, (presumably) due to copying times. We were unable to optimise this to standard after weeks of development, thus we decided to move on from this prototype. Prototype 4 (January-February 2020): After much research, we decided to use the LattePanda Delta 432. The OpenCV pipeline is now entirely CPU bound, and despite not using the GPU at all, we are able to achieve good performance.","title":"SBC iterations"},{"location":"sbc/#camera-module-iterations","text":"In addition to the SBC, the camera module has undergone many iterations, as it\u2019s also an important element in the vision pipeline. Pi Camera: The initial camera we used in hardware prototypes 1-3, was the Raspberry Pi Camera. In prototype 1, we used the Pi Cam v1.3 which is an OV5647 connected via MIPI, and in prototypes 2 and 3 we used the Pi Cam v2 which is an IMX219 again connected via MIPI. We had to drop this camera in later iterations because the LattePanda doesn\u2019t have a MIPI port. Both of these cameras were capable of around 720p at 60 fps. OV4689-based module: We experimented with a generic OV4689 USB 2.0 camera module from Amazon, which is capable of streaming JPEGs (aka an MJPEG stream) at 720p at 120 fps (we could get around 90 fps in practice with no processing). While this framerate was useful, the camera module suffered from noise and flickering in relatively good lighting conditions, so it was dropped. e-con Hyperyon: After these two failures, we began looking into industrial-grade cameras to use on our robot. While most of these, from companies like FLIR, are out of our price range, we found e-con Systems as a relatively affordable vendor of high quality cameras. We narrowed down our selection to two devices: the See3CAM_CU30 USB 2/USB 3 2MP camera module, which is fairly standard and affordable, as well as the more expensive Hyperyon described above. After testing both, we decided the Hyperon fit our needs better, despite its higher latency, due to its extremely good low light performance.","title":"Camera module iterations"},{"location":"strategy/","text":"Game strategies With the hardware features provided by our double kicker and dribbler, and the software features provided by Omicam and the FSM, our team was able to deploy advanced gameplay strategies Note: Due to COVID and development delays, game strategy was never properly completed. This documentation section is incomplete.","title":"Game strategies"},{"location":"strategy/#game-strategies","text":"With the hardware features provided by our double kicker and dribbler, and the software features provided by Omicam and the FSM, our team was able to deploy advanced gameplay strategies Note: Due to COVID and development delays, game strategy was never properly completed. This documentation section is incomplete.","title":"Game strategies"},{"location":"structural_design/","text":"Hardware Design Author(s): Ethan Lo, mechanical & electrical design This year we focused on designing a simple and robust robot which was quick to assemble and disassemble when needed. We also aimed on improving hardware from previous years including the custom omni-directional wheels introduced last year. The following are the major hardware improvements that have occurred this year: Custom omni-directional wheels Custom kicker unit Custom microcontroller circuits Custom camera hardware Mouse sensor implemenation Various debugging options for software to use Design Overview TODO: PUT PICTURE OF DESIGN HERE, POSSIBLE EXPLODED VIEW OR A360 LINK Omni-directional Wheels Omni-directional wheels (also referred to as omniwheels ) are specially designed to give robots full freedom of motion, enabling them to travel in any direction without turning. The downside to these wheels is the relative lack of tracktion and robustness when compared to conventional tires due to the individual rollers. As such, the primary goal of new iterations is to increase the grip and distance of the wheels. The following image shows the development of the omniwheel used throughout the years of competition with the rightmost being the latest design: TODO: PUT PICTURE HERE Past Iterations The two leftmost designs, the Kornylak and GTF omniwheel are stock designs bought pre-made from the manufacturer. Through using both these wheels for a year, we discovered that they were lacking in grip and robustness respectively, leading us to develop our own designs. The third design was our first attempt at a custom omniwheel, featuring double layered rollers and individual roller axles for faster repair. Though it allowed for much smoother movement and better traction, the wheels were large and unwieldly and the rollers still broke due to the use of softer silicon rollers. Furthermore, the use of 3D printed components caused rollers to fall out of the wheel during games. TODO: PUT PICTURES OF NATOINALS WHEEL, DOUBLE WHEEL WITH SMALL ROLLERS AND BIG WHEEL WITH SMALL ROLLERS Development of current wheel The leftmost design was a revised version of your first custom omniwheel. Various different materials were lasercut in order to replace the 3D printed components. After extensive testing, it was decided that PVC was to be used as it was durable, cheap and easy to laser cut. Alternatives materials included acrylic, which was not strong enough, and aluminium, which was too expensive to manufacture. The middle design is our second attempt at a custom omniwheel, once again featuring double layered rollers. However, this time smaller diameter rollers were used to allow the wheel to be much more compact. Consequently, this permitted the use of wheel protection guards, as seen in the image below. This prevented the wheels from getting damaged, often a result of the wheels grinding up against other robots or objects. In addition, the individual roller axels were replaced with metal wire to prevent the rollers from falling out. The rightmost design is a modified version of the middle design. Instead of having lots of smaller rollers, fewer longer rollers are used instead. This makes the wheel much easier to take apart and the rollers less likely to break. TODO: modify paragraph above once final design has been chosen to demonstrate the \u201cdevelopment flow\u201d TODO: Explain new design when it decides to exist Kicker Unit TODO: Idek if this works Chip-on-Board Microcontrollers Custom Vision System The primary goal for this year was to improve the vision system of the robot by developing a custom vision pipeline. This was achieved with the Debugging Further improvements upon the debugging options were made this year including: * 4 piezzoelectric buzzers for audio feedback * 8 LEDs and a text LCD screen for visual feedback * Wireless degbugging UI for easy visualisation","title":"Structural design"},{"location":"structural_design/#hardware-design","text":"Author(s): Ethan Lo, mechanical & electrical design This year we focused on designing a simple and robust robot which was quick to assemble and disassemble when needed. We also aimed on improving hardware from previous years including the custom omni-directional wheels introduced last year. The following are the major hardware improvements that have occurred this year: Custom omni-directional wheels Custom kicker unit Custom microcontroller circuits Custom camera hardware Mouse sensor implemenation Various debugging options for software to use","title":"Hardware Design"},{"location":"structural_design/#design-overview","text":"TODO: PUT PICTURE OF DESIGN HERE, POSSIBLE EXPLODED VIEW OR A360 LINK","title":"Design Overview"},{"location":"structural_design/#omni-directional-wheels","text":"Omni-directional wheels (also referred to as omniwheels ) are specially designed to give robots full freedom of motion, enabling them to travel in any direction without turning. The downside to these wheels is the relative lack of tracktion and robustness when compared to conventional tires due to the individual rollers. As such, the primary goal of new iterations is to increase the grip and distance of the wheels. The following image shows the development of the omniwheel used throughout the years of competition with the rightmost being the latest design: TODO: PUT PICTURE HERE","title":"Omni-directional Wheels"},{"location":"structural_design/#past-iterations","text":"The two leftmost designs, the Kornylak and GTF omniwheel are stock designs bought pre-made from the manufacturer. Through using both these wheels for a year, we discovered that they were lacking in grip and robustness respectively, leading us to develop our own designs. The third design was our first attempt at a custom omniwheel, featuring double layered rollers and individual roller axles for faster repair. Though it allowed for much smoother movement and better traction, the wheels were large and unwieldly and the rollers still broke due to the use of softer silicon rollers. Furthermore, the use of 3D printed components caused rollers to fall out of the wheel during games. TODO: PUT PICTURES OF NATOINALS WHEEL, DOUBLE WHEEL WITH SMALL ROLLERS AND BIG WHEEL WITH SMALL ROLLERS","title":"Past Iterations"},{"location":"structural_design/#development-of-current-wheel","text":"The leftmost design was a revised version of your first custom omniwheel. Various different materials were lasercut in order to replace the 3D printed components. After extensive testing, it was decided that PVC was to be used as it was durable, cheap and easy to laser cut. Alternatives materials included acrylic, which was not strong enough, and aluminium, which was too expensive to manufacture. The middle design is our second attempt at a custom omniwheel, once again featuring double layered rollers. However, this time smaller diameter rollers were used to allow the wheel to be much more compact. Consequently, this permitted the use of wheel protection guards, as seen in the image below. This prevented the wheels from getting damaged, often a result of the wheels grinding up against other robots or objects. In addition, the individual roller axels were replaced with metal wire to prevent the rollers from falling out. The rightmost design is a modified version of the middle design. Instead of having lots of smaller rollers, fewer longer rollers are used instead. This makes the wheel much easier to take apart and the rollers less likely to break. TODO: modify paragraph above once final design has been chosen to demonstrate the \u201cdevelopment flow\u201d TODO: Explain new design when it decides to exist","title":"Development of current wheel"},{"location":"structural_design/#kicker-unit","text":"TODO: Idek if this works","title":"Kicker Unit"},{"location":"structural_design/#chip-on-board-microcontrollers","text":"","title":"Chip-on-Board Microcontrollers"},{"location":"structural_design/#custom-vision-system","text":"The primary goal for this year was to improve the vision system of the robot by developing a custom vision pipeline. This was achieved with the","title":"Custom Vision System"},{"location":"structural_design/#debugging","text":"Further improvements upon the debugging options were made this year including: * 4 piezzoelectric buzzers for audio feedback * 8 LEDs and a text LCD screen for visual feedback * Wireless degbugging UI for easy visualisation","title":"Debugging"}]}