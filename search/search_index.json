{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BBC Robotics - Team Omicron Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys' College from Queensland, Australia, competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot's hardware, electrical and software design. Please feel free to browse at your own pleasure, and don't hesitate to contact us by visiting the About page. We're more than happy to answer any questions you may have about any aspect of our robots! You may also be interested in reading our Team Description Paper, available for reading here , although it doesn't contain anything not already listed on our website. Towards the end of 2020, our entire codebase including mechanical designs, electrical designs and all software will become open source for everyone to learn from and study. We hope you'll enjoy this opportunity, so please stay tuned.","title":"Home"},{"location":"#bbc-robotics-team-omicron","text":"Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys' College from Queensland, Australia, competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot's hardware, electrical and software design. Please feel free to browse at your own pleasure, and don't hesitate to contact us by visiting the About page. We're more than happy to answer any questions you may have about any aspect of our robots! You may also be interested in reading our Team Description Paper, available for reading here , although it doesn't contain anything not already listed on our website. Towards the end of 2020, our entire codebase including mechanical designs, electrical designs and all software will become open source for everyone to learn from and study. We hope you'll enjoy this opportunity, so please stay tuned.","title":"BBC Robotics - Team Omicron"},{"location":"about/","text":"About us About Team Omicron Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical & electrical deign, strategy code, docs Omicron TBA James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron TBA (TODO: will need to be fixed for stupid robocup rules) Our members are all very experienced, having competed in previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. We have been preparing for this competition for about (TODO: how many?) months and we estimate to have a combined (TODO: put hours here) hours developing and improving our current iteration of robots. Our current robots This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : our custom vision pipeline developed to replace the OpenMV H7. Programmed in C/C++, running on a LattePanda Delta 437, this application is capable of 720p@60fps (TODO: please confirm) field object tracking, and centimetre accurate field localisation using only camera data (no LRFs, etc) via a novel approach that uses non-linear optimisation algorithms. Omicontrol : our custom, wireless, all-in-one robot control software. Written in Kotlin, this program is used to tune our cameras, move robots around and visualise sensor data. Mouse sensor and velocity control : our robots have fully working PWM-3360 mouse sensors, which we use to accurately estimate and control our robot's velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : using the above projects, our movement software developers have combined highly accurate localisation data with precise velocity control to enable the execution of advanced ball-manipulation strategies on the field. Wheels(?) Double dribbler and kicker(?) Some technologies we build upon this year include: FSM : last year, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a novel method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \"switched\" through. This year, we continue to build upon this technology, introducing more state machines and more states. Protocol Buffers: last year, one of our original teams won the Innovation Prize for its use of Protocol Buffers, a Google-developed technology allowing the easy and fast transmission of data between devices. This year, we improve on this technology by using Protocol Buffers in our complex device network. PCBs (modular?) Wheels","title":"About"},{"location":"about/#about-us","text":"","title":"About us"},{"location":"about/#about-team-omicron","text":"Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical & electrical deign, strategy code, docs Omicron TBA James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron TBA (TODO: will need to be fixed for stupid robocup rules) Our members are all very experienced, having competed in previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. We have been preparing for this competition for about (TODO: how many?) months and we estimate to have a combined (TODO: put hours here) hours developing and improving our current iteration of robots.","title":"About Team Omicron"},{"location":"about/#our-current-robots","text":"This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : our custom vision pipeline developed to replace the OpenMV H7. Programmed in C/C++, running on a LattePanda Delta 437, this application is capable of 720p@60fps (TODO: please confirm) field object tracking, and centimetre accurate field localisation using only camera data (no LRFs, etc) via a novel approach that uses non-linear optimisation algorithms. Omicontrol : our custom, wireless, all-in-one robot control software. Written in Kotlin, this program is used to tune our cameras, move robots around and visualise sensor data. Mouse sensor and velocity control : our robots have fully working PWM-3360 mouse sensors, which we use to accurately estimate and control our robot's velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : using the above projects, our movement software developers have combined highly accurate localisation data with precise velocity control to enable the execution of advanced ball-manipulation strategies on the field. Wheels(?) Double dribbler and kicker(?) Some technologies we build upon this year include: FSM : last year, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a novel method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \"switched\" through. This year, we continue to build upon this technology, introducing more state machines and more states. Protocol Buffers: last year, one of our original teams won the Innovation Prize for its use of Protocol Buffers, a Google-developed technology allowing the easy and fast transmission of data between devices. This year, we improve on this technology by using Protocol Buffers in our complex device network. PCBs (modular?) Wheels","title":"Our current robots"},{"location":"omicam/","text":"Omicam One of the biggest innovation Team Omicron brings this year is our advanced, all-in-one, custom developed vision and localisation application called Omicam . The application is developed mostly in C, with some C++ code to interface with OpenCV. Performance, results Omicam is capable of detecting 4 field objects at over 60fps at 720p (1280x720) resolution . Compared to the previous OpenMV H7, this is 23x higher resolution at 2x the framerate. In addition, using the novel vision-based localisation algorithm we developed this year, we can determine the robot's position to ~1cm accuracy at realtime speeds. This is over 5x/25x ^ more accurate than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable. ^ depending on whether LRF based/goal based localisation was used To achieve this performance, we made heavy use of parallel programming techniques, OpenCV's x86 SIMD CPU optimisations, Clang optimiser flags as well as intelligent image downscaling for the goal threshold frames (as they are mostly unused). In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application is asynchronous. Hardware Omicam runs on a Linux-based LattePanda Delta 432. More here, camera and stuff. Vision The primary responsibility of Omicam is to detect field objects: the ball, goals and also lines. To do this, we use the popular computer vision library OpenCV, v4. We use Linux's UVC driver to acquire an MJPEG stream from the USB camera. Then, we apply any pre-processing steps such as downscaling the goal frames and gamma boosting. Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV's thresholding function but a custom parallel framework (as it doesn't run in parallel by default). Then, we use OpenCV's parallel connected component labeller, specifically the algorithm by Grana et al[1] to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. We then dispatch the largest detected blob's centroid via UART to the ESP32, encoded using Protocol Buffers. Localisation Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball. Previous methods Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals in the camera to estimate the robot's position. This approach is very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in super team). Expected accuracy is 15-25cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects, such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor, which is impossible to know. The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard this as not an ideal approach. Our solution This year, Team Omicron presents a novel approach to robot localisation based on the middle-size league paper by Lu, Li, Zhang, Hu & Zheng (2013). We localise using purely vision data by solving a non-linear optimisation problem using the lines on the playing field. In practice, this works by the vision processor passing its thresholded line data to the localiser. Using its worker threads, a certain number of rays are casted on the line image using Bresenham's line algorithm[3] to find every time a ray intersects a line (these are called \"line points\"). Using a manually determined equation, these points are then dewarped to counter the distortion of the 360 degree mirror. To localise, the Subplex[4] non-linear optimiser (part of the NLopt package[5]) is used to minimise an objective function that TODO describe what it does in simple terms. In simple terms, we optimise by moving a virtual robot around on a field and comparing the distances between what it might see and what we really see to \"reverse-engineer\" the (x,y) position of the real robot. Interfacing with Omicontrol Debugging Low-level compiled languages such as C and C++ are notoriously unstable. In order to improve the stability of Omicam and fix bugs, we used Google's Address Sanitizer to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. In addition, we used the LLVM toolchain's debugger lldb to analyse the application frequently. References TODO USE MENDELEY [1] C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-based Connected Components Labeling with Decision Trees,\u201d IEEE Transac-tions on Image Processing, vol. 19, no. 6, pp. 1596\u20131609, 2010. [2] TODO CITE PROPERLY Robust and Real-time Self-Localization Based on Omnidirectional Vision for Soccer Robots [3] Bresenham's paper [4] Subplex paper [5] NLopt paper","title":"Omicam (vision and localisastion)"},{"location":"omicam/#omicam","text":"One of the biggest innovation Team Omicron brings this year is our advanced, all-in-one, custom developed vision and localisation application called Omicam . The application is developed mostly in C, with some C++ code to interface with OpenCV.","title":"Omicam"},{"location":"omicam/#performance-results","text":"Omicam is capable of detecting 4 field objects at over 60fps at 720p (1280x720) resolution . Compared to the previous OpenMV H7, this is 23x higher resolution at 2x the framerate. In addition, using the novel vision-based localisation algorithm we developed this year, we can determine the robot's position to ~1cm accuracy at realtime speeds. This is over 5x/25x ^ more accurate than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable. ^ depending on whether LRF based/goal based localisation was used To achieve this performance, we made heavy use of parallel programming techniques, OpenCV's x86 SIMD CPU optimisations, Clang optimiser flags as well as intelligent image downscaling for the goal threshold frames (as they are mostly unused). In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application is asynchronous.","title":"Performance, results"},{"location":"omicam/#hardware","text":"Omicam runs on a Linux-based LattePanda Delta 432. More here, camera and stuff.","title":"Hardware"},{"location":"omicam/#vision","text":"The primary responsibility of Omicam is to detect field objects: the ball, goals and also lines. To do this, we use the popular computer vision library OpenCV, v4. We use Linux's UVC driver to acquire an MJPEG stream from the USB camera. Then, we apply any pre-processing steps such as downscaling the goal frames and gamma boosting. Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV's thresholding function but a custom parallel framework (as it doesn't run in parallel by default). Then, we use OpenCV's parallel connected component labeller, specifically the algorithm by Grana et al[1] to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. We then dispatch the largest detected blob's centroid via UART to the ESP32, encoded using Protocol Buffers.","title":"Vision"},{"location":"omicam/#localisation","text":"Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball.","title":"Localisation"},{"location":"omicam/#previous-methods","text":"Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals in the camera to estimate the robot's position. This approach is very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in super team). Expected accuracy is 15-25cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects, such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor, which is impossible to know. The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard this as not an ideal approach.","title":"Previous methods"},{"location":"omicam/#our-solution","text":"This year, Team Omicron presents a novel approach to robot localisation based on the middle-size league paper by Lu, Li, Zhang, Hu & Zheng (2013). We localise using purely vision data by solving a non-linear optimisation problem using the lines on the playing field. In practice, this works by the vision processor passing its thresholded line data to the localiser. Using its worker threads, a certain number of rays are casted on the line image using Bresenham's line algorithm[3] to find every time a ray intersects a line (these are called \"line points\"). Using a manually determined equation, these points are then dewarped to counter the distortion of the 360 degree mirror. To localise, the Subplex[4] non-linear optimiser (part of the NLopt package[5]) is used to minimise an objective function that TODO describe what it does in simple terms. In simple terms, we optimise by moving a virtual robot around on a field and comparing the distances between what it might see and what we really see to \"reverse-engineer\" the (x,y) position of the real robot.","title":"Our solution"},{"location":"omicam/#interfacing-with-omicontrol","text":"","title":"Interfacing with Omicontrol"},{"location":"omicam/#debugging","text":"Low-level compiled languages such as C and C++ are notoriously unstable. In order to improve the stability of Omicam and fix bugs, we used Google's Address Sanitizer to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. In addition, we used the LLVM toolchain's debugger lldb to analyse the application frequently.","title":"Debugging"},{"location":"omicam/#references","text":"TODO USE MENDELEY [1] C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-based Connected Components Labeling with Decision Trees,\u201d IEEE Transac-tions on Image Processing, vol. 19, no. 6, pp. 1596\u20131609, 2010. [2] TODO CITE PROPERLY Robust and Real-time Self-Localization Based on Omnidirectional Vision for Soccer Robots [3] Bresenham's paper [4] Subplex paper [5] NLopt paper","title":"References"},{"location":"omicontrol/","text":"Omicontrol","title":"Omicontrol (debugging and control)"},{"location":"omicontrol/#omicontrol","text":"","title":"Omicontrol"},{"location":"protobuf/","text":"Protocol Buffers","title":"Protocol Buffers"},{"location":"protobuf/#protocol-buffers","text":"","title":"Protocol Buffers"},{"location":"strategy/","text":"","title":"Game strategies"},{"location":"structural_design/","text":"Structural Design","title":"Structural design"},{"location":"structural_design/#structural-design","text":"","title":"Structural Design"}]}