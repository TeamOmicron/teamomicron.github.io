{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BBC Robotics - Team Omicron Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys' College in Queensland, Australia, competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot's hardware, electrical and software design. Please feel free to browse at your own pleasure, and don't hesitate to contact us by visiting the About page. We're more than happy to answer any questions you may have about any aspect of our robots! You may also be interested in reading our team description paper , although it doesn't contain anything not already listed on our website. Towards the end of 2020, our entire codebase including mechanical designs, electrical designs and all software will become open source for everyone to learn from and study. We hope you'll enjoy this opportunity, so please stay tuned.","title":"Home"},{"location":"#bbc-robotics-team-omicron","text":"Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys' College in Queensland, Australia, competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot's hardware, electrical and software design. Please feel free to browse at your own pleasure, and don't hesitate to contact us by visiting the About page. We're more than happy to answer any questions you may have about any aspect of our robots! You may also be interested in reading our team description paper , although it doesn't contain anything not already listed on our website. Towards the end of 2020, our entire codebase including mechanical designs, electrical designs and all software will become open source for everyone to learn from and study. We hope you'll enjoy this opportunity, so please stay tuned.","title":"BBC Robotics - Team Omicron"},{"location":"about/","text":"About us About Team Omicron Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical & electrical deign, strategy code, docs Omicron TBA James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron TBA (TODO: will need to be fixed for stupid robocup rules) Our members are all very experienced, having competed in previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. We have been preparing for this competition for about (TODO: how many?) months and we estimate to have a combined (TODO: put hours here) hours developing and improving our current iteration of robots. Our current robots This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : our custom vision pipeline developed to replace the OpenMV H7. Programmed in C/C++, running on a LattePanda Delta 432, this application is capable of 720p@60fps (TODO: please confirm) field object tracking, and centimetre accurate field localisation using only camera data (no LRFs, etc) via a novel approach that uses non-linear optimisation algorithms. Omicontrol : our custom, wireless, all-in-one robot control software. Written in Kotlin, this program is used to tune our cameras, move robots around and visualise sensor data. Mouse sensor and velocity control : our robots have fully working PWM-3360 mouse sensors, which we use to accurately estimate and control our robot's velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : using the above projects, our movement software developers have combined highly accurate localisation data with precise velocity control to enable the execution of advanced ball-manipulation strategies on the field. Wheels(?) Double dribbler and kicker(?) Some technologies we build upon this year include: FSM : last year, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a novel method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \"switched\" through. This year, we continue to build upon this technology, introducing more state machines and more states. Protocol Buffers: at the 2019 Internationals, one of our founding teams (Deus Vult/Omicron) won the Innovation Prize for its use of Protocol Buffers, a Google-developed technology allowing the easy and fast transmission of data between devices. This year, we improve on this technology by using Protocol Buffers in our complex device network. PCBs (modular?) Wheels","title":"About"},{"location":"about/#about-us","text":"","title":"About us"},{"location":"about/#about-team-omicron","text":"Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical & electrical deign, strategy code, docs Omicron TBA James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron TBA (TODO: will need to be fixed for stupid robocup rules) Our members are all very experienced, having competed in previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. We have been preparing for this competition for about (TODO: how many?) months and we estimate to have a combined (TODO: put hours here) hours developing and improving our current iteration of robots.","title":"About Team Omicron"},{"location":"about/#our-current-robots","text":"This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : our custom vision pipeline developed to replace the OpenMV H7. Programmed in C/C++, running on a LattePanda Delta 432, this application is capable of 720p@60fps (TODO: please confirm) field object tracking, and centimetre accurate field localisation using only camera data (no LRFs, etc) via a novel approach that uses non-linear optimisation algorithms. Omicontrol : our custom, wireless, all-in-one robot control software. Written in Kotlin, this program is used to tune our cameras, move robots around and visualise sensor data. Mouse sensor and velocity control : our robots have fully working PWM-3360 mouse sensors, which we use to accurately estimate and control our robot's velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : using the above projects, our movement software developers have combined highly accurate localisation data with precise velocity control to enable the execution of advanced ball-manipulation strategies on the field. Wheels(?) Double dribbler and kicker(?) Some technologies we build upon this year include: FSM : last year, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a novel method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \"switched\" through. This year, we continue to build upon this technology, introducing more state machines and more states. Protocol Buffers: at the 2019 Internationals, one of our founding teams (Deus Vult/Omicron) won the Innovation Prize for its use of Protocol Buffers, a Google-developed technology allowing the easy and fast transmission of data between devices. This year, we improve on this technology by using Protocol Buffers in our complex device network. PCBs (modular?) Wheels","title":"Our current robots"},{"location":"omicam/","text":"Omicam One of the biggest innovation Team Omicron brings this year is our advanced, custom developed vision and localisation application called Omicam . The application is developed mostly in C, with some C++ code to interface with OpenCV. It runs on the powerful LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS. We are proud to report that Omicam is a significant step up compared to previous vision applications in use at BBC Robotics, as shown below. Omicam consists of X lines of code, and took about X hours to develop. Background and previous methods Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer. With the introduction of the orange ball and advanced teams' application of \"ball-hiding\" strategies, accurate and fast computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the best solution was to do what the most advanced teams were doing, and develop a custom vision application running on a single board computer (SBC). TODO cover Omicam development with other SBCs Performance and results Omicam is capable of detecting 4 field objects at over 60fps at 720p (1280x720) resolution . Compared to the previous OpenMV H7, this is 23x higher resolution at 3x the framerate. 1 In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the robot's position to ~1cm accuracy at realtime speeds. This is over 5x/25x more accurate 2 than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable. To achieve this performance, we made heavy use of parallel programming techniques, OpenCV's x86 SIMD CPU optimisations, Clang optimiser flags as well as intelligent image downscaling for the goal threshold images (as they are mostly unused). In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application is asynchronous. 1 results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution. 2 depending on whether LRF based/goal based localisation was used. Hardware Omicam runs on a Linux-based LattePanda Delta 432. It uses an OV4689 USB 2.0 camera module, which is capable of streaming motion JPEG (MJPEG) at 720p 100+ fps. More here, justifcations for why we picked this hardware. Field object detection The primary responsibility of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. To do this, we use the popular computer vision library OpenCV (v4.2.0). We use Linux's UVC driver to acquire an MJPEG stream from the USB camera. Then, we apply any pre-processing steps such as downscaling the goal frames and gamma boosting. Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV's inRange thresholder but a custom parallel framework (as it doesn't run in parallel by default). Thresholding generates a 1-bit binary mask of the image, where each pixel is 255 (true) if it's inside the RGB value specified, and 0 (false) if it's not. Then, we use OpenCV's parallel connected component labeller, specifically the BBDT algorithm[1] to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. OpenCV automatically calculates the bounding box and centroid for each of these connected regions. We then dispatch the largest detected blob's centroid via UART to the ESP32, encoded using Protocol Buffers and using POSIX termios for UART configuration. Localisation Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball. Previous methods Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals in the camera to estimate the robot's position. This approach is very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in super team). Expected accuracy is 15-25cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects, such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor, which is impossible to know. The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard this as not an ideal approach. Our solution This year, Team Omicron presents a novel approach to robot localisation based on a middle-size league paper by Lu, Li, Zhang, Hu & Zheng (2013). We localise using purely RGB camera data by solving a non-linear optimisation problem using the lines on the playing field. The principle method of operation of our algorithm is that we need to match a virtual model of field geometry to the observed one from the camera. If we match the lines so that they align in both the virtual model and real-world model, then we can infer that the calculated virtual robot coordinates are the same as the real, unknown robot coordinates. Essentially, we're taking what we know: the static layout of the field, and observed field geometry at our current position, and using it to infer the unknown 2D position vector. We divide our new localisation method into three main sub-processes: Image analysis Camera normalisation Coordinate optimisation Image analysis The localiser's input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding for the colour white, which is handled by the vision pipeline described earlier. With the input provided, a certain number of rays (usually 64) are casted over the line image using a modified version Bresenham's line algorithm[3] to find every time a ray intersects a line (these are called \"line points\"). We modified Bresenham's original algorithm so that it works with rays instead of lines, and terminates when it reaches the edge of the image instead of when it's finished drawing a line. Camera normalisation These points are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by measuring the pixels between points along a ruler and comparing this to the real centimetre distances on the ruler. Using a regression software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances, and thus the mirror distortion can be countered. Below we demonstrate the result of dewarping an entire image (using the output of the OpenMV H7), which we deemed to inefficient to run in realtime. This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres. Figure 1: example of frame dewarping from the old, low reolution OpenMV H7 The second phase of the camera normalisation is to rotate the points relative to the robot's heading. The robot's heading value, which is relative to when it was powered on, is transmitted by the ESP32, again using Protocol Buffers. It is calculated using the accurate BNO055 IMU using IMUPLUS (sensor fusion between accelerometer and gyroscope) mode. For more information, see the ESP32 and movement code page. Position optimisation The main part of our solution is the Subplex[4] local derivative-free non-linear optimiser, re-implemented as part of the NLopt package[5]. This algorithm essentially acts as an efficiency and stability improvement over the well-known Nelder-Mead Simplex algorithm. The most critical part of this process is the objective function , which is a function that takes an N-dimensional vector (in our case, an estimated 2D position) and calculates essentially a \"score\" of how accurate the value is. This objective function must be highly optimised as it could be evaluated thousands of times by the optimisation algorithm. We spent a great deal of effort drafting the most efficient objective function, and the approach we used makes heavy use of pre-computation via a \"field file\". This field file is a binary Protcol Buffer file that encodes the geometry of any RoboCup field by dividing it into a grid, where each cell contains the distance in centimetres to the closest line. Increasing the resolution of the grid will increase its accuracy, but also significantly increase its file size. We use a 1cm grid, which stores 44,226 cells and is 172 KiB on disk and takes about 2 seconds to generate on a fast desktop computer. The field file is generated by a Python script which can be easily modified to support an arbitrary number of different field layouts, such as super team or the Australian field (without the goalie box). The objective function essentially works as follows: For each line point, snap it to the nearest grid cell unit and look it up in the field file to determine the error to the real line distance. ( TODO: better explanation needed ) Sum all these errors to produce a total error. By running most computations outside the objective function, we greatly increase the speed of the localisation. As the total error approaches zero, the field position becomes more and more accurate. Ideally, an error of 0.0 would mean that the estimate is as accurate as possible given the current field file's grid unit. Although a derivative-based algorithm may be more efficient at solving the problem, we deemed it far too difficult to calculate the derivative of the objective function. Adding new sensor data to the world model Our localisation algorithm is also flexible with new sensor inputs. Because the use a \"virtual world model\" approach, if any sensors can be modelled as a measurement of a distance to a static object, then they can be integrated as extra data in the world model. This is extremely helpful when vision data runs into limitations, for example, in SuperTeam where it may be difficult to see enough field lines in one frame due to the size of the field. We integrate the LRFs on the robot as extra distance sensors, using their reported distance to the field walls just as we use the raycasted \"line points\" in the vision based approach. It would also be possible to use the distance to the goals as virtual information to supplement a potential lack of line data and thus increase accuracy. Other distance sensors such as 360 LiDARS can also be implemented with ease. Interfacing with Omicontrol To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as the temperature of the SBC. We use the SIMD optimised libjpeg-turbo to efficiently encode JPEG frames, so as to not waste performance to the remote debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit images, it was determined that zlib could compress them more efficiently (around about 460,800x reduction in size). With all these optimisations, even at high framerates (60+ packets per second), the remote debug system only uses 300KB/s to 1 MB/s of network bandwidth. Debugging and performance optimisation Low-level compiled languages such as C and C++ are notoriously unstable. In order to improve the stability of Omicam and fix bugs, we used Google's Address Sanitizer to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. In addition, we used the LLVM toolchain's debugger lldb (or just gdb) to analyse the application frequently. To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application. Although the Clang compiler may have marginally worse performance than GCC, we chose Clang because it's more modern and has better debugging support (namely, GCC's Adress Sanitizer implementation is broken for us). To improve performance of the localiser, we use the last extrapolated position from the mouse sensor as a seed for the initial position of the next search. This means instead of starting from a random position, the localiser will complete much quickly a it's already relatively close to the true position. Also cover Linux CPU optimisation and associated thermal issues if relevant References [1] C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-Based Connected Components Labeling With Decision Trees,\u201d IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596\u20131609, 2010, doi: 10.1109/TIP.2010.2044963. [2] H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, \u201cRobust and real-time self-localization based on omnidirectional vision for soccer robots,\u201d Adv. Robot., vol. 27, no. 10, pp. 799\u2013811, Jul. 2013, doi: 10.1080/01691864.2013.785473. [3] J. E. Bresenhman, \u201cAlgorithm for computer control of a digital plotter,\u201d IBM Syst. J., vol. 4, no. 1, pp. 25\u201330, 1965. [4] T. H. Rowan, \u201cFunctional stability analysis of numerical algorithms,\u201d Unpuplished Diss., p. 218, 1990. [5] Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt","title":"Omicam (vision and localisastion)"},{"location":"omicam/#omicam","text":"One of the biggest innovation Team Omicron brings this year is our advanced, custom developed vision and localisation application called Omicam . The application is developed mostly in C, with some C++ code to interface with OpenCV. It runs on the powerful LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS. We are proud to report that Omicam is a significant step up compared to previous vision applications in use at BBC Robotics, as shown below. Omicam consists of X lines of code, and took about X hours to develop.","title":"Omicam"},{"location":"omicam/#background-and-previous-methods","text":"Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer. With the introduction of the orange ball and advanced teams' application of \"ball-hiding\" strategies, accurate and fast computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the best solution was to do what the most advanced teams were doing, and develop a custom vision application running on a single board computer (SBC). TODO cover Omicam development with other SBCs","title":"Background and previous methods"},{"location":"omicam/#performance-and-results","text":"Omicam is capable of detecting 4 field objects at over 60fps at 720p (1280x720) resolution . Compared to the previous OpenMV H7, this is 23x higher resolution at 3x the framerate. 1 In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the robot's position to ~1cm accuracy at realtime speeds. This is over 5x/25x more accurate 2 than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable. To achieve this performance, we made heavy use of parallel programming techniques, OpenCV's x86 SIMD CPU optimisations, Clang optimiser flags as well as intelligent image downscaling for the goal threshold images (as they are mostly unused). In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application is asynchronous. 1 results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution. 2 depending on whether LRF based/goal based localisation was used.","title":"Performance and results"},{"location":"omicam/#hardware","text":"Omicam runs on a Linux-based LattePanda Delta 432. It uses an OV4689 USB 2.0 camera module, which is capable of streaming motion JPEG (MJPEG) at 720p 100+ fps. More here, justifcations for why we picked this hardware.","title":"Hardware"},{"location":"omicam/#field-object-detection","text":"The primary responsibility of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. To do this, we use the popular computer vision library OpenCV (v4.2.0). We use Linux's UVC driver to acquire an MJPEG stream from the USB camera. Then, we apply any pre-processing steps such as downscaling the goal frames and gamma boosting. Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV's inRange thresholder but a custom parallel framework (as it doesn't run in parallel by default). Thresholding generates a 1-bit binary mask of the image, where each pixel is 255 (true) if it's inside the RGB value specified, and 0 (false) if it's not. Then, we use OpenCV's parallel connected component labeller, specifically the BBDT algorithm[1] to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. OpenCV automatically calculates the bounding box and centroid for each of these connected regions. We then dispatch the largest detected blob's centroid via UART to the ESP32, encoded using Protocol Buffers and using POSIX termios for UART configuration.","title":"Field object detection"},{"location":"omicam/#localisation","text":"Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball.","title":"Localisation"},{"location":"omicam/#previous-methods","text":"Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals in the camera to estimate the robot's position. This approach is very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in super team). Expected accuracy is 15-25cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects, such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor, which is impossible to know. The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard this as not an ideal approach.","title":"Previous methods"},{"location":"omicam/#our-solution","text":"This year, Team Omicron presents a novel approach to robot localisation based on a middle-size league paper by Lu, Li, Zhang, Hu & Zheng (2013). We localise using purely RGB camera data by solving a non-linear optimisation problem using the lines on the playing field. The principle method of operation of our algorithm is that we need to match a virtual model of field geometry to the observed one from the camera. If we match the lines so that they align in both the virtual model and real-world model, then we can infer that the calculated virtual robot coordinates are the same as the real, unknown robot coordinates. Essentially, we're taking what we know: the static layout of the field, and observed field geometry at our current position, and using it to infer the unknown 2D position vector. We divide our new localisation method into three main sub-processes: Image analysis Camera normalisation Coordinate optimisation","title":"Our solution"},{"location":"omicam/#image-analysis","text":"The localiser's input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding for the colour white, which is handled by the vision pipeline described earlier. With the input provided, a certain number of rays (usually 64) are casted over the line image using a modified version Bresenham's line algorithm[3] to find every time a ray intersects a line (these are called \"line points\"). We modified Bresenham's original algorithm so that it works with rays instead of lines, and terminates when it reaches the edge of the image instead of when it's finished drawing a line.","title":"Image analysis"},{"location":"omicam/#camera-normalisation","text":"These points are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by measuring the pixels between points along a ruler and comparing this to the real centimetre distances on the ruler. Using a regression software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances, and thus the mirror distortion can be countered. Below we demonstrate the result of dewarping an entire image (using the output of the OpenMV H7), which we deemed to inefficient to run in realtime. This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres. Figure 1: example of frame dewarping from the old, low reolution OpenMV H7 The second phase of the camera normalisation is to rotate the points relative to the robot's heading. The robot's heading value, which is relative to when it was powered on, is transmitted by the ESP32, again using Protocol Buffers. It is calculated using the accurate BNO055 IMU using IMUPLUS (sensor fusion between accelerometer and gyroscope) mode. For more information, see the ESP32 and movement code page.","title":"Camera normalisation"},{"location":"omicam/#position-optimisation","text":"The main part of our solution is the Subplex[4] local derivative-free non-linear optimiser, re-implemented as part of the NLopt package[5]. This algorithm essentially acts as an efficiency and stability improvement over the well-known Nelder-Mead Simplex algorithm. The most critical part of this process is the objective function , which is a function that takes an N-dimensional vector (in our case, an estimated 2D position) and calculates essentially a \"score\" of how accurate the value is. This objective function must be highly optimised as it could be evaluated thousands of times by the optimisation algorithm. We spent a great deal of effort drafting the most efficient objective function, and the approach we used makes heavy use of pre-computation via a \"field file\". This field file is a binary Protcol Buffer file that encodes the geometry of any RoboCup field by dividing it into a grid, where each cell contains the distance in centimetres to the closest line. Increasing the resolution of the grid will increase its accuracy, but also significantly increase its file size. We use a 1cm grid, which stores 44,226 cells and is 172 KiB on disk and takes about 2 seconds to generate on a fast desktop computer. The field file is generated by a Python script which can be easily modified to support an arbitrary number of different field layouts, such as super team or the Australian field (without the goalie box). The objective function essentially works as follows: For each line point, snap it to the nearest grid cell unit and look it up in the field file to determine the error to the real line distance. ( TODO: better explanation needed ) Sum all these errors to produce a total error. By running most computations outside the objective function, we greatly increase the speed of the localisation. As the total error approaches zero, the field position becomes more and more accurate. Ideally, an error of 0.0 would mean that the estimate is as accurate as possible given the current field file's grid unit. Although a derivative-based algorithm may be more efficient at solving the problem, we deemed it far too difficult to calculate the derivative of the objective function.","title":"Position optimisation"},{"location":"omicam/#adding-new-sensor-data-to-the-world-model","text":"Our localisation algorithm is also flexible with new sensor inputs. Because the use a \"virtual world model\" approach, if any sensors can be modelled as a measurement of a distance to a static object, then they can be integrated as extra data in the world model. This is extremely helpful when vision data runs into limitations, for example, in SuperTeam where it may be difficult to see enough field lines in one frame due to the size of the field. We integrate the LRFs on the robot as extra distance sensors, using their reported distance to the field walls just as we use the raycasted \"line points\" in the vision based approach. It would also be possible to use the distance to the goals as virtual information to supplement a potential lack of line data and thus increase accuracy. Other distance sensors such as 360 LiDARS can also be implemented with ease.","title":"Adding new sensor data to the world model"},{"location":"omicam/#interfacing-with-omicontrol","text":"To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as the temperature of the SBC. We use the SIMD optimised libjpeg-turbo to efficiently encode JPEG frames, so as to not waste performance to the remote debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit images, it was determined that zlib could compress them more efficiently (around about 460,800x reduction in size). With all these optimisations, even at high framerates (60+ packets per second), the remote debug system only uses 300KB/s to 1 MB/s of network bandwidth.","title":"Interfacing with Omicontrol"},{"location":"omicam/#debugging-and-performance-optimisation","text":"Low-level compiled languages such as C and C++ are notoriously unstable. In order to improve the stability of Omicam and fix bugs, we used Google's Address Sanitizer to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. In addition, we used the LLVM toolchain's debugger lldb (or just gdb) to analyse the application frequently. To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application. Although the Clang compiler may have marginally worse performance than GCC, we chose Clang because it's more modern and has better debugging support (namely, GCC's Adress Sanitizer implementation is broken for us). To improve performance of the localiser, we use the last extrapolated position from the mouse sensor as a seed for the initial position of the next search. This means instead of starting from a random position, the localiser will complete much quickly a it's already relatively close to the true position. Also cover Linux CPU optimisation and associated thermal issues if relevant","title":"Debugging and performance optimisation"},{"location":"omicam/#references","text":"[1] C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-Based Connected Components Labeling With Decision Trees,\u201d IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596\u20131609, 2010, doi: 10.1109/TIP.2010.2044963. [2] H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, \u201cRobust and real-time self-localization based on omnidirectional vision for soccer robots,\u201d Adv. Robot., vol. 27, no. 10, pp. 799\u2013811, Jul. 2013, doi: 10.1080/01691864.2013.785473. [3] J. E. Bresenhman, \u201cAlgorithm for computer control of a digital plotter,\u201d IBM Syst. J., vol. 4, no. 1, pp. 25\u201330, 1965. [4] T. H. Rowan, \u201cFunctional stability analysis of numerical algorithms,\u201d Unpuplished Diss., p. 218, 1990. [5] Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt","title":"References"},{"location":"omicontrol/","text":"Omicontrol","title":"Omicontrol (debugging and control)"},{"location":"omicontrol/#omicontrol","text":"","title":"Omicontrol"},{"location":"protobuf/","text":"Protocol Buffers","title":"Protocol Buffers"},{"location":"protobuf/#protocol-buffers","text":"","title":"Protocol Buffers"},{"location":"strategy/","text":"Game strategies With the hardware features provided by our double kicker and dribbler, and the software features provided by Omicam and the FSM, our team was able to deploy advanced gameplay strategies","title":"Game strategies"},{"location":"strategy/#game-strategies","text":"With the hardware features provided by our double kicker and dribbler, and the software features provided by Omicam and the FSM, our team was able to deploy advanced gameplay strategies","title":"Game strategies"},{"location":"structural_design/","text":"Hardware Design This year we focused on designing a simple and robust robot which was quick to assemble and disassemble when needed. We also aimed on improving hardware from previous years including the custom omni-directional wheels introduced last year. The following are the major hardware improvements that have occurred this year: - Custom omni-directional wheels - Custom wound solenoid and kicking circuit - Custom microcontroller circuits - Custom camera hardware - Mouse sensor implemenation - Various debugging options for software to use Omni-directional Wheels Omni-directional wheels (also referred to as omniwheels ) are specially designed to give robots full freedom of motion, enabling them to travel in any direction without turning. The downside to these wheels is the relative lack of tracktion and robustness when compared to conventional tires due to the individual rollers. As such, the primary goal of new iterations is to increase the grip and strength of the wheels. The following image shows the development of the omniwheel used throughout the years of competition with the rightmost being the latest design: TODO: PUT PICTURE HERE The two leftmost designs, the Kornylak and GTF omniwheel are stock designs bought pre-made from the manufacturer. Through using both these wheels for a year, we discovered that they were lacking in grip and robustness respectively, leading us to develop our own designs. The third design was our first attempt at a custom omniwheel, featuring double layered rollers and individual roller axles for faster repair. Though it allowed for much smoother movement and better traction, the wheels were large and unwieldly and the rollers still broke due to the use of softer silicon rollers. Furthermore, the use of 3D printed components caused rollers to fall out of the wheel during games. TODO: Explain new design when it decides to exist Kicker Unit TODO: Idek if this works","title":"Structural design"},{"location":"structural_design/#hardware-design","text":"This year we focused on designing a simple and robust robot which was quick to assemble and disassemble when needed. We also aimed on improving hardware from previous years including the custom omni-directional wheels introduced last year. The following are the major hardware improvements that have occurred this year: - Custom omni-directional wheels - Custom wound solenoid and kicking circuit - Custom microcontroller circuits - Custom camera hardware - Mouse sensor implemenation - Various debugging options for software to use","title":"Hardware Design"},{"location":"structural_design/#omni-directional-wheels","text":"Omni-directional wheels (also referred to as omniwheels ) are specially designed to give robots full freedom of motion, enabling them to travel in any direction without turning. The downside to these wheels is the relative lack of tracktion and robustness when compared to conventional tires due to the individual rollers. As such, the primary goal of new iterations is to increase the grip and strength of the wheels. The following image shows the development of the omniwheel used throughout the years of competition with the rightmost being the latest design: TODO: PUT PICTURE HERE The two leftmost designs, the Kornylak and GTF omniwheel are stock designs bought pre-made from the manufacturer. Through using both these wheels for a year, we discovered that they were lacking in grip and robustness respectively, leading us to develop our own designs. The third design was our first attempt at a custom omniwheel, featuring double layered rollers and individual roller axles for faster repair. Though it allowed for much smoother movement and better traction, the wheels were large and unwieldly and the rollers still broke due to the use of softer silicon rollers. Furthermore, the use of 3D printed components caused rollers to fall out of the wheel during games. TODO: Explain new design when it decides to exist","title":"Omni-directional Wheels"},{"location":"structural_design/#kicker-unit","text":"TODO: Idek if this works","title":"Kicker Unit"}]}