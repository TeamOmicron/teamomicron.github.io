{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BBC Robotics - Team Omicron Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys' College in Queensland, Australia, competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot's hardware, electrical and software design. Please feel free to browse at your own pleasure, and don't hesitate to contact us by visiting the About page. We're more than happy to answer any questions you may have about any aspect of our robots! You may also be interested in reading our team description paper , although it doesn't contain anything not already listed on our website. Towards the end of 2020, our entire codebase including mechanical designs, electrical designs and all software will become open source for everyone to learn from and study. We hope you'll enjoy this opportunity, so please stay tuned.","title":"Home"},{"location":"#bbc-robotics-team-omicron","text":"Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys' College in Queensland, Australia, competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot's hardware, electrical and software design. Please feel free to browse at your own pleasure, and don't hesitate to contact us by visiting the About page. We're more than happy to answer any questions you may have about any aspect of our robots! You may also be interested in reading our team description paper , although it doesn't contain anything not already listed on our website. Towards the end of 2020, our entire codebase including mechanical designs, electrical designs and all software will become open source for everyone to learn from and study. We hope you'll enjoy this opportunity, so please stay tuned.","title":"BBC Robotics - Team Omicron"},{"location":"about/","text":"About us About Team Omicron Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical electrical deign, strategy code, docs Omicron TBA James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron TBA (TODO: will need to be fixed for stupid robocup rules) Our members are all very experienced, having competed in previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. We have been preparing for this competition for about (TODO: how many?) months and we estimate to have a combined (TODO: put hours here) hours developing and improving our current iteration of robots. Our current robots This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : our custom vision pipeline developed to replace the OpenMV H7. Programmed in C/C++, running on a LattePanda Delta 432, this application is capable of 720p@60fps (TODO: please confirm) field object tracking, and centimetre accurate field localisation using only camera data (no LRFs, etc) via a novel approach that uses non-linear optimisation algorithms. Omicontrol : our custom, wireless, all-in-one robot control software. Written in Kotlin, this program is used to tune our cameras, move robots around and visualise sensor data. Mouse sensor and velocity control : our robots have fully working PWM-3360 mouse sensors, which we use to accurately estimate and control our robot's velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : using the above projects, our movement software developers have combined highly accurate localisation data with precise velocity control to enable the execution of advanced ball-manipulation strategies on the field. Wheels(?) Double dribbler and kicker(?) Some technologies we build upon this year include: FSM : last year, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a novel method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \"switched\" through. This year, we continue to build upon this technology, introducing more state machines and more states. Protocol Buffers: at the 2019 Internationals, one of our founding teams (Deus Vult/Omicron) won the Innovation Prize for its use of Protocol Buffers, a Google-developed technology allowing the easy and fast transmission of data between devices. This year, we improve on this technology by using Protocol Buffers in our complex device network. PCBs (modular?) Wheels","title":"About"},{"location":"about/#about-us","text":"","title":"About us"},{"location":"about/#about-team-omicron","text":"Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical electrical deign, strategy code, docs Omicron TBA James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron TBA (TODO: will need to be fixed for stupid robocup rules) Our members are all very experienced, having competed in previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. We have been preparing for this competition for about (TODO: how many?) months and we estimate to have a combined (TODO: put hours here) hours developing and improving our current iteration of robots.","title":"About Team Omicron"},{"location":"about/#our-current-robots","text":"This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : our custom vision pipeline developed to replace the OpenMV H7. Programmed in C/C++, running on a LattePanda Delta 432, this application is capable of 720p@60fps (TODO: please confirm) field object tracking, and centimetre accurate field localisation using only camera data (no LRFs, etc) via a novel approach that uses non-linear optimisation algorithms. Omicontrol : our custom, wireless, all-in-one robot control software. Written in Kotlin, this program is used to tune our cameras, move robots around and visualise sensor data. Mouse sensor and velocity control : our robots have fully working PWM-3360 mouse sensors, which we use to accurately estimate and control our robot's velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : using the above projects, our movement software developers have combined highly accurate localisation data with precise velocity control to enable the execution of advanced ball-manipulation strategies on the field. Wheels(?) Double dribbler and kicker(?) Some technologies we build upon this year include: FSM : last year, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a novel method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \"switched\" through. This year, we continue to build upon this technology, introducing more state machines and more states. Protocol Buffers: at the 2019 Internationals, one of our founding teams (Deus Vult/Omicron) won the Innovation Prize for its use of Protocol Buffers, a Google-developed technology allowing the easy and fast transmission of data between devices. This year, we improve on this technology by using Protocol Buffers in our complex device network. PCBs (modular?) Wheels","title":"Our current robots"},{"location":"omicam/","text":"Omicam One of the biggest innovation Team Omicron brings this year is our advanced, all-in-one, custom developed vision and localisation application called Omicam . The application is developed mostly in C, with some C++ code to interface with OpenCV. It runs on the powerful LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS. We are proud to report that Omicam is a significant step up compared to previous vision applications in use at BBC Robotics, as you will see below. Omicam consists of X lines of code, and took about X hours to develop. Background and previous methods Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer. With the introduction of the orange ball and advanced teams' application of \"ball-hiding\" strategies, accurate and fast computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the best solution was to do what the most advanced teams were doing, and develop a custom vision application running on a single board computer (SBC). TODO cover Omicam development with other SBCs Performance and results Omicam is capable of detecting 4 field objects at over 60fps at 720p (1280x720) resolution . Compared to the previous OpenMV H7, this is 12x higher resolution at 3x the framerate. 1 In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the robot's position to ~1cm accuracy at realtime speeds. This is over 5x/25x more accurate 2 than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable. To achieve this performance, we made heavy use of parallel programming techniques, OpenCV's x86 SIMD CPU optimisations, Clang optimiser flags as well as intelligent image downscaling for the goal threshold images (as they are mostly unused). In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application is asynchronous. 1 results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution. 2 depending on whether LRF based/goal based localisation was used. Hardware Omicam runs on a Linux-based LattePanda Delta 432. The camera used is a USB 2.0 camera using the OV 4689 sensor which can capture video footage at 720p 60 FPS. More here, camera and stuff. Field object detection The primary responsibility of Omicam is to detect field objects: the ball, goals and also lines. To do this, we use the popular computer vision library OpenCV, v4. We use Linux's UVC driver to acquire an MJPEG stream from the USB camera. Then, we apply any pre-processing steps such as downscaling the goal frames and gamma boosting. Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV's thresholding function but a custom parallel framework (as it doesn't run in parallel by default). Then, we use OpenCV's parallel connected component labeller, specifically the algorithm by Grana et al[1] to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. We then dispatch the largest detected blob's centroid via UART to the ESP32, encoded using Protocol Buffers. TODO describe in more detail Localisation Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball. Previous methods Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals in the camera to estimate the robot's position. This approach is very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in super team). Expected accuracy is 15-25cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects, such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor, which is impossible to know. The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard this as not an ideal approach. Our solution This year, Team Omicron presents a novel approach to robot localisation based on a middle-size league paper by Lu, Li, Zhang, Hu Zheng (2013). We localise using purely RGB camera data by solving a non-linear optimisation problem using the lines on the playing field. In practice, this works by the vision processor passing its thresholded line data to the localiser. Using its worker threads, a certain number of rays are casted on the line image using Bresenham's line algorithm[3] to find every time a ray intersects a line (these are called \"line points\"). Using a manually determined equation, these points are then dewarped to counter the distortion of the 360 degree mirror. To localise, the Subplex[4] non-linear optimiser (part of the NLopt package[5]) is used to minimise an objective function that TODO describe what it does in simple terms. In simple terms, we optimise by moving a virtual robot around on a field and comparing the distances between what it might see and what we really see to \"reverse-engineer\" the (x,y) position of the real robot. Interfacing with Omicontrol To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as the temperature of the SBC. We use the SIMD optimised libjpeg-turbo to efficiently encode JPEG frames, so as to not waste performance to the remote debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit images, it was determined that zlib could compress them more efficiently (around about 460,800x reduction in size). With all these optimisations, even at high framerates (60+ packets per second), the remote debug system only uses 300KB/s to 1 MB/s of network bandwidth. Debugging Low-level compiled languages such as C and C++ are notoriously unstable. In order to improve the stability of Omicam and fix bugs, we used Google's Address Sanitizer to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. In addition, we used the LLVM toolchain's debugger lldb to analyse the application frequently. To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application. References TODO USE MENDELEY [1] C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-based Connected Components Labeling with Decision Trees,\u201d IEEE Transac-tions on Image Processing, vol. 19, no. 6, pp. 1596\u20131609, 2010. [2] TODO CITE PROPERLY Robust and Real-time Self-Localization Based on Omnidirectional Vision for Soccer Robots [3] Bresenham's paper [4] Subplex paper [5] NLopt paper","title":"Omicam (vision and localisastion)"},{"location":"omicam/#omicam","text":"One of the biggest innovation Team Omicron brings this year is our advanced, all-in-one, custom developed vision and localisation application called Omicam . The application is developed mostly in C, with some C++ code to interface with OpenCV. It runs on the powerful LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS. We are proud to report that Omicam is a significant step up compared to previous vision applications in use at BBC Robotics, as you will see below. Omicam consists of X lines of code, and took about X hours to develop.","title":"Omicam"},{"location":"omicam/#background-and-previous-methods","text":"Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer. With the introduction of the orange ball and advanced teams' application of \"ball-hiding\" strategies, accurate and fast computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the best solution was to do what the most advanced teams were doing, and develop a custom vision application running on a single board computer (SBC). TODO cover Omicam development with other SBCs","title":"Background and previous methods"},{"location":"omicam/#performance-and-results","text":"Omicam is capable of detecting 4 field objects at over 60fps at 720p (1280x720) resolution . Compared to the previous OpenMV H7, this is 12x higher resolution at 3x the framerate. 1 In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the robot's position to ~1cm accuracy at realtime speeds. This is over 5x/25x more accurate 2 than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable. To achieve this performance, we made heavy use of parallel programming techniques, OpenCV's x86 SIMD CPU optimisations, Clang optimiser flags as well as intelligent image downscaling for the goal threshold images (as they are mostly unused). In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application is asynchronous. 1 results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution. 2 depending on whether LRF based/goal based localisation was used.","title":"Performance and results"},{"location":"omicam/#hardware","text":"Omicam runs on a Linux-based LattePanda Delta 432. The camera used is a USB 2.0 camera using the OV 4689 sensor which can capture video footage at 720p 60 FPS. More here, camera and stuff.","title":"Hardware"},{"location":"omicam/#field-object-detection","text":"The primary responsibility of Omicam is to detect field objects: the ball, goals and also lines. To do this, we use the popular computer vision library OpenCV, v4. We use Linux's UVC driver to acquire an MJPEG stream from the USB camera. Then, we apply any pre-processing steps such as downscaling the goal frames and gamma boosting. Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV's thresholding function but a custom parallel framework (as it doesn't run in parallel by default). Then, we use OpenCV's parallel connected component labeller, specifically the algorithm by Grana et al[1] to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. We then dispatch the largest detected blob's centroid via UART to the ESP32, encoded using Protocol Buffers. TODO describe in more detail","title":"Field object detection"},{"location":"omicam/#localisation","text":"Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball.","title":"Localisation"},{"location":"omicam/#previous-methods","text":"Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals in the camera to estimate the robot's position. This approach is very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in super team). Expected accuracy is 15-25cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects, such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor, which is impossible to know. The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard this as not an ideal approach.","title":"Previous methods"},{"location":"omicam/#our-solution","text":"This year, Team Omicron presents a novel approach to robot localisation based on a middle-size league paper by Lu, Li, Zhang, Hu Zheng (2013). We localise using purely RGB camera data by solving a non-linear optimisation problem using the lines on the playing field. In practice, this works by the vision processor passing its thresholded line data to the localiser. Using its worker threads, a certain number of rays are casted on the line image using Bresenham's line algorithm[3] to find every time a ray intersects a line (these are called \"line points\"). Using a manually determined equation, these points are then dewarped to counter the distortion of the 360 degree mirror. To localise, the Subplex[4] non-linear optimiser (part of the NLopt package[5]) is used to minimise an objective function that TODO describe what it does in simple terms. In simple terms, we optimise by moving a virtual robot around on a field and comparing the distances between what it might see and what we really see to \"reverse-engineer\" the (x,y) position of the real robot.","title":"Our solution"},{"location":"omicam/#interfacing-with-omicontrol","text":"To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as the temperature of the SBC. We use the SIMD optimised libjpeg-turbo to efficiently encode JPEG frames, so as to not waste performance to the remote debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit images, it was determined that zlib could compress them more efficiently (around about 460,800x reduction in size). With all these optimisations, even at high framerates (60+ packets per second), the remote debug system only uses 300KB/s to 1 MB/s of network bandwidth.","title":"Interfacing with Omicontrol"},{"location":"omicam/#debugging","text":"Low-level compiled languages such as C and C++ are notoriously unstable. In order to improve the stability of Omicam and fix bugs, we used Google's Address Sanitizer to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. In addition, we used the LLVM toolchain's debugger lldb to analyse the application frequently. To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application.","title":"Debugging"},{"location":"omicam/#references","text":"TODO USE MENDELEY [1] C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-based Connected Components Labeling with Decision Trees,\u201d IEEE Transac-tions on Image Processing, vol. 19, no. 6, pp. 1596\u20131609, 2010. [2] TODO CITE PROPERLY Robust and Real-time Self-Localization Based on Omnidirectional Vision for Soccer Robots [3] Bresenham's paper [4] Subplex paper [5] NLopt paper","title":"References"},{"location":"omicontrol/","text":"Omicontrol","title":"Omicontrol (debugging and control)"},{"location":"omicontrol/#omicontrol","text":"","title":"Omicontrol"},{"location":"protobuf/","text":"Protocol Buffers","title":"Protocol Buffers"},{"location":"protobuf/#protocol-buffers","text":"","title":"Protocol Buffers"},{"location":"strategy/","text":"Game strategies With the hardware features provided by our double kicker and dribbler, and the software features provided by Omicam and the FSM, our team was able to deploy advanced gameplay strategies","title":"Game strategies"},{"location":"strategy/#game-strategies","text":"With the hardware features provided by our double kicker and dribbler, and the software features provided by Omicam and the FSM, our team was able to deploy advanced gameplay strategies","title":"Game strategies"},{"location":"structural_design/","text":"Hardware Design This year we focused on designing a simple and robust robot which was quick to assemble and disassemble when needed. We also aimed on improving hardware from previous years including the custom omni-directional wheels introduced last year. The following are the major hardware improvements that have occurred this year: - Custom omni-directional wheels - Custom wound solenoid and kicking circuit - Custom microcontroller circuits - Custom camera hardware - Mouse sensor implemenation - Various debugging options for software to use Omni-directional Wheels Omni-directional wheels (also referred to as omniwheels ) are specially designed to give robots full freedom of motion, enabling them to travel in any direction without turning. The downside to these wheels is the relative lack of tracktion and robustness when compared to conventional tires due to the individual rollers. As such, the primary goal of new iterations is to increase the grip and strength of the wheels. The following image shows the development of the omniwheel used throughout the years of competition with the rightmost being the latest design: TODO: PUT PICTURE HERE The two leftmost designs, the Kornylak and GTF omniwheel are stock designs bought pre-made from the manufacturer. Through using both these wheels for a year, we discovered that they were lacking in grip and robustness respectively, leading us to develop our own designs. The third design was our first attempt at a custom omniwheel, featuring double layered rollers and individual roller axles for faster repair. Though it allowed for much smoother movement and better traction, the wheels were large and unwieldly and the rollers still broke due to the use of softer silicon rollers. Furthermore, the use of 3D printed components caused rollers to fall out of the wheel during games. TODO: Explain new design when it decides to exist Kicker Unit TODO: Idek if this works","title":"Structural design"},{"location":"structural_design/#hardware-design","text":"This year we focused on designing a simple and robust robot which was quick to assemble and disassemble when needed. We also aimed on improving hardware from previous years including the custom omni-directional wheels introduced last year. The following are the major hardware improvements that have occurred this year: - Custom omni-directional wheels - Custom wound solenoid and kicking circuit - Custom microcontroller circuits - Custom camera hardware - Mouse sensor implemenation - Various debugging options for software to use","title":"Hardware Design"},{"location":"structural_design/#omni-directional-wheels","text":"Omni-directional wheels (also referred to as omniwheels ) are specially designed to give robots full freedom of motion, enabling them to travel in any direction without turning. The downside to these wheels is the relative lack of tracktion and robustness when compared to conventional tires due to the individual rollers. As such, the primary goal of new iterations is to increase the grip and strength of the wheels. The following image shows the development of the omniwheel used throughout the years of competition with the rightmost being the latest design: TODO: PUT PICTURE HERE The two leftmost designs, the Kornylak and GTF omniwheel are stock designs bought pre-made from the manufacturer. Through using both these wheels for a year, we discovered that they were lacking in grip and robustness respectively, leading us to develop our own designs. The third design was our first attempt at a custom omniwheel, featuring double layered rollers and individual roller axles for faster repair. Though it allowed for much smoother movement and better traction, the wheels were large and unwieldly and the rollers still broke due to the use of softer silicon rollers. Furthermore, the use of 3D printed components caused rollers to fall out of the wheel during games. TODO: Explain new design when it decides to exist","title":"Omni-directional Wheels"},{"location":"structural_design/#kicker-unit","text":"TODO: Idek if this works","title":"Kicker Unit"}]}