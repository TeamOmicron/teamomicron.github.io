{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BBC Robotics - Team Omicron Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys\u2019 College in Queensland, Australia, competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot\u2019s hardware, electrical and software design. Please feel free to browse at your own pleasure, and don\u2019t hesitate to contact us by visiting the About page. We\u2019re more than happy to answer any questions you may have about any aspect of our robots! You may also be interested in reading our team description paper , although it doesn\u2019t contain anything not already listed on our website. Some time in the next year or so (from competition date), our entire codebase including mechanical designs, electrical designs and all software will become open source for everyone to learn from and study. The exact timeframe is still yet to be determined, and depends on a variety of factors, so we don\u2019t have a particular date in mind right now but we can assure you it will happen.","title":"Home"},{"location":"#bbc-robotics-team-omicron","text":"Welcome to the official documentation website for Team Omicron, a robotics team from Brisbane Boys\u2019 College in Queensland, Australia, competing in RoboCup Junior Open Soccer at the Bordeaux 2020 Internationals. This website contains all the information about our robot\u2019s hardware, electrical and software design. Please feel free to browse at your own pleasure, and don\u2019t hesitate to contact us by visiting the About page. We\u2019re more than happy to answer any questions you may have about any aspect of our robots! You may also be interested in reading our team description paper , although it doesn\u2019t contain anything not already listed on our website. Some time in the next year or so (from competition date), our entire codebase including mechanical designs, electrical designs and all software will become open source for everyone to learn from and study. The exact timeframe is still yet to be determined, and depends on a variety of factors, so we don\u2019t have a particular date in mind right now but we can assure you it will happen.","title":"BBC Robotics - Team Omicron"},{"location":"about/","text":"About us About Team Omicron Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical & electrical deign, strategy code, docs Omicron TBA James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron TBA (TODO: will need to be fixed for stupid robocup rules) Our members are all very experienced, having competed in 2-3 previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. We have been preparing for this competition for about (TODO: how many?) months and we estimate to have a combined (TODO: put hours here) hours developing and improving our current iteration of robots. Our current robots This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : our advanced, custom-developed vision application designed to replace the OpenMV H7. Written mainly in C, Omicam is capable of 70 FPS field object tracking at 720p (1280x720 pixels), and 1cm accurate robot position estimation (localisation) by solving a multi-variate optimisation problem in real time. Omicontrol : our custom, wireless/wired, all-in-one robot control software. Written in Kotlin, this program is used to tune our cameras, move robots around and visualise sensor data. Mouse sensor and velocity control : our robots have fully working PWM-3360 mouse sensors, which we use to accurately estimate and control our robot\u2019s velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : using the above projects, our movement software developers have combined highly accurate localisation data with precise velocity control to enable the execution of advanced ball-manipulation strategies on the field, designed to get the upper edge on our opponents. Wheels(?) Double dribbler and kicker(?) Some technologies we build upon this year include: FSM : last year, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a novel method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \u201cswitched\u201d through. This year, we continue to build upon this technology, introducing more states to our FSM. Protocol Buffers: last year, we unveiled Protocol Buffers as a an easy and intuitive way of passing complex data between devices. This year, we continue to use the Google-developed technology, expanding upon its usage scope significantly. PCBs (modular?) Wheels","title":"About"},{"location":"about/#about-us","text":"","title":"About us"},{"location":"about/#about-team-omicron","text":"Team Omicron was formed in 2019 as a merger between two BBC teams, J-TEC (previously Team APEX) and Team Omicron (previously Deus Vult). Our team members are: Name Primary responsibilities Original team Contact Lachlan Ellis Strategy code J-TEC TBA Tynan Jones Electrical design J-TEC TBA Ethan Lo Mechanical & electrical deign, strategy code, docs Omicron TBA James Talkington Mechanical design J-TEC TBA Matt Young Vision systems developer, docs Omicron TBA (TODO: will need to be fixed for stupid robocup rules) Our members are all very experienced, having competed in 2-3 previous internationals competitions. Our team collectively also has many victories and podium placements in Australian Nationals, States and Regionals competitions. We have been preparing for this competition for about (TODO: how many?) months and we estimate to have a combined (TODO: put hours here) hours developing and improving our current iteration of robots.","title":"About Team Omicron"},{"location":"about/#our-current-robots","text":"This year, our team brings many new and exciting innovations to the table, as well as building on reliable technologies we have previously developed. Some new innovations we have developed this year include: Omicam : our advanced, custom-developed vision application designed to replace the OpenMV H7. Written mainly in C, Omicam is capable of 70 FPS field object tracking at 720p (1280x720 pixels), and 1cm accurate robot position estimation (localisation) by solving a multi-variate optimisation problem in real time. Omicontrol : our custom, wireless/wired, all-in-one robot control software. Written in Kotlin, this program is used to tune our cameras, move robots around and visualise sensor data. Mouse sensor and velocity control : our robots have fully working PWM-3360 mouse sensors, which we use to accurately estimate and control our robot\u2019s velocity on the field, in addition to interpolating our localisation data. Advanced game strategies : using the above projects, our movement software developers have combined highly accurate localisation data with precise velocity control to enable the execution of advanced ball-manipulation strategies on the field, designed to get the upper edge on our opponents. Wheels(?) Double dribbler and kicker(?) Some technologies we build upon this year include: FSM : last year, we introduced the concept of a Hierarchical Finite State Machine (HFSM) as a novel method of organising robot behaviour through a graph of inter-connected state machines, each which contains a series of states that can be \u201cswitched\u201d through. This year, we continue to build upon this technology, introducing more states to our FSM. Protocol Buffers: last year, we unveiled Protocol Buffers as a an easy and intuitive way of passing complex data between devices. This year, we continue to use the Google-developed technology, expanding upon its usage scope significantly. PCBs (modular?) Wheels","title":"Our current robots"},{"location":"electrical_design/","text":"Electrical Design PCBs were used for our electronics to mitigate wiring, as well us enable us to use components that would otherwise be unusable. We use 4 custom-made PCBs, which we label as the \u201cMain\u201d, \u201cBase\u201d, \u201cKicker\u201d and \u201cDebug\u201d boards. TODO: WE NEED IMAGES Main Board The main board is the central board of our electrical design. It contains the power supply, master microcontroller, test points and connections to all the other external electronics. Of these, the most noteworthy development is the use of test points. These are regions of exposed copper designed for convenient probing during the debug stage. The Main Board contains: Power supply and safety circuitry Chip-on-Board ESP32 microcontroller Teensy 4.0 microcontroller Micro-USB port for uploading 2 position slide switching for choosing upload target BNO055 9-Axis IMU Laser Rangefinders Buzzers for audio debug LEDs for visual debug Various test points for easy probing Base Board At the bottom of our robot is the base board. It holds our light sensors, light gate outputs, mouse sensor, and motor controllers. To drive these a slave microcontroller (namely the ATMEGA328p) was soldered directly onto the board. This is to save money and vertical clearance, and to decrease the chance of a short occurring due to the board\u2019s proximity to the motors. Kicker Board On the kicker board is the necessary electronics to drive two solenoids. This is not new to RoboCup, however we have made major modifications to the tried and tested design. Instead of directly driving the power MOSFET, we use an optocoupler to isolate the sensitive signal lines from the high voltages running through the circuit in an effort to protect the microcontroller in the event of a failure. Furthermore, we have moved away from relays and instead use MOSFETS to switch the load. We found that relays wore out after many frequent uses, and therefore we decided to use a non-mechanical solution. Debug Board A problem we have previously faced is the lack of debug tools for our robot. Whilst debug messages are adequate for software design, when the robot is running it is often difficult to connect a computer to it. Hence, we developed a dedicated debug board that allows us to manually interface with our robot for debug purposes.","title":"Electrical design"},{"location":"electrical_design/#electrical-design","text":"PCBs were used for our electronics to mitigate wiring, as well us enable us to use components that would otherwise be unusable. We use 4 custom-made PCBs, which we label as the \u201cMain\u201d, \u201cBase\u201d, \u201cKicker\u201d and \u201cDebug\u201d boards. TODO: WE NEED IMAGES","title":"Electrical Design"},{"location":"electrical_design/#main-board","text":"The main board is the central board of our electrical design. It contains the power supply, master microcontroller, test points and connections to all the other external electronics. Of these, the most noteworthy development is the use of test points. These are regions of exposed copper designed for convenient probing during the debug stage. The Main Board contains: Power supply and safety circuitry Chip-on-Board ESP32 microcontroller Teensy 4.0 microcontroller Micro-USB port for uploading 2 position slide switching for choosing upload target BNO055 9-Axis IMU Laser Rangefinders Buzzers for audio debug LEDs for visual debug Various test points for easy probing","title":"Main Board"},{"location":"electrical_design/#base-board","text":"At the bottom of our robot is the base board. It holds our light sensors, light gate outputs, mouse sensor, and motor controllers. To drive these a slave microcontroller (namely the ATMEGA328p) was soldered directly onto the board. This is to save money and vertical clearance, and to decrease the chance of a short occurring due to the board\u2019s proximity to the motors.","title":"Base Board"},{"location":"electrical_design/#kicker-board","text":"On the kicker board is the necessary electronics to drive two solenoids. This is not new to RoboCup, however we have made major modifications to the tried and tested design. Instead of directly driving the power MOSFET, we use an optocoupler to isolate the sensitive signal lines from the high voltages running through the circuit in an effort to protect the microcontroller in the event of a failure. Furthermore, we have moved away from relays and instead use MOSFETS to switch the load. We found that relays wore out after many frequent uses, and therefore we decided to use a non-mechanical solution.","title":"Kicker Board"},{"location":"electrical_design/#debug-board","text":"A problem we have previously faced is the lack of debug tools for our robot. Whilst debug messages are adequate for software design, when the robot is running it is often difficult to connect a computer to it. Hence, we developed a dedicated debug board that allows us to manually interface with our robot for debug purposes.","title":"Debug Board"},{"location":"omicam/","text":"Omicam One of the biggest innovation Team Omicron brings this year is our advanced, custom developed vision and localisation application called Omicam . The application is developed mostly in C, with some C++ code to interface with OpenCV. It runs on the powerful LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS. We are proud to report that Omicam is a significant step up compared to previous vision applications in use at BBC Robotics, as explained in the \u201cPerformance and results\u201d section. Omicam consists of X lines of code, and took about X hours to develop. Background and previous methods Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer. With the introduction of the orange ball and advanced teams\u2019 application of \u201cball-hiding\u201d strategies, high resolution yet performant computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. However, detecting the field objects (ball, goals, etc) is now only the bare minimum. Advanced teams also need to accurately estimate their position on the field (localise) in order to execute advanced strategies and gain the upper hand in the competition. Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the best solution was to do what the most advanced teams were doing, and develop a custom vision application running on a single board computer (SBC). In terms of localisation, in the past we managed to get away with not using any, or using a low-fidelity approach based on detecting the goals in the image. Performance and results Omicam is capable of detecting the ball, both goals and lines at 60-70fps at 720p (1280x720) resolution . Compared to the previous OpenMV H7, this is 23x higher resolution at 3x the framerate. 1 In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the robot\u2019s position to <1cm accuracy at around 30 Hz. This is over 5x/25x more accurate 2 than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable. Finally, using the e-con Systems Hyperyon based around the ultra low-light performance IMX290 sensor, our vision pipeline is robust against lighting conditions ranging from near pitch black to direct sunlight. 1 previous results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution. 2 depending on whether LRF based/goal based localisation was used. Hardware Omicam supports any single board computer (SBC) that can run Linux. In our case, we use a LattePanda Delta 432 with a 2.4 GHz quad-core Intel Celeron N4100, 4GB of dual-channel DDR4 RAM, 32GB of storage, WiFi, Bluetooth, gigabit Ethernet and a UART bus via the inbuilt ATMega32U4. The current camera we use is an e-con Systems Hyperyon, based on the Sony Starvis IMX290 ultra low-light sensor capable of seeing in almost pitch black at high framerates. This is a USB 2.0 camera module, since the LattePanda has no MIPI port. SBC iterations The current iteration of Omicam\u2019s SBC setup is the cumulation of around 2 years of prototyping iterations. Each of the previous SBCs we tried have had one problem or another, and we believe the LattePanda Delta is the most powerful yet cost efficient approach. Prototype 1 (December 2018-January 2019): This consisted of a Raspberry Pi Zero W, with a 1 GHz single-core CPU. It was our initial prototype for a single-board computer, however, we quickly found it was far too weak to do any vision, and our inexperience at the time didn\u2019t help. Thus, we canned the SBC project for around a year. Prototype 2 (December 2019): After resurrecting the SBC R&D project for our 2020 Internationals, we started development with the Raspberry Pi 4. This has a 1.5 GHz quad-core ARM Cortex-A72 CPU. We began developing a custom computer vision library tailored specifically to our task, using the Pi\u2019s MMAL API for GPU-accelerated camera decoding. Initial results showed we could threshold images successfully, but we believed it would be too slow to localise and run a connected-component labeller in real time. Prototype 3 (January 2020): Next, we moved onto the NVIDIA Jetson Nano, containing a 1.43 GHz quad-core Cortex-A57, but far more importantly a 128-core Maxwell GPU. At this time we also switched to using OpenCV 4 for our computer vision. In theory, using the GPU for processing would lead to a huge performance boost due to the parallelism, however, in practice we observed the GPU was significantly slower than even the weaker Cortex-A43 CPU, (presumably) due to copying times. We were unable to optimise this to standard after weeks of development, thus we decided to move on from this prototype. Prototype 4 (January-February 2020): After much research, we decided to use the LattePanda Delta 432. The OpenCV pipeline is now entirely CPU bound, and despite not using the GPU at all, we are able to achieve very good performance. Camera module iterations We have spent a great deal of time and money trying to find the absolute best camera module to use in our hardware setup. After all, no matter how good our software may be, we are still limited by the quality of our image sensor. Pi Camera: The initial camera we used in hardware prototypes 1-3, was the Raspberry Pi Camera. In prototype 1, we used the Pi Cam v1.3 which is an OV5647 connected via MIPI, and in prototypes 2 and 3 we used the Pi Cam v2 which is an IMX219 again connected via MIPI. We had to drop this camera in later iterations because the LattePanda doesn\u2019t have a MIPI port. Both of these cameras were capable of around 720p at 60 fps. OV4689-based module: We experimented with a generic OV4689 USB 2.0 camera module from Amazon, which is capable of streaming JPEGs (aka an MJPEG stream) at 720p at 120 fps (we could get around 90 fps in practice with no processing). While this framerate was impressive, the camera module suffered from terrible noise and flickering in relatively good lighting conditions, so it was dropped. e-con Hyperyon: After these two failures, we began looking into professional industrial-grade cameras to use on our robot. While most of these, from companies like FLIR, are out of our price range, we found e-con Systems as a relatively affordable vendor of extremely high quality cameras. We narrowed down our selection to two devices: the See3CAM_CU30 USB 2/USB 3 2MP camera module, which is fairly standard and affordable, as well as the more expensive Hyperyon described above. Using our contacts at CSIRO, we managed to acquire a sample of some of the e-con cameras they used including the Hyperyon and a device similar to the CU30. While both can do 720p at 60 fps, we observed absolutely fantastic lighting performance from the Hyperyon but 100ms latency, while mediocre lighting performance from the CU30 but 60ms latency. After much deliberation, we decided to make the difficult trade-off to prioritise lighting over latency and use the Hyperyon. Field object detection The primary responsibility of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. To do this, we use the popular computer vision library OpenCV (v4.2.0). We use Video4Linux2 (V4L2) to acquire an MJPEG stream from the USB camera, via the popular gstreamer library. Then, we apply our pre-processing steps which are: crop the image, mask out the outside of the mirror, mask out the robot and downscale for the goal processing. Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV\u2019s inRange thresholder but a custom parallel framework (as it doesn\u2019t run in parallel by default). Thresholding generates a 1-bit binary mask of the image, where each pixel is 255 (true) if it\u2019s inside the RGB range specified, and 0 (false) if it\u2019s not. Then, we use OpenCV\u2019s parallel connected component labeller, specifically the algorithm by Grana et al. 1 to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. OpenCV automatically calculates the bounding box and centroid for each of these connected regions. We then dispatch the largest detected blob\u2019s centroid via UART to the ESP32, encoded using Protocol Buffers and using POSIX termios for UART configuration. TODO images and/or video of thresholded field Localisation Previous methods and background Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball. Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals in the camera to estimate the robot\u2019s position. This approach is very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in super team). Expected accuracy is 15-25cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects, such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor, which is impossible to know. The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard this as not an ideal approach. Our solution This year, Team Omicron presents a novel approach to robot localisation based partly on a middle-size league paper by Lu, Li, Zhang, Hu & Zheng 2 . We localise using only RGB camera data by solving a multi-variate optimisation problem in realtime, via the Subplex non-linear gradient-free optimisation algorithm. The principle method of operation of our algorithm is that we need to match a virtual model of field geometry to the observed one from the camera, thereby inferring our position. Another way to think of it is we need to generate a transform such that a virtual model of the field will align with the real one, thereby also calculating our 2D position. If we match the lines so that they align in both the virtual model and real-world model, then we can infer that the calculated virtual robot coordinates are the same as the real, unknown robot coordinates. Essentially, we\u2019re taking what we know: the static layout of the field, and observed field geometry at our current position, and using it to infer the unknown 2D position vector. This is a form of the orthogonal Procrustes problem, which can be solved through a multitude of approaches such as iterative closest point (commonly used with 3D LiDARS), Monte-Carlo localisation via a particle filter or gradient fields. However, most of these approaches also consider rotation as a factor. Due to to our use of a high-accuracy BNO-055 IMU, we consider rotation to be a non-issue that can be easily corrected for, thereby reducing the complexity of the problem. Thus, we developed a novel three step algorithm involving ray-casting to infer our 2D position, ignoring rotation as a factor to be solved for. These steps are: Image analysis Camera normalisation Coordinate optimisation Image analysis The localiser\u2019s input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding for the colour white, which is handled by the vision pipeline described earlier. With the input provided, a certain number of rays (usually 128) are emitted from the centre of the line image. A ray terminates when it touches a line, reaches the edge of the image or reaches the edge of the mirror (as it would be a waste of time to check outside the mirror). The theory of operation behind this is, essentially, for each field position each ray should have its own unique distance. Rays are stored as only a length in a regular C array, as we can infer the angle between each ray as: 2pi / n_rays Figure 1: example of ray casting on field, with a position near to the centre Camera normalisation These rays are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by measuring the pixels between points along evenly spaced tape placed on the real field, via Omicontrol. Using regression software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances. In our case, we simply apply the dewarp function to each ray length instead, leaving us with each ray essentially in field coordinates (or field lengths) rather than camera coordinates. This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres. Figure 2: example of applying the dewarp function to an entire image, on the low resolution OpenMV H7. The second phase of the camera normalisation is to rotate the rays relative to the robot\u2019s heading, using a rotation matrix. The robot\u2019s heading value, which is relative to when it was powered on, is transmitted by the ESP32, again using Protocol Buffers. For information about how this value is calculated using the IMU, see the ESP32 and movement code page. Position optimisation The main part of our solution is the Subplex local derivative-free non-linear optimiser 3 , re-implemented as part of the NLopt package 4 . This algorithm essentially acts as an efficiency and stability improvement over the well-known Nelder-Mead Simplex algorithm. The most critical part of this process is the objective function , which is a function that takes an N-dimensional vector (in our case, an estimated 2D position) and calculates essentially a \u201cscore\u201d of how accurate the value is. This objective function must be highly optimised as it could be evaluated hundreds of times by the optimisation algorithm. Figure 3: map of objective function for a robot placed at the centre of the field. White pixels indicate high accuracy areas and black pixels indicate less accurate areas. A naive approach, where every grid cell is evaluated and the lowest value is picked, would require 44,226 objective function evaluation which takes about 5 seconds on a fast computer. Using the Subplex optimiser, this can be reduced to just 62 evaluations for a 1.2mm accurate fix. TODO: cover approaches we drafted: line points, etc. Also, use more images in the docs We spent a great deal of effort drafting the most efficient objective function, and the approach we present makes heavy use of pre-computation via a \u201cfield file\u201d. This field file is a binary Protcol Buffer file that encodes the geometry of any RoboCup field by dividing it into a grid, where each cell is true if on a line, otherwise false. Increasing the resolution of the grid will increase its accuracy, but also significantly increase its file size. We use a 1cm grid, which stores 44,226 cells and is 42 KiB on disk. This takes about 2 seconds to generate on a fast desktop computer, and is copied across to the LattePanda. The field file is generated by a Python script which can be easily modified to support an arbitrary number of different field layouts, such as SuperTeam or our regional Australian field. Although the field file could theoretically be loaded via the bitmap image below, images are more difficult to load than Protobuf files and it would not support grid cell sizes smaller or larger than 1cm. Figure 3: bitmap image displaying generated field file. In the original 243x182 image, 1 pixel = 1cm The objective function essentially compares the difference between the ray lengths, as follows: Consider the estimated position (eX, eY). Move to (eX, eY) in the field file and raycast out to generate the array expectedRays . For each ray in the dewarped observedRays from the camera (which are now in field units, same as expectedRays ), do the following: totalError += abs(expectedRays[i] - observedRays[i]) Although a derivative-based algorithm may be more efficient at solving the problem, we deemed it far too difficult to calculate the derivative of the objective function. Adding new sensor data to the world model Our localisation algorithm is also flexible with new sensor inputs. Due to the use of a \u201cvirtual world model\u201d approach, if any sensors can be modelled as a measurement of a distance to a static object, then they can be integrated as extra data in the world model. This is extremely helpful when vision data runs into limitations, for example, in SuperTeam where it may be difficult to see enough field lines in one frame due to the size of the field. We integrate the LRFs on the robot as extra distance sensors, using their reported distance to the field walls just as we use the raycasted \u201cline points\u201d in the vision based approach. It would also be possible to use the distance to the goals as virtual information to supplement a potential lack of line data and thus increase accuracy. Other distance sensors such as 360 LiDARS can also be implemented with ease. Justification for our approach Observant readers will notice that the objective function is technically linear (as it contains no exponentials or powers). Hence, it may be observed that linear programming (essentially linear optimisation) could be used to solve the problem. In fact, it\u2019s very plausible that something like Dantzig\u2019s Simplex/criss-cross algorithm or an interior point algorithm could locate the optimum far more efficiently than the Subplex/Nelder-Mead simplex optimiser. However, there is a distinct lack of easy to use and non-restrictively licensed linear optimisers for C. The best candidate was probably COIN-OR\u2019s CLP or Google\u2019s Glop, but both of these methods are quite complex compared to NLopt\u2019s Subplex algorithm (and also written in C++). We determined that given the performance of the localiser (about 30 Hz maximum) and mouse sensor interpolation, it\u2019s not worth switching to a linear optimiser when the non-linear one works fine and fast enough. Finally, it was brought to our attention that it may be possible to forgo the optimisation process and sum the error of the rays in some geometric method to transform the robot\u2019s position. While we agree that this may be possible and worth looking into, we believe that optimisation leads to more stable and less error-prone results (though this is not confirmed). Interfacing with Omicontrol To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as the temperature of the SBC. Although C isn\u2019t an officially supported language by Google for Protocol Buffers, we use the mature nanopb library to do the encoding. This is the same library used on the ESP32 and Teensy as well, and so far we\u2019ve had no issues with it. We use the SIMD optimised libjpeg-turbo to efficiently encode JPEG frames, so as to not waste performance to the remote debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit images, it was determined that zlib could compress them more efficiently (around about 460,800x reduction in size). With all these optimisations, even at high framerates (60+ packets per second), the remote debug system uses no more than 1 MB/s of outgoing bandwidth, which is small enough to work reliably on both local and Internet networks. Debugging and performance optimisation To achieve Omicam\u2019s performance, we made heavy use of parallel programming techniques, OpenCV\u2019s x86 SIMD CPU optimisations, Clang optimiser flags as well as intelligent image downscaling for the goal threshold images (as they are mostly unused). In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application is asynchronous. Low-level compiled languages such as C and C++ are notoriously unstable and difficult to debug. In order to improve the stability of Omicam and fix bugs, we used Google\u2019s Address Sanitizer (ASan) and Undefined Behaviour Sanitizer (UBSan) to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. In addition, we used the LLVM toolchain\u2019s debugger lldb (or just gdb) to analyse the application frequently. To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application. Although the Clang compiler may have marginally worse performance than GCC, we chose Clang because it\u2019s more modern and has better debugging support (namely, GCC\u2019s Adress Sanitizer implementation is broken for us). To improve performance of the localiser, we use the last extrapolated position from the mouse sensor as a seed for the initial position of the next search. This means instead of starting from a random position, the localiser will complete much quickly a it\u2019s already relatively close to the true position. Also cover Linux CPU optimisation and associated thermal issues if relevant References C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-Based Connected Components Labeling With Decision Trees,\u201d IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596\u20131609, 2010, doi: 10.1109/TIP.2010.2044963. \u21a9 H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, \u201cRobust and real-time self-localization based on omnidirectional vision for soccer robots,\u201d Adv. Robot., vol. 27, no. 10, pp. 799\u2013811, Jul. 2013, doi: 10.1080/01691864.2013.785473. \u21a9 T. H. Rowan, \u201cFunctional stability analysis of numerical algorithms,\u201d Unpuplished Diss., p. 218, 1990. \u21a9 Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt \u21a9","title":"Omicam (vision and localisastion)"},{"location":"omicam/#omicam","text":"One of the biggest innovation Team Omicron brings this year is our advanced, custom developed vision and localisation application called Omicam . The application is developed mostly in C, with some C++ code to interface with OpenCV. It runs on the powerful LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS. We are proud to report that Omicam is a significant step up compared to previous vision applications in use at BBC Robotics, as explained in the \u201cPerformance and results\u201d section. Omicam consists of X lines of code, and took about X hours to develop.","title":"Omicam"},{"location":"omicam/#background-and-previous-methods","text":"Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer. With the introduction of the orange ball and advanced teams\u2019 application of \u201cball-hiding\u201d strategies, high resolution yet performant computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. However, detecting the field objects (ball, goals, etc) is now only the bare minimum. Advanced teams also need to accurately estimate their position on the field (localise) in order to execute advanced strategies and gain the upper hand in the competition. Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the best solution was to do what the most advanced teams were doing, and develop a custom vision application running on a single board computer (SBC). In terms of localisation, in the past we managed to get away with not using any, or using a low-fidelity approach based on detecting the goals in the image.","title":"Background and previous methods"},{"location":"omicam/#performance-and-results","text":"Omicam is capable of detecting the ball, both goals and lines at 60-70fps at 720p (1280x720) resolution . Compared to the previous OpenMV H7, this is 23x higher resolution at 3x the framerate. 1 In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the robot\u2019s position to <1cm accuracy at around 30 Hz. This is over 5x/25x more accurate 2 than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable. Finally, using the e-con Systems Hyperyon based around the ultra low-light performance IMX290 sensor, our vision pipeline is robust against lighting conditions ranging from near pitch black to direct sunlight. 1 previous results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution. 2 depending on whether LRF based/goal based localisation was used.","title":"Performance and results"},{"location":"omicam/#hardware","text":"Omicam supports any single board computer (SBC) that can run Linux. In our case, we use a LattePanda Delta 432 with a 2.4 GHz quad-core Intel Celeron N4100, 4GB of dual-channel DDR4 RAM, 32GB of storage, WiFi, Bluetooth, gigabit Ethernet and a UART bus via the inbuilt ATMega32U4. The current camera we use is an e-con Systems Hyperyon, based on the Sony Starvis IMX290 ultra low-light sensor capable of seeing in almost pitch black at high framerates. This is a USB 2.0 camera module, since the LattePanda has no MIPI port.","title":"Hardware"},{"location":"omicam/#sbc-iterations","text":"The current iteration of Omicam\u2019s SBC setup is the cumulation of around 2 years of prototyping iterations. Each of the previous SBCs we tried have had one problem or another, and we believe the LattePanda Delta is the most powerful yet cost efficient approach. Prototype 1 (December 2018-January 2019): This consisted of a Raspberry Pi Zero W, with a 1 GHz single-core CPU. It was our initial prototype for a single-board computer, however, we quickly found it was far too weak to do any vision, and our inexperience at the time didn\u2019t help. Thus, we canned the SBC project for around a year. Prototype 2 (December 2019): After resurrecting the SBC R&D project for our 2020 Internationals, we started development with the Raspberry Pi 4. This has a 1.5 GHz quad-core ARM Cortex-A72 CPU. We began developing a custom computer vision library tailored specifically to our task, using the Pi\u2019s MMAL API for GPU-accelerated camera decoding. Initial results showed we could threshold images successfully, but we believed it would be too slow to localise and run a connected-component labeller in real time. Prototype 3 (January 2020): Next, we moved onto the NVIDIA Jetson Nano, containing a 1.43 GHz quad-core Cortex-A57, but far more importantly a 128-core Maxwell GPU. At this time we also switched to using OpenCV 4 for our computer vision. In theory, using the GPU for processing would lead to a huge performance boost due to the parallelism, however, in practice we observed the GPU was significantly slower than even the weaker Cortex-A43 CPU, (presumably) due to copying times. We were unable to optimise this to standard after weeks of development, thus we decided to move on from this prototype. Prototype 4 (January-February 2020): After much research, we decided to use the LattePanda Delta 432. The OpenCV pipeline is now entirely CPU bound, and despite not using the GPU at all, we are able to achieve very good performance.","title":"SBC iterations"},{"location":"omicam/#camera-module-iterations","text":"We have spent a great deal of time and money trying to find the absolute best camera module to use in our hardware setup. After all, no matter how good our software may be, we are still limited by the quality of our image sensor. Pi Camera: The initial camera we used in hardware prototypes 1-3, was the Raspberry Pi Camera. In prototype 1, we used the Pi Cam v1.3 which is an OV5647 connected via MIPI, and in prototypes 2 and 3 we used the Pi Cam v2 which is an IMX219 again connected via MIPI. We had to drop this camera in later iterations because the LattePanda doesn\u2019t have a MIPI port. Both of these cameras were capable of around 720p at 60 fps. OV4689-based module: We experimented with a generic OV4689 USB 2.0 camera module from Amazon, which is capable of streaming JPEGs (aka an MJPEG stream) at 720p at 120 fps (we could get around 90 fps in practice with no processing). While this framerate was impressive, the camera module suffered from terrible noise and flickering in relatively good lighting conditions, so it was dropped. e-con Hyperyon: After these two failures, we began looking into professional industrial-grade cameras to use on our robot. While most of these, from companies like FLIR, are out of our price range, we found e-con Systems as a relatively affordable vendor of extremely high quality cameras. We narrowed down our selection to two devices: the See3CAM_CU30 USB 2/USB 3 2MP camera module, which is fairly standard and affordable, as well as the more expensive Hyperyon described above. Using our contacts at CSIRO, we managed to acquire a sample of some of the e-con cameras they used including the Hyperyon and a device similar to the CU30. While both can do 720p at 60 fps, we observed absolutely fantastic lighting performance from the Hyperyon but 100ms latency, while mediocre lighting performance from the CU30 but 60ms latency. After much deliberation, we decided to make the difficult trade-off to prioritise lighting over latency and use the Hyperyon.","title":"Camera module iterations"},{"location":"omicam/#field-object-detection","text":"The primary responsibility of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. To do this, we use the popular computer vision library OpenCV (v4.2.0). We use Video4Linux2 (V4L2) to acquire an MJPEG stream from the USB camera, via the popular gstreamer library. Then, we apply our pre-processing steps which are: crop the image, mask out the outside of the mirror, mask out the robot and downscale for the goal processing. Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV\u2019s inRange thresholder but a custom parallel framework (as it doesn\u2019t run in parallel by default). Thresholding generates a 1-bit binary mask of the image, where each pixel is 255 (true) if it\u2019s inside the RGB range specified, and 0 (false) if it\u2019s not. Then, we use OpenCV\u2019s parallel connected component labeller, specifically the algorithm by Grana et al. 1 to detect regions of the same colour in the image. The largest connected region will be the field object we are looking for. OpenCV automatically calculates the bounding box and centroid for each of these connected regions. We then dispatch the largest detected blob\u2019s centroid via UART to the ESP32, encoded using Protocol Buffers and using POSIX termios for UART configuration. TODO images and/or video of thresholded field","title":"Field object detection"},{"location":"omicam/#localisation","text":"","title":"Localisation"},{"location":"omicam/#previous-methods-and-background","text":"Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order to develop advanced strategies and precise movement control, instead of just driving directly towards the ball. Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals in the camera to estimate the robot\u2019s position. This approach is very inaccurate because of the low resolution of most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes the goals are not visible (especially in super team). Expected accuracy is 15-25cm. The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects, such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor, which is impossible to know. The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard this as not an ideal approach.","title":"Previous methods and background"},{"location":"omicam/#our-solution","text":"This year, Team Omicron presents a novel approach to robot localisation based partly on a middle-size league paper by Lu, Li, Zhang, Hu & Zheng 2 . We localise using only RGB camera data by solving a multi-variate optimisation problem in realtime, via the Subplex non-linear gradient-free optimisation algorithm. The principle method of operation of our algorithm is that we need to match a virtual model of field geometry to the observed one from the camera, thereby inferring our position. Another way to think of it is we need to generate a transform such that a virtual model of the field will align with the real one, thereby also calculating our 2D position. If we match the lines so that they align in both the virtual model and real-world model, then we can infer that the calculated virtual robot coordinates are the same as the real, unknown robot coordinates. Essentially, we\u2019re taking what we know: the static layout of the field, and observed field geometry at our current position, and using it to infer the unknown 2D position vector. This is a form of the orthogonal Procrustes problem, which can be solved through a multitude of approaches such as iterative closest point (commonly used with 3D LiDARS), Monte-Carlo localisation via a particle filter or gradient fields. However, most of these approaches also consider rotation as a factor. Due to to our use of a high-accuracy BNO-055 IMU, we consider rotation to be a non-issue that can be easily corrected for, thereby reducing the complexity of the problem. Thus, we developed a novel three step algorithm involving ray-casting to infer our 2D position, ignoring rotation as a factor to be solved for. These steps are: Image analysis Camera normalisation Coordinate optimisation","title":"Our solution"},{"location":"omicam/#image-analysis","text":"The localiser\u2019s input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding for the colour white, which is handled by the vision pipeline described earlier. With the input provided, a certain number of rays (usually 128) are emitted from the centre of the line image. A ray terminates when it touches a line, reaches the edge of the image or reaches the edge of the mirror (as it would be a waste of time to check outside the mirror). The theory of operation behind this is, essentially, for each field position each ray should have its own unique distance. Rays are stored as only a length in a regular C array, as we can infer the angle between each ray as: 2pi / n_rays Figure 1: example of ray casting on field, with a position near to the centre","title":"Image analysis"},{"location":"omicam/#camera-normalisation","text":"These rays are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by measuring the pixels between points along evenly spaced tape placed on the real field, via Omicontrol. Using regression software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances. In our case, we simply apply the dewarp function to each ray length instead, leaving us with each ray essentially in field coordinates (or field lengths) rather than camera coordinates. This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres. Figure 2: example of applying the dewarp function to an entire image, on the low resolution OpenMV H7. The second phase of the camera normalisation is to rotate the rays relative to the robot\u2019s heading, using a rotation matrix. The robot\u2019s heading value, which is relative to when it was powered on, is transmitted by the ESP32, again using Protocol Buffers. For information about how this value is calculated using the IMU, see the ESP32 and movement code page.","title":"Camera normalisation"},{"location":"omicam/#position-optimisation","text":"The main part of our solution is the Subplex local derivative-free non-linear optimiser 3 , re-implemented as part of the NLopt package 4 . This algorithm essentially acts as an efficiency and stability improvement over the well-known Nelder-Mead Simplex algorithm. The most critical part of this process is the objective function , which is a function that takes an N-dimensional vector (in our case, an estimated 2D position) and calculates essentially a \u201cscore\u201d of how accurate the value is. This objective function must be highly optimised as it could be evaluated hundreds of times by the optimisation algorithm. Figure 3: map of objective function for a robot placed at the centre of the field. White pixels indicate high accuracy areas and black pixels indicate less accurate areas. A naive approach, where every grid cell is evaluated and the lowest value is picked, would require 44,226 objective function evaluation which takes about 5 seconds on a fast computer. Using the Subplex optimiser, this can be reduced to just 62 evaluations for a 1.2mm accurate fix. TODO: cover approaches we drafted: line points, etc. Also, use more images in the docs We spent a great deal of effort drafting the most efficient objective function, and the approach we present makes heavy use of pre-computation via a \u201cfield file\u201d. This field file is a binary Protcol Buffer file that encodes the geometry of any RoboCup field by dividing it into a grid, where each cell is true if on a line, otherwise false. Increasing the resolution of the grid will increase its accuracy, but also significantly increase its file size. We use a 1cm grid, which stores 44,226 cells and is 42 KiB on disk. This takes about 2 seconds to generate on a fast desktop computer, and is copied across to the LattePanda. The field file is generated by a Python script which can be easily modified to support an arbitrary number of different field layouts, such as SuperTeam or our regional Australian field. Although the field file could theoretically be loaded via the bitmap image below, images are more difficult to load than Protobuf files and it would not support grid cell sizes smaller or larger than 1cm. Figure 3: bitmap image displaying generated field file. In the original 243x182 image, 1 pixel = 1cm The objective function essentially compares the difference between the ray lengths, as follows: Consider the estimated position (eX, eY). Move to (eX, eY) in the field file and raycast out to generate the array expectedRays . For each ray in the dewarped observedRays from the camera (which are now in field units, same as expectedRays ), do the following: totalError += abs(expectedRays[i] - observedRays[i]) Although a derivative-based algorithm may be more efficient at solving the problem, we deemed it far too difficult to calculate the derivative of the objective function.","title":"Position optimisation"},{"location":"omicam/#adding-new-sensor-data-to-the-world-model","text":"Our localisation algorithm is also flexible with new sensor inputs. Due to the use of a \u201cvirtual world model\u201d approach, if any sensors can be modelled as a measurement of a distance to a static object, then they can be integrated as extra data in the world model. This is extremely helpful when vision data runs into limitations, for example, in SuperTeam where it may be difficult to see enough field lines in one frame due to the size of the field. We integrate the LRFs on the robot as extra distance sensors, using their reported distance to the field walls just as we use the raycasted \u201cline points\u201d in the vision based approach. It would also be possible to use the distance to the goals as virtual information to supplement a potential lack of line data and thus increase accuracy. Other distance sensors such as 360 LiDARS can also be implemented with ease.","title":"Adding new sensor data to the world model"},{"location":"omicam/#justification-for-our-approach","text":"Observant readers will notice that the objective function is technically linear (as it contains no exponentials or powers). Hence, it may be observed that linear programming (essentially linear optimisation) could be used to solve the problem. In fact, it\u2019s very plausible that something like Dantzig\u2019s Simplex/criss-cross algorithm or an interior point algorithm could locate the optimum far more efficiently than the Subplex/Nelder-Mead simplex optimiser. However, there is a distinct lack of easy to use and non-restrictively licensed linear optimisers for C. The best candidate was probably COIN-OR\u2019s CLP or Google\u2019s Glop, but both of these methods are quite complex compared to NLopt\u2019s Subplex algorithm (and also written in C++). We determined that given the performance of the localiser (about 30 Hz maximum) and mouse sensor interpolation, it\u2019s not worth switching to a linear optimiser when the non-linear one works fine and fast enough. Finally, it was brought to our attention that it may be possible to forgo the optimisation process and sum the error of the rays in some geometric method to transform the robot\u2019s position. While we agree that this may be possible and worth looking into, we believe that optimisation leads to more stable and less error-prone results (though this is not confirmed).","title":"Justification for our approach"},{"location":"omicam/#interfacing-with-omicontrol","text":"To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as the temperature of the SBC. Although C isn\u2019t an officially supported language by Google for Protocol Buffers, we use the mature nanopb library to do the encoding. This is the same library used on the ESP32 and Teensy as well, and so far we\u2019ve had no issues with it. We use the SIMD optimised libjpeg-turbo to efficiently encode JPEG frames, so as to not waste performance to the remote debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit images, it was determined that zlib could compress them more efficiently (around about 460,800x reduction in size). With all these optimisations, even at high framerates (60+ packets per second), the remote debug system uses no more than 1 MB/s of outgoing bandwidth, which is small enough to work reliably on both local and Internet networks.","title":"Interfacing with Omicontrol"},{"location":"omicam/#debugging-and-performance-optimisation","text":"To achieve Omicam\u2019s performance, we made heavy use of parallel programming techniques, OpenCV\u2019s x86 SIMD CPU optimisations, Clang optimiser flags as well as intelligent image downscaling for the goal threshold images (as they are mostly unused). In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application is asynchronous. Low-level compiled languages such as C and C++ are notoriously unstable and difficult to debug. In order to improve the stability of Omicam and fix bugs, we used Google\u2019s Address Sanitizer (ASan) and Undefined Behaviour Sanitizer (UBSan) to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. In addition, we used the LLVM toolchain\u2019s debugger lldb (or just gdb) to analyse the application frequently. To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application. Although the Clang compiler may have marginally worse performance than GCC, we chose Clang because it\u2019s more modern and has better debugging support (namely, GCC\u2019s Adress Sanitizer implementation is broken for us). To improve performance of the localiser, we use the last extrapolated position from the mouse sensor as a seed for the initial position of the next search. This means instead of starting from a random position, the localiser will complete much quickly a it\u2019s already relatively close to the true position. Also cover Linux CPU optimisation and associated thermal issues if relevant","title":"Debugging and performance optimisation"},{"location":"omicam/#references","text":"C. Grana, D. Borghesani, and R. Cucchiara, \u201cOptimized Block-Based Connected Components Labeling With Decision Trees,\u201d IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596\u20131609, 2010, doi: 10.1109/TIP.2010.2044963. \u21a9 H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, \u201cRobust and real-time self-localization based on omnidirectional vision for soccer robots,\u201d Adv. Robot., vol. 27, no. 10, pp. 799\u2013811, Jul. 2013, doi: 10.1080/01691864.2013.785473. \u21a9 T. H. Rowan, \u201cFunctional stability analysis of numerical algorithms,\u201d Unpuplished Diss., p. 218, 1990. \u21a9 Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt \u21a9","title":"References"},{"location":"omicontrol/","text":"Omicontrol Omicontrol is Team Omicron\u2019s wireless robot/camera debugging and management application. With the creation of Omicam, we needed a way of visualising camera output, managing the SBC and editing thresholds. In the past, we used the OpenMV IDE, but given that Omicam is entirely custom developed, this meant we also had to custom-develop our own camera application. In addition, with our new advanced strategies and localisation, as well as the potential auto referee future rule addition, we felt like this year was a good time to add a remote robot management feature with the ability to reposition, reorient and automatically reset robots on the field wirelessly, as well as visualise their positions. We added both of these features into Omicontrol, to make it essentially our all-in-one robot management application. Because the application will be used frequently in debugging as well as in the high-pressure mid-game interval, Omicontrol\u2019s primary design goals are to be easy to use and reliable. Omicontrol is a cross-platform application that is confirmed to work on Windows, Mac and Linux. It\u2019s developed in Kotlin and uses JavaFX for the GUI. Figure 1: Omicontrol main window running on KDE neon TODO: maybe make the above a video demonstration fo Omicontrol Wireless connection Being able to connect to any robot wirelessly and manage it is a great quality of life improvement, which saves looking for cables, troubleshooting connection problems and more. There are a variety of wireless protocols in existence such as Bluetooth Classic, Bluetooth LE, Zigbee and others. However, we decided to use TCP/IP over WiFi because it\u2019s easy to setup and reliable, compared to Bluetooth which has issues on many platforms. To accomplish this, the LattePanda SBC creates a WiFi access point (AP) with no internet connection that the client computer then connects to, establishing a direct link between the two computers. As shown in Figure 1, to connect you simply have to enter the LattePanda\u2019s IP on your local network. Due to the nature of TCP/IP, we also observed the ability to connect to Omicontrol across the Internet. In this setup, an Internet-connected router hosts the LattePanda device on its network via Ethernet or WiFi. The router must port forward port 42708 which is used for Omicam to Omicontrol communications. Then, the client computer in an entirely different location, even a different country, connects to the Omicam router\u2019s public IP address and can interact with the camera and robots as normal. While this is interesting, nonetheless it\u2019s not used in practice due to lack of a security-focused design or a need to connect across large distances. Communication protocol Communication between the two devices makes heavy use of Protocol Buffers, on the Omicam via nanopb, and on the Omicontrol end in Kotlin via the official Java API from Google. Although there\u2019s no Kotlin API for Protocol Buffers, Kotlin is fully backwards compatible with Java so there\u2019s no issue using the generated Java files.","title":"Omicontrol (debugging and control)"},{"location":"omicontrol/#omicontrol","text":"Omicontrol is Team Omicron\u2019s wireless robot/camera debugging and management application. With the creation of Omicam, we needed a way of visualising camera output, managing the SBC and editing thresholds. In the past, we used the OpenMV IDE, but given that Omicam is entirely custom developed, this meant we also had to custom-develop our own camera application. In addition, with our new advanced strategies and localisation, as well as the potential auto referee future rule addition, we felt like this year was a good time to add a remote robot management feature with the ability to reposition, reorient and automatically reset robots on the field wirelessly, as well as visualise their positions. We added both of these features into Omicontrol, to make it essentially our all-in-one robot management application. Because the application will be used frequently in debugging as well as in the high-pressure mid-game interval, Omicontrol\u2019s primary design goals are to be easy to use and reliable. Omicontrol is a cross-platform application that is confirmed to work on Windows, Mac and Linux. It\u2019s developed in Kotlin and uses JavaFX for the GUI. Figure 1: Omicontrol main window running on KDE neon TODO: maybe make the above a video demonstration fo Omicontrol","title":"Omicontrol"},{"location":"omicontrol/#wireless-connection","text":"Being able to connect to any robot wirelessly and manage it is a great quality of life improvement, which saves looking for cables, troubleshooting connection problems and more. There are a variety of wireless protocols in existence such as Bluetooth Classic, Bluetooth LE, Zigbee and others. However, we decided to use TCP/IP over WiFi because it\u2019s easy to setup and reliable, compared to Bluetooth which has issues on many platforms. To accomplish this, the LattePanda SBC creates a WiFi access point (AP) with no internet connection that the client computer then connects to, establishing a direct link between the two computers. As shown in Figure 1, to connect you simply have to enter the LattePanda\u2019s IP on your local network. Due to the nature of TCP/IP, we also observed the ability to connect to Omicontrol across the Internet. In this setup, an Internet-connected router hosts the LattePanda device on its network via Ethernet or WiFi. The router must port forward port 42708 which is used for Omicam to Omicontrol communications. Then, the client computer in an entirely different location, even a different country, connects to the Omicam router\u2019s public IP address and can interact with the camera and robots as normal. While this is interesting, nonetheless it\u2019s not used in practice due to lack of a security-focused design or a need to connect across large distances.","title":"Wireless connection"},{"location":"omicontrol/#communication-protocol","text":"Communication between the two devices makes heavy use of Protocol Buffers, on the Omicam via nanopb, and on the Omicontrol end in Kotlin via the official Java API from Google. Although there\u2019s no Kotlin API for Protocol Buffers, Kotlin is fully backwards compatible with Java so there\u2019s no issue using the generated Java files.","title":"Communication protocol"},{"location":"open_source/","text":"Open source acknowledgement Team Omicron\u2019s software developers would like to acknowledge the use of the following open source libraries in our projects: Omicam LICENSES HERE Omicontrol LICENSES HERE ESP32 firmware LICENSES HERE ATMega/Teensy firmware LICENSES HERE","title":"Open source acknowledgement"},{"location":"open_source/#open-source-acknowledgement","text":"Team Omicron\u2019s software developers would like to acknowledge the use of the following open source libraries in our projects:","title":"Open source acknowledgement"},{"location":"open_source/#omicam","text":"LICENSES HERE","title":"Omicam"},{"location":"open_source/#omicontrol","text":"LICENSES HERE","title":"Omicontrol"},{"location":"open_source/#esp32-firmware","text":"LICENSES HERE","title":"ESP32 firmware"},{"location":"open_source/#atmegateensy-firmware","text":"LICENSES HERE","title":"ATMega/Teensy firmware"},{"location":"protobuf/","text":"Protocol Buffers","title":"Protocol Buffers"},{"location":"protobuf/#protocol-buffers","text":"","title":"Protocol Buffers"},{"location":"strategy/","text":"Game strategies With the hardware features provided by our double kicker and dribbler, and the software features provided by Omicam and the FSM, our team was able to deploy advanced gameplay strategies","title":"Game strategies"},{"location":"strategy/#game-strategies","text":"With the hardware features provided by our double kicker and dribbler, and the software features provided by Omicam and the FSM, our team was able to deploy advanced gameplay strategies","title":"Game strategies"},{"location":"structural_design/","text":"Hardware Design This year we focused on designing a simple and robust robot which was quick to assemble and disassemble when needed. We also aimed on improving hardware from previous years including the custom omni-directional wheels introduced last year. The following are the major hardware improvements that have occurred this year: Custom omni-directional wheels Custom kicker unit Custom microcontroller circuits Custom camera hardware Mouse sensor implemenation Various debugging options for software to use Design Overview TODO: PUT PICTURE OF DESIGN HERE, POSSIBLE EXPLODED VIEW OR A360 LINK Omni-directional Wheels Omni-directional wheels (also referred to as omniwheels ) are specially designed to give robots full freedom of motion, enabling them to travel in any direction without turning. The downside to these wheels is the relative lack of tracktion and robustness when compared to conventional tires due to the individual rollers. As such, the primary goal of new iterations is to increase the grip and distance of the wheels. The following image shows the development of the omniwheel used throughout the years of competition with the rightmost being the latest design: TODO: PUT PICTURE HERE Past Iterations The two leftmost designs, the Kornylak and GTF omniwheel are stock designs bought pre-made from the manufacturer. Through using both these wheels for a year, we discovered that they were lacking in grip and robustness respectively, leading us to develop our own designs. The third design was our first attempt at a custom omniwheel, featuring double layered rollers and individual roller axles for faster repair. Though it allowed for much smoother movement and better traction, the wheels were large and unwieldly and the rollers still broke due to the use of softer silicon rollers. Furthermore, the use of 3D printed components caused rollers to fall out of the wheel during games. TODO: PUT PICTURES OF NATOINALS WHEEL, DOUBLE WHEEL WITH SMALL ROLLERS AND BIG WHEEL WITH SMALL ROLLERS Development of current wheel The leftmost design was a revised version of your first custom omniwheel. Various different materials were lasercut in order to replace the 3D printed components. After extensive testing, it was decided that PVC was to be used as it was durable, cheap and easy to laser cut. Alternatives materials included acrylic, which was not strong enough, and aluminium, which was too expensive to manufacture. The middle design is our second attempt at a custom omniwheel, once again featuring double layered rollers. However, this time smaller diameter rollers were used to allow the wheel to be much more compact. Consequently, this permitted the use of wheel protection guards, as seen in the image below. This prevented the wheels from getting damaged, often a result of the wheels grinding up against other robots or objects. In addition, the individual roller axels were replaced with metal wire to prevent the rollers from falling out. The rightmost design is a modified version of the middle design. Instead of having lots of smaller rollers, fewer longer rollers are used instead. This makes the wheel much easier to take apart and the rollers less likely to break. TODO: modify paragraph above once final design has been chosen to demonstrate the \u201cdevelopment flow\u201d TODO: Explain new design when it decides to exist Kicker Unit TODO: Idek if this works Chip-on-Board Microcontrollers Custom Vision System The primary goal for this year was to improve the vision system of the robot by developing a custom vision pipeline. This was achieved with the Debugging Further improvements upon the debugging options were made this year including: * 4 piezzoelectric buzzers for audio feedback * 8 LEDs and a text LCD screen for visual feedback * Wireless degbugging UI for easy visualisation","title":"Structural design"},{"location":"structural_design/#hardware-design","text":"This year we focused on designing a simple and robust robot which was quick to assemble and disassemble when needed. We also aimed on improving hardware from previous years including the custom omni-directional wheels introduced last year. The following are the major hardware improvements that have occurred this year: Custom omni-directional wheels Custom kicker unit Custom microcontroller circuits Custom camera hardware Mouse sensor implemenation Various debugging options for software to use","title":"Hardware Design"},{"location":"structural_design/#design-overview","text":"TODO: PUT PICTURE OF DESIGN HERE, POSSIBLE EXPLODED VIEW OR A360 LINK","title":"Design Overview"},{"location":"structural_design/#omni-directional-wheels","text":"Omni-directional wheels (also referred to as omniwheels ) are specially designed to give robots full freedom of motion, enabling them to travel in any direction without turning. The downside to these wheels is the relative lack of tracktion and robustness when compared to conventional tires due to the individual rollers. As such, the primary goal of new iterations is to increase the grip and distance of the wheels. The following image shows the development of the omniwheel used throughout the years of competition with the rightmost being the latest design: TODO: PUT PICTURE HERE","title":"Omni-directional Wheels"},{"location":"structural_design/#past-iterations","text":"The two leftmost designs, the Kornylak and GTF omniwheel are stock designs bought pre-made from the manufacturer. Through using both these wheels for a year, we discovered that they were lacking in grip and robustness respectively, leading us to develop our own designs. The third design was our first attempt at a custom omniwheel, featuring double layered rollers and individual roller axles for faster repair. Though it allowed for much smoother movement and better traction, the wheels were large and unwieldly and the rollers still broke due to the use of softer silicon rollers. Furthermore, the use of 3D printed components caused rollers to fall out of the wheel during games. TODO: PUT PICTURES OF NATOINALS WHEEL, DOUBLE WHEEL WITH SMALL ROLLERS AND BIG WHEEL WITH SMALL ROLLERS","title":"Past Iterations"},{"location":"structural_design/#development-of-current-wheel","text":"The leftmost design was a revised version of your first custom omniwheel. Various different materials were lasercut in order to replace the 3D printed components. After extensive testing, it was decided that PVC was to be used as it was durable, cheap and easy to laser cut. Alternatives materials included acrylic, which was not strong enough, and aluminium, which was too expensive to manufacture. The middle design is our second attempt at a custom omniwheel, once again featuring double layered rollers. However, this time smaller diameter rollers were used to allow the wheel to be much more compact. Consequently, this permitted the use of wheel protection guards, as seen in the image below. This prevented the wheels from getting damaged, often a result of the wheels grinding up against other robots or objects. In addition, the individual roller axels were replaced with metal wire to prevent the rollers from falling out. The rightmost design is a modified version of the middle design. Instead of having lots of smaller rollers, fewer longer rollers are used instead. This makes the wheel much easier to take apart and the rollers less likely to break. TODO: modify paragraph above once final design has been chosen to demonstrate the \u201cdevelopment flow\u201d TODO: Explain new design when it decides to exist","title":"Development of current wheel"},{"location":"structural_design/#kicker-unit","text":"TODO: Idek if this works","title":"Kicker Unit"},{"location":"structural_design/#chip-on-board-microcontrollers","text":"","title":"Chip-on-Board Microcontrollers"},{"location":"structural_design/#custom-vision-system","text":"The primary goal for this year was to improve the vision system of the robot by developing a custom vision pipeline. This was achieved with the","title":"Custom Vision System"},{"location":"structural_design/#debugging","text":"Further improvements upon the debugging options were made this year including: * 4 piezzoelectric buzzers for audio feedback * 8 LEDs and a text LCD screen for visual feedback * Wireless degbugging UI for easy visualisation","title":"Debugging"}]}