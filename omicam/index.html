



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Documentation website for Team Omicron from BBC Robotics, competing in the RoboCup Jr 2020 Internationals.">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.6.0">
    
    
      
        <title>Omicam (vision and localisastion) - Team Omicron</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.1b62728e.css">
      
      
    
    
      <script src="../assets/javascripts/modernizr.268332fc.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#omicam" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="Team Omicron" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Team Omicron
            </span>
            <span class="md-header-nav__topic">
              
                Omicam (vision and localisastion)
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="Team Omicron" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Team Omicron
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../about/" title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Hardware
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Hardware
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../structural_design/" title="Structural design" class="md-nav__link">
      Structural design
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../electrical_design/" title="Electrical design" class="md-nav__link">
      Electrical design
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      Software
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Software
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Omicam (vision and localisastion)
      </label>
    
    <a href="./" title="Omicam (vision and localisastion)" class="md-nav__link md-nav__link--active">
      Omicam (vision and localisastion)
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background-and-previous-methods" class="md-nav__link">
    Background and previous methods
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-and-results" class="md-nav__link">
    Performance and results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    Hardware
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prototypes" class="md-nav__link">
    Prototypes
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#field-object-detection" class="md-nav__link">
    Field object detection
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#localisation" class="md-nav__link">
    Localisation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#previous-methods-and-background" class="md-nav__link">
    Previous methods and background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-solution" class="md-nav__link">
    Our solution
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#image-analysis" class="md-nav__link">
    Image analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-normalisation" class="md-nav__link">
    Camera normalisation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-optimisation" class="md-nav__link">
    Position optimisation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adding-new-sensor-data-to-the-world-model" class="md-nav__link">
    Adding new sensor data to the world model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#justification-for-our-approach" class="md-nav__link">
    Justification for our approach
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interfacing-with-omicontrol" class="md-nav__link">
    Interfacing with Omicontrol
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#debugging-and-performance-optimisation" class="md-nav__link">
    Debugging and performance optimisation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../omicontrol/" title="Omicontrol (debugging and control)" class="md-nav__link">
      Omicontrol (debugging and control)
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../strategy/" title="Game strategies" class="md-nav__link">
      Game strategies
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../protobuf/" title="Protocol Buffers" class="md-nav__link">
      Protocol Buffers
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../open_source/" title="Open source acknowledgement" class="md-nav__link">
      Open source acknowledgement
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background-and-previous-methods" class="md-nav__link">
    Background and previous methods
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-and-results" class="md-nav__link">
    Performance and results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    Hardware
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prototypes" class="md-nav__link">
    Prototypes
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#field-object-detection" class="md-nav__link">
    Field object detection
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#localisation" class="md-nav__link">
    Localisation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#previous-methods-and-background" class="md-nav__link">
    Previous methods and background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-solution" class="md-nav__link">
    Our solution
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#image-analysis" class="md-nav__link">
    Image analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-normalisation" class="md-nav__link">
    Camera normalisation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-optimisation" class="md-nav__link">
    Position optimisation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adding-new-sensor-data-to-the-world-model" class="md-nav__link">
    Adding new sensor data to the world model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#justification-for-our-approach" class="md-nav__link">
    Justification for our approach
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interfacing-with-omicontrol" class="md-nav__link">
    Interfacing with Omicontrol
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#debugging-and-performance-optimisation" class="md-nav__link">
    Debugging and performance optimisation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="omicam">Omicam</h1>
<p>One of the biggest innovation Team Omicron brings this year is our advanced, custom developed vision and
localisation application called <em>Omicam</em>. The application is developed mostly in C, with some C++ code to interface with
OpenCV. It runs on the powerful LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS.</p>
<p>We are proud to report that Omicam is a significant step up compared to previous vision applications in use at BBC
Robotics, as explained in the &ldquo;Performance and results&rdquo; section.</p>
<p>Omicam consists of <strong>X</strong> lines of code, and took about <strong>X</strong> hours to develop.</p>
<h2 id="background-and-previous-methods">Background and previous methods</h2>
<p>Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer.
With the introduction of the orange ball and advanced teams&rsquo; application of &ldquo;ball-hiding&rdquo; strategies, high resolution yet
performant computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. However, detecting the
field objects (ball, goals, etc) is now only the bare minimum. Advanced teams also need to accurately estimate their 
position on the field (localise) in order to execute advanced strategies and gain the upper hand in the competition.</p>
<p>Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with
an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although
this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the
best solution was to do what the most advanced teams were doing, and develop a custom vision application running on a 
single board computer (SBC). In terms of localisation, in the past we managed to get away with not using any, or using
a low-fidelity approach based on detecting the goals in the image.</p>
<h2 id="performance-and-results">Performance and results</h2>
<p>Omicam is capable of detecting 4 field objects at <strong>60-70fps at 720p (1280x720) resolution</strong>. Compared to the previous
OpenMV H7, this is <strong>23x higher resolution at 3x the framerate.<sup>1</sup></strong></p>
<p>In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the
robot&rsquo;s position to <strong>&lt;1cm accuracy</strong> at realtime speeds. This is over <strong>5x/25x more accurate<sup>2</sup></strong> than any previous methods 
used at BBC Robotics, and has been shown to be much more reliable and stable.</p>
<p>To achieve this performance, we made heavy use of parallel programming techniques, OpenCV&rsquo;s x86 SIMD CPU optimisations,
Clang optimiser flags as well as intelligent image downscaling for the goal threshold images (as they are mostly unused).
In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application
is asynchronous.</p>
<p><em><sup>1 previous results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution.</sup></em> </p>
<p><em><sup>2 depending on whether LRF based/goal based localisation was used.</sup></em></p>
<h2 id="hardware">Hardware</h2>
<p>Omicam runs on a Linux-based LattePanda Delta 432. It uses an OV4689 USB 2.0 camera module, which is capable of streaming
motion JPEG (MJPEG) at 720p 100+ fps.</p>
<h2 id="prototypes">Prototypes</h2>
<p>The current iteration of Omicam&rsquo;s hardware setup is the cumulation of around 2 years of prototyping iterations.</p>
<p><strong>Prototype 0 (December 2018-January 2019):</strong> Raspberry Pi Zero W, initial prototype sort of stuff. Too slow. Project canned.</p>
<p><strong>Prototype 1 (December 2019):</strong> Raspberry Pi 4, custom CV library, GPU acceleration with MMAL. Too slow.</p>
<p><strong>Prototype 2 (January 2020):</strong> Jetson Nano, OpenCV, CUDA. No performance benefits observed on GPU (slow down in fact).</p>
<p><strong>Prototype 3 (January-February 2020):</strong> LattePanda Delta 432, OpenCV, purely CPU bound. Final iteration.</p>
<p><strong>This is out of date</strong></p>
<p><strong>More here, justifcations for why we picked this hardware.</strong></p>
<h2 id="field-object-detection">Field object detection</h2>
<p>The primary responsibility of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. 
To do this, we use the popular computer vision library OpenCV (v4.2.0). 
We use Video4Linux2 (V4L2) to acquire an MJPEG stream from the USB camera, via the popular gstreamer library.
Then, we apply any pre-processing steps such as downscaling the goal frames and gamma boosting.</p>
<p>Next, we threshold all objects in parallel to make use of our quad-core CPU, using OpenCV&rsquo;s <code>inRange</code> thresholder but a custom
parallel framework (as it doesn&rsquo;t run in parallel by default). Thresholding generates a 1-bit binary mask of the image, where
each pixel is 255 (true) if it&rsquo;s inside the RGB range specified, and 0 (false) if it&rsquo;s not.
Then, we use OpenCV&rsquo;s parallel connected component labeller, specifically the algorithm by Grana et al.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> to detect regions
of the same colour in the image. The largest connected region will be the field object we are looking for. OpenCV automatically
calculates the bounding box and centroid for each of these connected regions.</p>
<p>We then dispatch the largest detected blob&rsquo;s centroid via UART to the ESP32, encoded using Protocol Buffers and using
POSIX termios for UART configuration.</p>
<p><strong>TODO images and/or video of thresholded field</strong></p>
<h2 id="localisation">Localisation</h2>
<h3 id="previous-methods-and-background">Previous methods and background</h3>
<p>Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order
to develop advanced strategies and precise movement control, instead of just driving directly towards the ball.</p>
<p>Currently, teams use three main ways of localisation. Firstly, the simplest approach uses the detected goals
in the camera to estimate the robot&rsquo;s position. This approach is very inaccurate because of the low resolution of
most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes
the goals are not visible (especially in super team). Expected accuracy is 15-25cm.</p>
<p>The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using
several of these sensors on a robot, the position of the robot can be inferred with trigonometry, by measuring the distance
to the walls. This approach has a major drawback: it is impossible to reliably distinguish between anomalous objects,
such as a hand or another robot, compared to a wall. This means that although this approach is somewhat accurate on an empty
field, it is very difficult to use reliably in actual games. Thus, localisation data was almost never trusted by teams
who used this approach and so is not very helpful. In addition, distances sensors, especially ultrasonics, are slow and suffer 
from inaccuracy at long distances. Expected accuracy is 5-10cm, but data is invalidated every time a robot moves in the way of a sensor,
which is impossible to know.</p>
<p>The third approach in use by some teams is one based on 360 degree LiDARS. These are expensive, heavy, difficult to use, slow and are
still vulnerable to all the problems the second approach suffers from as well. We are not aware of expected accuracy, but regard
this as not an ideal approach.</p>
<h3 id="our-solution">Our solution</h3>
<p>This year, Team Omicron presents a novel approach to robot localisation based partly on a middle-size league paper by Lu, Li, Zhang,
Hu &amp; Zheng<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. We localise using only RGB camera data by solving a multi-variate optimisation problem in realtime, via the
Subplex non-linear gradient-free optimisation algorithm.</p>
<p>The principle method of operation of our algorithm is that we need to match a virtual model of field geometry to the observed
one from the camera, thereby inferring our position. Another way to think of it is we need to generate a transform such that
a virtual model of the field will align with the real one, thereby also calculating our 2D position.<br />
If we match the lines so that they align in both the virtual model and real-world model, then we can infer that the 
calculated virtual robot coordinates are the same as the real, unknown robot coordinates. Essentially, we&rsquo;re
taking what we know: the static layout of the field, and observed field geometry at our current position, and using it to
infer the unknown 2D position vector.</p>
<p>This is a form of the orthogonal Procrustes problem, which can be solved through a multitude of approaches such as iterative
closest point (commonly used with 3D LiDARS), Monte-Carlo localisation via a particle filter or gradient fields. However,
most of these approaches also consider rotation as a factor. Due to to our use of a high-accuracy BNO-055 IMU, we consider
rotation to be a non-issue that can be easily corrected for, thereby reducing the complexity of the problem. 
Thus, we developed a novel three step algorithm involving ray-casting to infer our 2D position, ignoring rotation as a factor 
to be solved for.</p>
<p>These steps are:</p>
<ol>
<li>Image analysis</li>
<li>Camera normalisation</li>
<li>Coordinate optimisation</li>
</ol>
<h4 id="image-analysis">Image analysis</h4>
<p>The localiser&rsquo;s input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding
for the colour white, which is handled by the vision pipeline described earlier.</p>
<p>With the input provided, a certain number of rays (usually 128) are emitted from the centre of the line image. A ray
terminates when it touches a line, reaches the edge of the image or reaches the edge of the mirror (as it would be a
waste of time to check outside the mirror). The theory of operation behind this is, essentially, for each field position
each ray should have its own unique distance.</p>
<p>Rays are stored as only a length in a regular C array, as we can infer the angle between each ray as: 2pi / n_rays</p>
<p><img alt="Raycasting" src="../images/raycasting.png" /> <br />
<em>Figure 1: example of ray casting on field, with a position near to the centre</em></p>
<h4 id="camera-normalisation">Camera normalisation</h4>
<p>These rays are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by
measuring the pixels between points along evenly spaced tape placed on the real field, via Omicontrol. Using regression 
software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances. 
In our case, we simply apply the dewarp function to each ray length instead, leaving us
with each ray essentially in field coordinates (or field lengths) rather than camera coordinates.</p>
<p>This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres.</p>
<p><img alt="Dewarped" src="../images/dewarped.png" />  <br />
<em>Figure 2: example of applying the dewarp function to an entire image, on the low resolution OpenMV H7.</em></p>
<p>The second phase of the camera normalisation is to rotate the rays relative to the robot&rsquo;s heading, using a rotation matrix.
The robot&rsquo;s heading value, which is relative to when it was powered on, is transmitted by the ESP32, again using Protocol Buffers.
For information about how this value is calculated using the IMU, see the ESP32 and movement code page.</p>
<h4 id="position-optimisation">Position optimisation</h4>
<p>The main part of our solution is the Subplex local derivative-free non-linear optimiser<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>, re-implemented as 
part of the NLopt package<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup>. This algorithm essentially acts as an efficiency and stability improvement over the well-known 
Nelder-Mead Simplex algorithm.</p>
<p>The most critical part of this process is the <em>objective function</em>, which is a function that takes an N-dimensional vector
(in our case, an estimated 2D position) and calculates essentially a &ldquo;score&rdquo; of how accurate the value is. This
objective function must be highly optimised as it could be evaluated hundreds of times by the optimisation algorithm.</p>
<p><img alt="Objective function" src="../images/objective_function.png" /> <br />
<em>Figure 3: map of objective function for a robot placed at the centre of the field. White pixels indicate high accuracy areas</em>
<em>and black pixels indicate less accurate areas.</em></p>
<p>A naive approach, where every grid cell is evaluated and the lowest value is picked, would require 44,226 objective function
evaluation which takes about 5 seconds on a fast computer. Using the Subplex optimiser, this can be reduced to just 62
evaluations for a 1.2mm accurate fix.</p>
<p><strong>TODO: cover approaches we drafted: line points, etc. Also, use more images in the docs</strong></p>
<p>We spent a great deal of effort drafting the most efficient objective function, and the approach we present makes heavy use
of pre-computation via a &ldquo;field file&rdquo;. This field file is a binary Protcol Buffer file that encodes the geometry of any
RoboCup field by dividing it into a grid, where each cell is true if on a line, otherwise false. Increasing
the resolution of the grid will increase its accuracy, but also significantly increase its file size. We use a 1cm grid,
which stores 44,226 cells and is 42 KiB on disk. This takes about 2 seconds to generate on a fast desktop computer, and
is copied across to the LattePanda.
The field file is generated by a Python script which can be easily modified to support an arbitrary number of different 
field layouts, such as SuperTeam or our regional Australian field. </p>
<p>Although the field file could theoretically be loaded
via the bitmap image below, images are more difficult to load than Protobuf files and it would not support grid cell sizes
smaller or larger than 1cm.</p>
<p><img alt="Field file" src="../images/field_file.png" />  <br />
<em>Figure 3: bitmap image displaying generated field file. In the original 243x182 image, 1 pixel = 1cm</em></p>
<p>The objective function essentially compares the difference between the ray lengths, as follows:</p>
<ol>
<li>Consider the estimated position (eX, eY). Move to (eX, eY) in the field file and raycast out to generate the array
   <code>expectedRays</code>.</li>
<li>For each ray in the dewarped <code>observedRays</code> from the camera (which are now in field units, same as <code>expectedRays</code>),
   do the following: <code>totalError += abs(expectedRays[i] - observedRays[i])</code></li>
</ol>
<p>Although a derivative-based algorithm may be more efficient at solving the problem, we deemed it far too difficult to calculate
the derivative of the objective function.</p>
<h3 id="adding-new-sensor-data-to-the-world-model">Adding new sensor data to the world model</h3>
<p>Our localisation algorithm is also flexible with new sensor inputs. Due to the use of a &ldquo;virtual world model&rdquo; approach, if
any sensors can be modelled as a measurement of a distance to a static object, then they can be integrated as extra data
in the world model.</p>
<p>This is extremely helpful when vision data runs into limitations, for example, in SuperTeam where it may be difficult
to see enough field lines in one frame due to the size of the field. We integrate the LRFs on the robot as
extra distance sensors, using their reported distance to the field walls just as we use the raycasted &ldquo;line points&rdquo; in
the vision based approach.</p>
<p>It would also be possible to use the distance to the goals as virtual information to supplement a potential lack of line
data and thus increase accuracy. Other distance sensors such as 360 LiDARS can also be implemented with ease.</p>
<h3 id="justification-for-our-approach">Justification for our approach</h3>
<p>Observant readers will notice that the objective function is technically linear (as it contains no exponentials or powers).
Hence, it may be observed that linear programming (essentially linear optimisation) could be used to solve the problem. In
fact, it&rsquo;s very plausible that something like Dantzig&rsquo;s Simplex/criss-cross algorithm or an interior point algorithm could
locate the optimum far more efficiently than the Subplex/Nelder-Mead simplex optimiser.</p>
<p>However, there is a distinct lack of easy to use and non-restrictively licensed linear optimisers for C. The best candidate
was probably COIN-OR&rsquo;s CLP or Google&rsquo;s Glop, but both of these methods are quite complex compared to NLopt&rsquo;s Subplex algorithm
(and also written in C++). We determined that given the performance of the localiser (about 30 Hz maximum) and mouse sensor 
interpolation, it&rsquo;s not worth switching to a linear optimiser when the non-linear one works fine and fast enough.</p>
<p>Finally, it was brought to our attention that it may be possible to forgo the optimisation process and sum the error of the
rays in some geometric method to transform the robot&rsquo;s position. While we agree that this may be possible and worth looking into, 
we believe that optimisation leads to more stable and less error-prone results (though this is not confirmed).</p>
<h2 id="interfacing-with-omicontrol">Interfacing with Omicontrol</h2>
<p>To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends
Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as
the temperature of the SBC. Although C isn&rsquo;t an officially supported language by Google for Protocol Buffers, we use the mature
nanopb library to do the encoding. This is the same library used on the ESP32 and Teensy as well, and so far we&rsquo;ve had no
issues with it.</p>
<p>We use the SIMD optimised libjpeg-turbo to efficiently encode JPEG frames, so as to not waste performance to the remote debugger
(which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit images,
it was determined that zlib could compress them more efficiently (around about 460,800x reduction in size). </p>
<p>With all these optimisations, even at high framerates (60+ packets per second), the remote debug system uses no more than
1 MB/s of outgoing bandwidth, which is small enough to work reliably on both local and Internet networks.</p>
<h2 id="debugging-and-performance-optimisation">Debugging and performance optimisation</h2>
<p>Low-level compiled languages such as C and C++ are notoriously unstable and difficult to debug. In order to improve the stability of Omicam
and fix bugs, we used Google&rsquo;s Address Sanitizer (ASan) and Undefined Behaviour Sanitizer (UBSan) to easily find and trace a variety of 
bugs such as buffer overflows, memory leaks and more. 
In addition, we used the LLVM toolchain&rsquo;s debugger lldb (or just gdb) to analyse the application frequently.</p>
<p>To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application.
Although the Clang compiler may have marginally worse performance than GCC, we chose Clang because it&rsquo;s more modern and has
better debugging support (namely, GCC&rsquo;s Adress Sanitizer implementation is broken for us).</p>
<p>To improve performance of the localiser, we use the last extrapolated position from the mouse sensor as a seed for the initial
position of the next search. This means instead of starting from a random position, the localiser will complete much quickly
a it&rsquo;s already relatively close to the true position.</p>
<p><strong>Also cover Linux CPU optimisation and associated thermal issues if relevant</strong></p>
<h2 id="references">References</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>C. Grana, D. Borghesani, and R. Cucchiara, “Optimized Block-Based Connected Components Labeling With Decision Trees,” IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596–1609, 2010, doi: 10.1109/TIP.2010.2044963.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, “Robust and real-time self-localization based on omnidirectional vision for soccer robots,” Adv. Robot., vol. 27, no. 10, pp. 799–811, Jul. 2013, doi: 10.1080/01691864.2013.785473.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>T. H. Rowan, “Functional stability analysis of numerical algorithms,” Unpuplished Diss., p. 218, 1990.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../electrical_design/" title="Electrical design" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Electrical design
              </span>
            </div>
          </a>
        
        
          <a href="../omicontrol/" title="Omicontrol (debugging and control)" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Omicontrol (debugging and control)
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright (c) 2020 Team Omicron.
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.808e90bb.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>