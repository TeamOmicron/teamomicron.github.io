



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Documentation website for Team Omicron from BBC Robotics, competing in the RoboCup Jr 2020 Internationals.">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-4.6.3">
    
    
      
        <title>Omicam - Team Omicron</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.adb8469c.css">
      
      
    
    
      <script src="../assets/javascripts/modernizr.86422ebf.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    <body dir="ltr">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#background-and-previous-methods" tabindex="0" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="Team Omicron" aria-label="Team Omicron" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Team Omicron
            </span>
            <span class="md-header-nav__topic">
              
                Omicam
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" aria-label="search" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="Team Omicron" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Team Omicron
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../about/" title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Hardware
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Hardware
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../structural_design/" title="Structural design" class="md-nav__link">
      Structural design
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../electrical_design/" title="Electrical design" class="md-nav__link">
      Electrical design
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      Software
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Software
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Omicam
      </label>
    
    <a href="./" title="Omicam" class="md-nav__link md-nav__link--active">
      Omicam
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background-and-previous-methods" class="md-nav__link">
    Background and previous methods
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-and-results" class="md-nav__link">
    Performance and results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    Hardware
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbc-iterations" class="md-nav__link">
    SBC iterations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-module-iterations" class="md-nav__link">
    Camera module iterations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#field-object-detection" class="md-nav__link">
    Field object detection
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pre-processing" class="md-nav__link">
    Pre-processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thresholding-and-component-labelling" class="md-nav__link">
    Thresholding and component labelling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordinate-transforms-and-dispatch" class="md-nav__link">
    Coordinate transforms and dispatch
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#localisation" class="md-nav__link">
    Localisation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#previous-methods-and-background" class="md-nav__link">
    Previous methods and background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-approach" class="md-nav__link">
    Our approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimate-calculation" class="md-nav__link">
    Estimate calculation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-analysis" class="md-nav__link">
    Image analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-normalisation" class="md-nav__link">
    Camera normalisation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordinate-optimisation-and-interpolation" class="md-nav__link">
    Coordinate optimisation and interpolation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#drawbacks" class="md-nav__link">
    Drawbacks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#extramiscellaneous-features" class="md-nav__link">
    Extra/miscellaneous features
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interfacing-with-omicontrol" class="md-nav__link">
    Interfacing with Omicontrol
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    Configuration
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-recording-and-match-replay" class="md-nav__link">
    Video recording and match replay
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#debugging-and-performance-optimisation" class="md-nav__link">
    Debugging and performance optimisation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../omicontrol/" title="Omicontrol" class="md-nav__link">
      Omicontrol
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../strategy/" title="Game strategies" class="md-nav__link">
      Game strategies
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../protobuf/" title="Protocol Buffers" class="md-nav__link">
      Protocol Buffers
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../fsm/" title="Finite State Machine" class="md-nav__link">
      Finite State Machine
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lowlevel/" title="Low-level code" class="md-nav__link">
      Low-level code
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../open_source/" title="Open source" class="md-nav__link">
      Open source
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background-and-previous-methods" class="md-nav__link">
    Background and previous methods
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-and-results" class="md-nav__link">
    Performance and results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    Hardware
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sbc-iterations" class="md-nav__link">
    SBC iterations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-module-iterations" class="md-nav__link">
    Camera module iterations
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#field-object-detection" class="md-nav__link">
    Field object detection
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pre-processing" class="md-nav__link">
    Pre-processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thresholding-and-component-labelling" class="md-nav__link">
    Thresholding and component labelling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordinate-transforms-and-dispatch" class="md-nav__link">
    Coordinate transforms and dispatch
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#localisation" class="md-nav__link">
    Localisation
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#previous-methods-and-background" class="md-nav__link">
    Previous methods and background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-approach" class="md-nav__link">
    Our approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimate-calculation" class="md-nav__link">
    Estimate calculation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-analysis" class="md-nav__link">
    Image analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-normalisation" class="md-nav__link">
    Camera normalisation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordinate-optimisation-and-interpolation" class="md-nav__link">
    Coordinate optimisation and interpolation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#drawbacks" class="md-nav__link">
    Drawbacks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#extramiscellaneous-features" class="md-nav__link">
    Extra/miscellaneous features
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interfacing-with-omicontrol" class="md-nav__link">
    Interfacing with Omicontrol
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    Configuration
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-recording-and-match-replay" class="md-nav__link">
    Video recording and match replay
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#debugging-and-performance-optimisation" class="md-nav__link">
    Debugging and performance optimisation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Omicam</h1>
                
                <p><img alt="Omicam logo" src="../images/omicam_logo_dark.png" />  <br />
One of the biggest innovation Team Omicron brings this year is our advanced, custom developed vision and
localisation application called <em>Omicam</em>. Omicam handles the complex process of detecting the ball and goals, determining
the robot&rsquo;s position on the field as well as encoding/transmitting this information, in one codebase.</p>
<p>The application is developed mostly in C, with some C++ code to interface with OpenCV. 
It runs on the LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS.</p>
<p>We are proud to report that Omicam is a significant improvement compared to previous vision applications in use at BBC
Robotics, as explained in the &ldquo;Performance and results&rdquo; section.</p>
<h2 id="background-and-previous-methods">Background and previous methods</h2>
<p>Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer.
With the introduction of the orange ball and advanced teams&rsquo; application of &ldquo;ball-hiding&rdquo; strategies, high resolution yet
performant computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. However, detecting the
field objects (ball, goals, etc) is now only the bare minimum. Advanced teams also need to accurately estimate their 
position on the field (localise) in order to execute advanced strategies and gain the upper hand in the competition.</p>
<p>Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined with
an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However, although
this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we decided the
best solution was to do what the most advanced teams were doing, and develop a custom vision application running on a 
single board computer (SBC). In terms of localisation, in the past we managed to get away with not using any, or using
a low-fidelity approach based on detecting the goals in the image. With the introduction of strategies this year, however,
team members needed more accurate position data, which requires more complex localisation.</p>
<h2 id="performance-and-results">Performance and results</h2>
<p><strong>TODO provide some more empiric data (tables and stuff) here. Maybe also provide a Pitfalls section at the end.</strong></p>
<p>Omicam is capable of detecting the ball, goals and lines at 60-70fps at 720p (1280x720) resolution. 
Compared to the previous OpenMV H7, this is 23x higher resolution at 3x the framerate.<sup>1</sup></p>
<p>In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the
robot&rsquo;s position to around 1.5cm accuracy at roughly 30 Hz. This is over 5x/25x more accurate<sup>2</sup>
than any previous methods used at BBC Robotics, and has been shown to be much more reliable and stable.</p>
<p>Using the e-con Systems Hyperyon camera based around the ultra low-light performance IMX290 sensor, Omicam
is robust against lighting conditions ranging from near pitch darkness to direct LED light.</p>
<p><em><sup>1 previous results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution.</sup></em> </p>
<p><em><sup>2 depending on whether LRF based/goal based localisation was used.</sup></em></p>
<h2 id="hardware">Hardware</h2>
<p>Omicam supports any single board computer (SBC) that can run Linux. In our case, we use a LattePanda Delta 432 with a 
2.4 GHz quad-core Intel Celeron N4100, 4GB RAM, 32GB of storage, WiFi, Bluetooth, gigabit Ethernet and a UART bus.</p>
<p>The current camera we use is an e-con Systems Hyperyon, based on the Sony Starvis IMX290 ultra low-light sensor capable
of seeing in almost pitch black at high framerates. This is a USB 2.0 camera module, since the LattePanda has no 
MIPI port.</p>
<h3 id="sbc-iterations">SBC iterations</h3>
<p>The current iteration of Omicam&rsquo;s SBC setup is the cumulation of around 2 years of prototyping iterations in both
hardware and software approaches.</p>
<p><strong>Prototype 1 (December 2018-January 2019):</strong> This consisted of a Raspberry Pi Zero W, with a 1 GHz single-core CPU.
It was our initial prototype for a single-board computer, however, we quickly found it was far too weak to do any vision,
and our inexperience at the time didn&rsquo;t help. Thus, we canned the SBC project for around a year.</p>
<p><strong>Prototype 2 (December 2019):</strong> After resurrecting the SBC R&amp;D project for our 2020 Internationals, we started development
with the Raspberry Pi 4. This has a 1.5 GHz quad-core ARM Cortex-A72 CPU. We began developing a custom computer vision
library tailored specifically to our task, using the Pi&rsquo;s MMAL API for GPU-accelerated camera decoding. Initial results
showed we could threshold images successfully, but we believed it would be too slow to localise and run a
connected-component labeller in real time.</p>
<p><strong>Prototype 3 (January 2020):</strong> Next, we moved onto the NVIDIA Jetson Nano, containing a 1.43 GHz quad-core Cortex-A57,
but far more importantly a 128-core Maxwell GPU. At this time we also switched to using OpenCV 4 for our computer vision.
In theory, using the GPU for processing would lead to a huge performance boost due to the parallelism, however, in practice
we observed the GPU was significantly slower than even the weaker Cortex-A43 CPU, (presumably) due to copying times. We
were unable to optimise this to standard after weeks of development, thus we decided to move on from this prototype.</p>
<p><strong>Prototype 4 (January-February 2020):</strong> After much research, we decided to use the LattePanda Delta 432. The OpenCV
pipeline is now entirely CPU bound, and despite not using the GPU at all, we are able to achieve good performance.</p>
<h3 id="camera-module-iterations">Camera module iterations</h3>
<p>In addition to the SBC, the camera module has undergone many iterations, as it&rsquo;s also an important element in the 
vision pipeline.</p>
<p><strong>Pi Camera:</strong> The initial camera we used in hardware prototypes 1-3, was the Raspberry Pi Camera. 
In prototype 1, we used the Pi Cam v1.3 which is an OV5647 connected via MIPI, and in prototypes 2 and 3 we used the 
Pi Cam v2 which is an IMX219 again connected via MIPI. We had to drop this camera in later iterations because
the LattePanda doesn&rsquo;t have a MIPI port. Both of these cameras were capable of around 720p at 60 fps.</p>
<p><strong>OV4689-based module:</strong> We experimented with a generic OV4689 USB 2.0 camera module from Amazon, which is capable of
streaming JPEGs (aka an MJPEG stream) at 720p at 120 fps (we could get around 90 fps in practice with no processing).
While this framerate was useful, the camera module suffered from noise and flickering in relatively good
lighting conditions, so it was dropped.</p>
<p><strong>e-con Hyperyon:</strong> After these two failures, we began looking into industrial-grade cameras to use on
our robot. While most of these, from companies like FLIR, are out of our price range, we found e-con Systems as a
relatively affordable vendor of high quality cameras. We narrowed down our selection to two devices: the
See3CAM_CU30 USB 2/USB 3 2MP camera module, which is fairly standard and affordable, as well as the more expensive
Hyperyon described above. After testing both, we decided the Hyperon fit our needs better, despite its higher latency,
due to its extremely good low light performance.</p>
<h2 id="field-object-detection">Field object detection</h2>
<p>The primary responsibility of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. 
This is accomplished by using the open-source computer vision library OpenCV (v4.2.0). 
We use a Gstreamer pipeline to decode the camera&rsquo;s MJPEG stream at our target resolution of 1280x720 pixels, which is
read in through an OpenCV <code>VideoCapture</code> object.</p>
<h3 id="pre-processing">Pre-processing</h3>
<p>With the camera frame in OpenCV memory, we then apply the following pre-processing steps to the image:</p>
<ol>
<li>Crop the frame to only leave a rectangle around the mirror visible</li>
<li>Flip the image, if necessary, to account for the mounting of the camera on the robot</li>
<li>Apply a circular mask to the image to mask out any pixels that aren&rsquo;t exactly on the mirror</li>
<li>Downscale the frame for use when processing the blue and yellow goals, since we don&rsquo;t need much accuracy on them and they&rsquo;re large.</li>
<li>(only for the field lines) Mask out the robot from the centre of the mirror (as it has reflective plastic on it which comes up as white)</li>
</ol>
<h3 id="thresholding-and-component-labelling">Thresholding and component labelling</h3>
<p>After pre-processing, we then use OpenCV&rsquo;s <code>parallel_for</code> and <code>inRange</code> functions to threshold three objects at once using
three threads (ideally one free core is left for the localisation). 
This produces a 1-bit binary mask of the image, where each pixel is 255 (true) if it&rsquo;s inside the RGB range specified, 
and 0 (false) if it&rsquo;s not.</p>
<p>For the lines, the vision processing step is finished here as we only need a binary mask. Unfortunately, the localisation
process can&rsquo;t begin until the goals have finished processing due to the localiser relying on the goals in its initial
estimate calculations (see below).</p>
<p>Finally, we use OpenCV&rsquo;s Grana<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> parallel connected component labeller to detect regions of the same colour in the image.
The largest connected region will be the field object we are looking for. 
OpenCV automatically calculates the bounding box and centroid for each of these connected regions.</p>
<h3 id="coordinate-transforms-and-dispatch">Coordinate transforms and dispatch</h3>
<p>Considering the centroid for each field object as a Cartesian vector in pixels coordinates, we convert this vector to
polar form and run the magnitude through our mirror dewarping function (see below) to convert it to a length in 
centimetres. This leaves us with a polar vector for each field object relative to the robot&rsquo;s centre.</p>
<p>We then convert back to Cartesian and use the last localiser position to infer the field object&rsquo;s absolute position in centimetres. </p>
<p>Finally, for the goals we also calculate the relative Cartesian coordinates (convert polar to Cartesian but don&rsquo;t add 
localiser position) for use in the localiser&rsquo;s initial estimate calculations.</p>
<p>This information is encoded into a Protobuf format with nanopb, and is sent over UART to the ESP32 using POSIX termios
at 115200 baud. Unfortunately, the UART bus is owned by an ATMega 32U4, so a sketch on that device forwards it
to the ESP32.</p>
<p><strong>TODO images and/or video of thresholded field</strong></p>
<p><strong>TODO more detail on dewarp function here (maybe move stuff up from localisation section)</strong></p>
<h2 id="localisation">Localisation</h2>
<h3 id="previous-methods-and-background">Previous methods and background</h3>
<p>Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order
to develop advanced strategies and precise movement control, instead of just driving directly towards the ball.</p>
<p>Currently, teams use three main methods of localisation. Firstly, the most common approach uses the detected goal blobs
in the camera to triangulate the robot&rsquo;s position. This approach can be very inaccurate because of the low resolution of
most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes
the goals are not visible (especially in SuperTeam). Using this approach on an OpenMV, we found accuracy of about 15 cm,
but running on Omicam at 720p we found accuracy of around 4-6 cm.</p>
<p>The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using
a few of these sensors on a robot, the position of the robot can be inferred with trigonometry by measuring the distance
to the walls. The drawback of this approach is that it&rsquo;s impossible to reliably distinguish between interfering objects,
such as a hand or another robot, compared to a wall. This means that although this approach can be accurate on an empty
field, it is very difficult to use reliably in actual games. On an empty field, teams have found accuracies of 5-10 cm,
but data is often invalid and can&rsquo;t be used reliably in real games.</p>
<p>The third approach in use by some teams is one based on 360 degree LiDARS. This approach, being similar to the second
approach, has similar accuracy and drawbacks. One additional drawback is the expensive cost and heavy weight of LiDARS.</p>
<p>Some teams, including us in the past, do not localise at all. Strictly speaking, with many gameplay behaviours,
knowing the robot&rsquo;s position is not necessary. However, with our advanced strategies, localisation is a necessity.</p>
<p><strong>TODO cover mouse sensor here too</strong></p>
<p><strong>TODO talk about moving average as well</strong></p>
<h3 id="our-approach">Our approach</h3>
<p>This year, Team Omicron presents a novel approach to robot localisation based on a hybrid
sensor-fusion/non-linear optimisation algorithm.
Our approach builds an initial estimate of the robot&rsquo;s position using faster, more inaccurate methods like goal
triangulation and mouse sensor velocity integration.
It then refines this estimate to a much higher accuracy by solving a 2D non-linear minimisation problem in realtime.
The addition of the optimisation stage to the algorithm increases accuracy by about 4.6x, to be as little as 1.5cm error.</p>
<p><strong>TODO use new paper</strong></p>
<p>The optimisation stage of our sensor fusion algorithm is based on a Middle-Size League paper published in <em>Advanced Robotics</em><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. 
However, there are some differences between their paper and our approach. While they generate line points and then 
use a particle filter (Monte-Carlo localisation), we instead cast rays over the image and solve a non-linear minimisation problem
based on ray distances, rather than point locations. However, both methods generally follow the same approach of sampling 
the line and solving an optimisation problem to figure out the location of the robot. </p>
<p>Our optimisation algorithm works by comparing observed field line geometry from the camera (sampled via raycasting), 
to a known model of the field. By trying to optimise the robot&rsquo;s unknown (x,y) position such that it minimises 
the error between the observed lines and expected lines at each position, we can infer the robot&rsquo;s coordinates to a 
very high level of accuracy.</p>
<p><strong>TODO move this paragraph to drawbacks section?</strong></p>
<p>In theory, this optimisation algorithm can already solve our localisation problem, and we did indeed observe very good
accuracy using idealistic Fusion 360 rendered images such as the one above <strong>(TODO provide picture)</strong>.
However, in the real world, we found the optimiser to be very unstable, because the perspective of our camera&rsquo;s 
mounting obscures very far away field lines that the optimiser needs to converge on a stable solution. 
To solve this issue, we decided to use other lower-accuracy, but more robust estimates of the robot&rsquo;s position to 
&ldquo;give hints&rdquo; to the optimisation algorithm, thus forming a sensor fusion approach.</p>
<p>Our approach has the following 4 main steps:</p>
<ol>
<li>Estimate calculation</li>
<li>Image analysis</li>
<li>Camera normalisation</li>
<li>Coordinate optimisation and interpolation</li>
</ol>
<h3 id="estimate-calculation">Estimate calculation</h3>
<p><strong>Goal maths, mouse sensor, estimate bounds</strong></p>
<h3 id="image-analysis">Image analysis</h3>
<p>The localiser&rsquo;s input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding
for the colour white, which is handled by the vision pipeline described earlier.</p>
<p>With the input provided, a certain number of rays (usually 128) are emitted from the centre of the line image. A ray
terminates when it touches a line, reaches the edge of the image or reaches the edge of the mirror (as it would be a
waste of time to check outside the mirror). The theory of operation behind this is, essentially, for each field position
each ray should have its own unique distance.</p>
<p>Rays are stored as only a length in a regular C array, as we can infer the angle between each ray as: 2pi / n_rays</p>
<p>This step can be summarised as essentially &ldquo;sampling&rdquo; the line around us on the field.</p>
<p><img alt="Raycasting" src="../images/raycasting.png" /> <br />
<em>Figure 1: example of ray casting on field, with a position near to the centre</em></p>
<h3 id="camera-normalisation">Camera normalisation</h3>
<p>These rays are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by
measuring the pixels between points along evenly spaced tape placed on the real field, via Omicontrol. Using regression 
software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances. 
In our case, we simply apply the dewarp function to each ray length instead, leaving us
with each ray essentially in field coordinates (or field lengths) rather than camera coordinates.</p>
<p>This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres.</p>
<p><img alt="Dewarped" src="../images/dewarped.png" />  <br />
<em>Figure 2: example of applying the dewarp function to an entire image, on the low resolution OpenMV H7.</em></p>
<p>The second phase of the camera normalisation is to rotate the rays relative to the robot&rsquo;s heading, using a rotation matrix.
The robot&rsquo;s heading value, which is relative to when it was powered on, is transmitted by the ESP32 (again using Protocol Buffers).
For information about how this value is calculated using the IMU, see the ESP32 and movement code page.</p>
<h3 id="coordinate-optimisation-and-interpolation">Coordinate optimisation and interpolation</h3>
<p>The coordinate optimisation stage is achieved by using the Subplex local derivative-free non-linear optimisation algorithm
<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>, re-implemented as part of the NLopt package<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup>. 
This algorithm essentially acts as an efficiency and stability improvement over the well-known Nelder-Mead Simplex algorithm.</p>
<p><strong>Talk more about numerical optimisation here? compared to solving</strong></p>
<p>Our problem description is as follows: (<strong>TODO not correct</strong>)</p>
<p><img alt="Mathematical description of optimisation problem" src="../images/problem.png" /></p>
<p>The most important part of this process is the <em>objective function</em>, which is a function that takes an N-dimensional vector
(in our case, an estimated 2D position) and calculates essentially a &ldquo;score&rdquo; of how accurate the value is. This
objective function must be highly optimised as it could be evaluated hundreds of times by the optimisation algorithm.</p>
<p><img alt="Objective function" src="../images/objective_function.png" /> <br />
<em>Figure 3: map of objective function for a robot placed at the centre of the field. White pixels indicate high accuracy areas</em>
<em>and black pixels indicate less accurate areas. This takes up to 30 seconds to calculate for all 44,226 positions.</em></p>
<p><img alt="Objective function 3D visualisation" src="../images/field_heightmap3.png" />  <br />
<em>Figure 4: if we treat the value of the objective function as a heightmap, we can also generate a 3D visualisation of it</em></p>
<p>The known geometry of the RoboCup field is encoded into a &ldquo;field file&rdquo;. This is a binary Protcol Buffer file that encodes the 
geometry of any RoboCup field (including SuperTeam) by dividing it into a grid, where each cell is true if on a line, otherwise false. 
Increasing the resolution of the grid will increase its accuracy, but also increase its file size. We use a 1cm grid,
which stores 44,226 cells and is 42 KiB on disk. This takes about 2 seconds to generate on a fast desktop computer, and
is copied across to the LattePanda SBC.
The field file is generated by a Python script which can be easily modified to support an arbitrary number of different 
field layouts, such as SuperTeam or our regional Australian field. </p>
<p><img alt="Field file" src="../images/field_file.png" />  <br />
<em>Figure 3: visualisation of the field data component of the field file. In the original 243x182 image, 1 pixel = 1cm</em></p>
<p>Although a derivative-based algorithm may be more efficient at solving the problem, we could not determine a way to
calculate the derivative of the objective function in a way that was easier than just using a derivative-free
algorithm. Especially considering the low dimensionality of the problem, a derivative-based algorithm wasn&rsquo;t really
required.</p>
<p>With the robot&rsquo;s position now determined to the highest accuracy possible, the final step in the localisation process
is to smooth and interpolate this coordinate. The optimisation algorithm can produce unstable results, especially in
environments where the line is not very visible or mistaken for other objects on the field. This is accomplished through
by using a simple windowed mean. A windowed median was also tried but led to more unstable results. In future, we would
like to investigate more complex digital signal processing such as a low pass filter to use on this value and judge
if it improves the localisation result.</p>
<p><strong>TODO cover dynamic size of moving mean</strong></p>
<h3 id="drawbacks">Drawbacks</h3>
<p>Although a lot of effort has been spent improving the performance of our approach, some issues with it remain:</p>
<ul>
<li>Accuracy can still be improved. Although 1.5cm is good, it&rsquo;s still a margin of error that may make some more precise
movements difficult.</li>
<li>The algorithm can be unstable, especially on internationals fields where lines are harder to see. The initial estimate
stage and optimiser bounds helps to correct this, but a better algorithm for detecting lines (perhaps using a Sobel filter)
should be looked into.</li>
<li>The optimisation stage is still slow, even with only 40 or so objective function evaluations. In the Nelder-Mead simplex
algorithm (which Subplex is based on), the evaluation of the simplex&rsquo;s vertices could be multi-threaded to speed it up if
more CPU cores were available. May also be possible to run this algorithm on the GPU, if we had one.</li>
<li>The algorithm is difficult to debug. Despite programming tools for Omicontrol, it&rsquo;s still difficult to determine exactly
why the algorithm won&rsquo;t localise sometimes, which could be a problem at competitions.</li>
</ul>
<h2 id="extramiscellaneous-features">Extra/miscellaneous features</h2>
<h3 id="interfacing-with-omicontrol">Interfacing with Omicontrol</h3>
<p>To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends
Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as
the temperature of the SBC. Although C isn&rsquo;t an officially supported language by Google for Protocol Buffers, we use 
the mature nanopb library to do the encoding.</p>
<p>We use the SIMD optimised libjpeg-turbo to efficiently encode JPEG frames, so as to not waste performance on the remote 
debugger (which is disabled during competition). Instead of compressing threshold frames with JPEG, because they are 1-bit 
images, it was determined that zlib could compress them more efficiently (around about 460,800x reduction in size compared 
to uncompressed data). </p>
<p>With all these optimisations, even at high framerates (60+ packets per second), the remote debug system uses no more than
1 MB/s of outgoing bandwidth, which is small enough to work reliably over both Ethernet and WiFi.</p>
<h3 id="configuration">Configuration</h3>
<p>Usually, we embed configuration in a &ldquo;defines.h&rdquo; file. Config includes information like the bounding box of the crop
rectangle, the radius of the mirror and the dewarp function.</p>
<p>Because this is embedded in a header, the project would have to be recompiled and relaunched every time a setting is updated
which is not ideal. For Omicam, we used an INI file stored on the SBC&rsquo;s disk that is parsed and loaded on every startup.</p>
<p>In addition, the config file can also be dynamically reloaded by an Omicontrol action, making even relaunching Omicam un-necessary. 
Because of this, we have much more flexibility and faster prototyping abilities when tuning to different venues.</p>
<h3 id="video-recording-and-match-replay">Video recording and match replay</h3>
<p><strong>TODO explain this</strong></p>
<h3 id="debugging-and-performance-optimisation">Debugging and performance optimisation</h3>
<p><strong>TODO we may use gcc not clang so talk about that too</strong></p>
<p>To achieve Omicam&rsquo;s performance, we made heavy use of parallel programming techniques, OpenCV&rsquo;s x86 SIMD CPU optimisations,
Clang optimiser flags as well as image downscaling for the goal threshold images (as they are mostly unused).
In addition, the localisation, vision and remote debug pipelines all run in parallel to each other so the entire application
is asynchronous.</p>
<p>Low-level compiled languages such as C and C++ can be difficult to debug in case of memory corruption bugs. 
In order to improve the stability of Omicam
and fix bugs, we used Google&rsquo;s Address Sanitizer (ASan) and Undefined Behaviour Sanitizer (UBSan) to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. 
In addition, we used the LLVM toolchain&rsquo;s debugger lldb (or just gdb) to analyse the application frequently.</p>
<p>To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application.
Although the Clang compiler may have marginally worse performance than GCC, we chose Clang because it&rsquo;s more modern and has
better debugging support (namely, GCC&rsquo;s Adress Sanitizer implementation is broken for us).</p>
<p>To improve performance of the localiser, we use the last extrapolated position from the mouse sensor as a seed for the initial
position of the next search. This means instead of starting from a random position, the localiser will complete much quickly
a it&rsquo;s already relatively close to the true position.</p>
<p><strong>Also cover Linux CPU optimisation and associated thermal issues if relevant</strong></p>
<h2 id="references">References</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>C. Grana, D. Borghesani, and R. Cucchiara, “Optimized Block-Based Connected Components Labeling With Decision Trees,” IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596–1609, 2010, doi: 10.1109/TIP.2010.2044963.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, “Robust and real-time self-localization based on omnidirectional vision for soccer robots,” Adv. Robot., vol. 27, no. 10, pp. 799–811, Jul. 2013, doi: 10.1080/01691864.2013.785473.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>T. H. Rowan, “Functional stability analysis of numerical algorithms,” Unpuplished Diss., p. 218, 1990.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../electrical_design/" title="Electrical design" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Electrical design
              </span>
            </div>
          </a>
        
        
          <a href="../omicontrol/" title="Omicontrol" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Omicontrol
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright (c) 2020 Team Omicron.
          </div>
        
        powered by
        <a href="https://www.mkdocs.org" target="_blank" rel="noopener">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.c33a9706.js"></script>
      
      <script>app.initialize({version:"1.1",url:{base:".."}})</script>
      
    
  </body>
</html>