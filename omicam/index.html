
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation website for Team Omicron, an open-source RoboCup Jr Open Soccer team from Brisbane Boys' College.">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.1.7">
    
    
      
        <title>Omicam - Team Omicron</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.19753c6b.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.196e0c26.min.css">
        
      
    
    
    
      
        
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
      
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#background-and-previous-methods" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="Team Omicron" class="md-header-nav__button md-logo" aria-label="Team Omicron">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Team Omicron
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Omicam
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Team Omicron" class="md-nav__button md-logo" aria-label="Team Omicron">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Team Omicron
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../about/" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" >
    <label class="md-nav__link" for="nav-3">
      Hardware
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Hardware" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon"></span>
        Hardware
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../structural_design/" class="md-nav__link">
      Structural design
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../electrical_design/" class="md-nav__link">
      Electrical design
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../sbc/" class="md-nav__link">
      SBC & Camera
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    <label class="md-nav__link" for="nav-4">
      Software
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Software" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon"></span>
        Software
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../open_source_release/" class="md-nav__link">
      Our open-source release!
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Omicam
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" class="md-nav__link md-nav__link--active">
      Omicam
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background-and-previous-methods" class="md-nav__link">
    Background and previous methods
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-and-results" class="md-nav__link">
    Performance and results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    Hardware
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#field-object-detection" class="md-nav__link">
    Field object detection
  </a>
  
    <nav class="md-nav" aria-label="Field object detection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pre-processing" class="md-nav__link">
    Pre-processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thresholding-and-component-labelling" class="md-nav__link">
    Thresholding and component labelling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordinate-transforms-and-dispatch" class="md-nav__link">
    Coordinate transforms and dispatch
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#localisation" class="md-nav__link">
    Localisation
  </a>
  
    <nav class="md-nav" aria-label="Localisation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#previous-methods-and-background" class="md-nav__link">
    Previous methods and background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-approach" class="md-nav__link">
    Our approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimate-calculation" class="md-nav__link">
    Estimate calculation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-analysis" class="md-nav__link">
    Image analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-normalisation" class="md-nav__link">
    Camera normalisation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordinate-optimisation-and-interpolation" class="md-nav__link">
    Coordinate optimisation and interpolation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#robot-detection" class="md-nav__link">
    Robot detection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#limitations-and-refinements" class="md-nav__link">
    Limitations and refinements
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#extramiscellaneous-features" class="md-nav__link">
    Extra/miscellaneous features
  </a>
  
    <nav class="md-nav" aria-label="Extra/miscellaneous features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interfacing-with-omicontrol" class="md-nav__link">
    Interfacing with Omicontrol
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    Configuration
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-recording" class="md-nav__link">
    Video recording
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#replay-system" class="md-nav__link">
    Replay system
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#debugging-and-diagnostics" class="md-nav__link">
    Debugging and diagnostics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../omicontrol/" class="md-nav__link">
      Omicontrol
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../fsm/" class="md-nav__link">
      Finite State Machine
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../strategy/" class="md-nav__link">
      Game strategies
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../protobuf/" class="md-nav__link">
      Protocol Buffers
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../communication/" class="md-nav__link">
      Communication
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../lowlevel/" class="md-nav__link">
      Low-level code
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../open_source/" class="md-nav__link">
      Third party library notices
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../future_work/" class="md-nav__link">
      Future ideas
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#background-and-previous-methods" class="md-nav__link">
    Background and previous methods
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-and-results" class="md-nav__link">
    Performance and results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    Hardware
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#field-object-detection" class="md-nav__link">
    Field object detection
  </a>
  
    <nav class="md-nav" aria-label="Field object detection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pre-processing" class="md-nav__link">
    Pre-processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thresholding-and-component-labelling" class="md-nav__link">
    Thresholding and component labelling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordinate-transforms-and-dispatch" class="md-nav__link">
    Coordinate transforms and dispatch
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#localisation" class="md-nav__link">
    Localisation
  </a>
  
    <nav class="md-nav" aria-label="Localisation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#previous-methods-and-background" class="md-nav__link">
    Previous methods and background
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-approach" class="md-nav__link">
    Our approach
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimate-calculation" class="md-nav__link">
    Estimate calculation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-analysis" class="md-nav__link">
    Image analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#camera-normalisation" class="md-nav__link">
    Camera normalisation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coordinate-optimisation-and-interpolation" class="md-nav__link">
    Coordinate optimisation and interpolation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#robot-detection" class="md-nav__link">
    Robot detection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#limitations-and-refinements" class="md-nav__link">
    Limitations and refinements
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#extramiscellaneous-features" class="md-nav__link">
    Extra/miscellaneous features
  </a>
  
    <nav class="md-nav" aria-label="Extra/miscellaneous features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interfacing-with-omicontrol" class="md-nav__link">
    Interfacing with Omicontrol
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    Configuration
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-recording" class="md-nav__link">
    Video recording
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#replay-system" class="md-nav__link">
    Replay system
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#debugging-and-diagnostics" class="md-nav__link">
    Debugging and diagnostics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Omicam</h1>
                
                <p><img alt="Omicam logo" src="../images/omicam_logo_dark.png" />    </p>
<p><em>Page author: Matt Young, vision systems &amp; low-level developer</em></p>
<p>One of the biggest innovation Team Omicron brings this year is our high-performance, custom vision and localisation
application (i.e. camera software) called <em>Omicam</em>. Omicam handles the complex process of detecting the ball and goals,
determining the robot&rsquo;s position on the field as well as encoding/transmitting this information, all in one highly
optimised codebase. It uses a novel method involving non-linear optimisation to calculate the robot&rsquo;s position.</p>
<p>The application is developed mostly in C, with some C++ code to interface with OpenCV. 
It runs on the LattePanda Delta 432 single board computer and uses Linux (Xubuntu 18.04) as its OS.</p>
<p>We are proud to report that Omicam is a significant improvement compared to previous vision applications in use at BBC
Robotics, as explained in the &ldquo;Performance and results&rdquo; section.</p>
<h2 id="background-and-previous-methods">Background and previous methods</h2>
<p>Intelligent, accurate and fast computer vision continues to become increasingly important in RoboCup Jr Open Soccer.
With the introduction of the orange ball and advanced teams&rsquo; application of &ldquo;ball-hiding&rdquo; strategies, high resolution
yet performant computer vision is now one of the most important elements of a successful RoboCup Jr Open robot. Advanced
teams now also need to accurately estimate their position on the field (localise) in order to execute strategies and
gain the upper hand in the competition.</p>
<p>Previously, our team used an OpenMV H7 to provide vision. This is a module which uses an STM32 MCU combined combined
with an OmniVision camera module to provide low-resolution vision in an easy-to-use MicroPython environment. However,
although this approach is functional, its resolution and framerate are extremely limiting for our use case. Hence, we
decided the best solution was to develop a custom vision application running on a <a href="../sbc/">single board computer (SBC)</a>. 
In terms of localisation, in the past we managed to get away with not using any, or using a low-fidelity approach based
on detecting the goals in the image. With the introduction of strategies this year, however, team members needed more
accurate position data, which requires more complex localisation.</p>
<h2 id="performance-and-results">Performance and results</h2>
<p>Omicam is capable of detecting the ball, goals and lines at 60-70fps at 720p (1280x720) resolution. 
Compared to the previous OpenMV H7, this is 23x higher resolution at 3x the framerate.<sup>1</sup></p>
<p>In addition, using the novel vision-based localisation algorithm we developed this year, we can now determine the
robot&rsquo;s position to around 1.5cm accuracy in less than 2ms. This is over 5x/25x more accurate<sup>2</sup>
than any previous methods used at BBC Robotics, most likely much faster, and has been shown to be much more reliable and 
stable in all conditions.</p>
<p>Using the e-con Systems Hyperyon camera based around the ultra low-light performance IMX290 sensor, Omicam
is robust against lighting conditions ranging from near pitch darkness to direct LED light.</p>
<p><em><sup>1: previous results based on mediocre lighting conditions running well optimised OpenMV H7 code at QVGA resolution.</sup></em> </p>
<p><em><sup>2: depending on whether LRF based/goal based localisation was used.</sup></em></p>
<h2 id="hardware">Hardware</h2>
<p>Omicam is capable of running on any x86 computer running Linux. In our case, we use a LattePanda Delta 432 SBC with a 
2.4 GHz quad-core Intel Celeron N4100 and 4GB of RAM. All computer vision tasks run on the CPU, we do not use the GPU.</p>
<p>The camera module we use is the e-con Systems Hyperon which uses the IMX290 ultra-low light image sensor.</p>
<p>For more information, please see <a href="../sbc/">this page</a>.</p>
<h2 id="field-object-detection">Field object detection</h2>
<p>One of the responsibilities of Omicam is to detect the bounding box and centroid of field objects: the ball, goals and also lines. 
This is accomplished by using the open-source computer vision library OpenCV. 
We use a Gstreamer pipeline to decode our camera&rsquo;s MJPEG stream at our target resolution of 1280x720 pixels, which is
read in through an OpenCV <code>VideoCapture</code> object.</p>
<p><img alt="Raycasting" src="../images/camera_original.jpg" /> <br />
<em>Figure 1: image captured directly from Omicam, showing how it views the field (note: downscaled for docs)</em></p>
<h3 id="pre-processing">Pre-processing</h3>
<p>With the camera frame in OpenCV memory, we then apply the following pre-processing steps to the image:</p>
<ol>
<li>Crop the frame to only leave a rectangle around the mirror visible</li>
<li>Flip the image, if necessary, to account for the mounting of the camera on the robot</li>
<li>Apply a circular mask to the image to mask out any pixels that aren&rsquo;t on the mirror</li>
<li>Downscale the frame for use when processing the blue and yellow goals to process faster at the cost of accuracy</li>
<li>Mask out the robot from the centre of the mirror (as it has reflective plastic on it which comes up as white)</li>
</ol>
<h3 id="thresholding-and-component-labelling">Thresholding and component labelling</h3>
<p>After pre-processing, we then use OpenCV&rsquo;s <code>inRange</code> inside a <code>parallel_for</code> to threshold all objects at once using
multiple threads. 
This produces a 1-bit binary mask of the image, where each pixel is 255 (true) if it&rsquo;s inside the RGB range specified, 
and 0 (false) if it&rsquo;s not.</p>
<p>For the lines, the vision processing step is finished here as we only need a binary mask. Unfortunately, the localisation
process can&rsquo;t begin until the goals have finished processing due to the localiser relying on the goals in its initial
estimate calculations (see below).</p>
<p>Finally, we use OpenCV&rsquo;s Grana<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> parallel connected component labeller to detect regions of the same colour in the image.
The largest connected region will be the field object we are looking for. 
OpenCV automatically calculates the bounding box and centroid for each of these connected regions.</p>
<h3 id="coordinate-transforms-and-dispatch">Coordinate transforms and dispatch</h3>
<p>Considering the centroid for each field object as a Cartesian vector in pixels coordinates, we convert this vector to
polar form and run the magnitude through our mirror dewarping function (see below) to convert it to a length in 
centimetres. This leaves us with a polar vector for each field object relative to the robot&rsquo;s centre.</p>
<p>We then convert back to Cartesian and use the last localiser position to infer the field object&rsquo;s absolute position in centimetres. </p>
<p>Finally, for the goals we also calculate the relative Cartesian coordinates (convert polar to Cartesian but don&rsquo;t add 
localiser position) for use in the localiser&rsquo;s initial estimate calculations.</p>
<p>This information is encoded into a Protobuf format with nanopb, and is sent over UART to the ESP32 using POSIX termios
at 115200 baud. Unfortunately, the UART bus is owned by an ATMega 32U4, so a sketch on that device forwards it
to the ESP32.</p>
<p><strong>TODO images and/or video of thresholded field</strong></p>
<p><strong>TODO more detail on dewarp function here (maybe move stuff up from localisation section)</strong></p>
<h2 id="localisation">Localisation</h2>
<h3 id="previous-methods-and-background">Previous methods and background</h3>
<p>Localisation is the problem of detecting where the robot is on the field. This information is essential to know in order
to develop advanced strategies and precise movement control, instead of just driving directly towards the ball.</p>
<p>Currently, teams use three main methods of localisation. Firstly, the most common approach uses the detected goal blobs
in the camera to triangulate the robot&rsquo;s position. This approach can be very inaccurate because of the low resolution of
most cameras (such as the OpenMV), the fact that there are only two goals to work with as well as the fact that sometimes
the goals are not visible (especially in SuperTeam). Using this approach on an OpenMV, we found accuracy of about 15 cm,
but running on Omicam at 720p we found accuracy of around 4-6 cm.</p>
<p>The second approach in use is based on distance sensors such laser range finders (LRFs) or ultrasonic sensors. By using
a few of these sensors on a robot, the position of the robot can be inferred with trigonometry by measuring the distance
to the walls. The drawback of this approach is that it&rsquo;s impossible to reliably distinguish between interfering objects,
such as a hand or another robot, compared to a wall. This means that although this approach can be accurate on an empty
field, it is very difficult to use reliably in actual games. On an empty field, teams have found accuracies of 5-10 cm,
but data is often invalid and can&rsquo;t be used reliably in real games.</p>
<p>The third approach in use by some teams is one based on 360 degree LiDARS. This approach, being similar to the second
approach, has similar accuracy and drawbacks. One additional drawback is the expensive cost and heavy weight of LiDARS.</p>
<p>Another approach that can be used is summing displacement values from a mouse sensor, or integrating acceleration data
from an IMU. While these approaches have high levels of accuracy initially, the repeated integration of the data 
gradually builds error over time, and if the robot is on for a particularly long time, the localised position may be
several centimetres inaccurate by the end. In addition, these approaches all return relative displacement to the 
initial position, not an absolute position on the field - meaning a method for calculating the robot&rsquo;s initial
position accurately is still required.</p>
<p>Some teams, including us in the past, do not localise at all. Strictly speaking, with many gameplay behaviours,
knowing the robot&rsquo;s position is not necessary. However, with our advanced strategies, localisation is a necessity.</p>
<h3 id="our-approach">Our approach</h3>
<p>This year, Team Omicron presents a novel approach to robot localisation based on a hybrid sensor-fusion/non-linear
optimisation algorithm. Our approach builds an initial estimate of the robot&rsquo;s position using faster, more inaccurate
methods like goal triangulation and mouse sensor displacement. It then refines this estimate to a much higher accuracy
by solving a 2D non-linear optimisation problem in realtime. The addition of the optimisation stage to the algorithm
increases accuracy by about 4.6x, to be as little as 1.5cm error.</p>
<p>Our optimisation algorithm works by comparing observed field line geometry from the camera (sampled via raycasting), 
to a known model of the field. By trying to optimise the robot&rsquo;s unknown (x,y) position such that it minimises 
the error between the observed lines and expected lines at each position, we can infer the robot&rsquo;s coordinates to a 
very high level of accuracy.</p>
<p>The optimisation stage of our sensor fusion algorithm is based on a paper by Lauer, Lange and Redmiller (2006) <sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>,
as covered in the paper by Lu, Li, Hu and Zheng (2013) <sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>.</p>
<p>However, our approaches are different in certain areas, with us making some concessions and improvements to better suit
the RoboCup Junior league (as the paper was written for middle-size league). Lauer et al&rsquo;s approach casts rays and
marks points where these rays intersect the lines, whereas we work with these rays and their lengths directly. Their
approach also calculates the orientation of the robot, while we trust the value of the IMU for orientation to simplify
our algorithm. Their optimisation stage uses the RPROP algorithm to minimise error, while we use Subplex (based on
the Nelder-Mead simplex algorithm). Finally, Lauer et al&rsquo;s approach has more complex global localisation and smoothing
that can handle unstable localisations with multiple local minima, while our approach assumes only one minimum is
present in the objective function.</p>
<p>Our approach has the following 5 main steps:</p>
<ol>
<li>Estimate calculation</li>
<li>Image analysis</li>
<li>Camera normalisation</li>
<li>Coordinate optimisation and interpolation</li>
<li>(Optional) Robot detection</li>
</ol>
<h3 id="estimate-calculation">Estimate calculation</h3>
<p>The localiser first constructs an initial estimate of the robot&rsquo;s position using faster but more inaccurate methods.
The preferred method is to sum the displacement data from our PWM3360 mouse sensor to determine our current position 
since last localisation. However, in certain cases such as the initial localisation after the robot is powered on,
we also use the blue and yellow goals to triangulate our position since there&rsquo;s no initial position for the mouse 
sensor to use. Once we have an initial localisation though, we can go back to using the mouse sensor data.</p>
<p><img alt="Method for calculating position using goals" src="../images/goal_maths.png" />  <br />
<em>Figure 2: method for triangulating position using goals coordinates and vectors</em></p>
<p>Once the initial estimate is calculated, which is usually accurate to about 6-10cm, we constrain the optimiser&rsquo;s bounds
to a certain sized rectangle around the centre of the estimated position. We also change the optimiser&rsquo;s initial
estimate position to the estimated position, and scale down its step size significantly. We assume that the estimate
is close to the actual position, so a smaller step size means that it won&rsquo;t overshoot the minimum and will hopefully
converge faster. The combination of the estimate bounds with changing localiser settings means that we can often
stably converge on the real position of the robot with around 40 objective function evaluations.</p>
<p>If the localiser isn&rsquo;t constrained to the initial estimate bounds, especially on the internationals field we found it
could be very unstable and would often get stuck in nonsensical positions like the corners of the field or inside
the goals. Constraining the optimiser to the estimate bounds is very helpful to reduce these problems.</p>
<h3 id="image-analysis">Image analysis</h3>
<p>The localiser&rsquo;s input is a 1-bit mask of pixels that are determined to be on field lines. This is determined by thresholding
for the colour white, which is handled by the vision pipeline described earlier.</p>
<p>With the input provided, a certain number of rays (usually 64) are emitted from the centre of the line image. A ray
terminates when it touches a line, leaves the image frame or reaches the edge of the mirror (as it would be a
waste of time to check outside the mirror). Currently we use simple trigonometry to project this ray, although we could
use the Bresenham line algorithm for more performance.</p>
<p>Rays are stored as only a length in a regular C array, as we can infer the angle between each ray as: 2pi / n_rays</p>
<p>This step can be summarised as essentially &ldquo;sampling&rdquo; the line around us on the field.</p>
<p><img alt="Raycasting" src="../images/raycasting.png" /> <br />
<em>Figure 3: example of ray casting on field, with a position near to the centre</em></p>
<h3 id="camera-normalisation">Camera normalisation</h3>
<p>These rays are then dewarped to counter the distortion of the 360 degree mirror. The equation to do so is determined by
measuring the pixels between points along evenly spaced tape placed on the real field, via Omicontrol. Using regression 
software such as Excel or Desmos, an equation can then be calculated to map pixel distances to real distances. 
In our case, we simply apply the dewarp function to each ray length instead, leaving us
with each ray essentially in field coordinates (or field lengths) rather than camera coordinates.</p>
<p>This dewarping equation is also used by the vision pipeline to determine the distance to the ball and goals in centimetres.</p>
<p><img alt="Dewarped" src="../images/dewarped.png" />  <br />
<em>Figure 4: results of applying the dewarp function to the entire image in Figure 1.</em></p>
<p>The second phase of the camera normalisation is to rotate the rays relative to the robot&rsquo;s heading, using a rotation matrix.
The robot&rsquo;s heading value, which is relative to when it was powered on, is transmitted by the ESP32 (again using Protocol Buffers).
For information about how this value is calculated using the IMU, see the ESP32 and movement code page.</p>
<h3 id="coordinate-optimisation-and-interpolation">Coordinate optimisation and interpolation</h3>
<p>The coordinate optimisation stage is achieved by using the Subplex local derivative-free non-linear optimisation
algorithm <sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup>, implemented as part of the NLopt package<sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup>. This algorithm essentially acts as an efficiency and
stability improvement over the well-known Nelder-Mead Simplex algorithm. Subplex is capable of quickly solving systems
that have no calculable derivatives with hundreds of variables, so in our case solving a two dimensional problem is
trivial for it. Due to the fact that our system is overdetermined, it is not easy to solve and hence why we have to
generate an approximate solution using an optimisation algorithm like Subplex (although, as mentioned later, it
should be possible to generate a least squares solution).</p>
<p>Our problem description is as follows: (<strong>TODO not correct</strong>)</p>
<p><img alt="Mathematical description of optimisation problem" src="../images/problem.png" /></p>
<p>The most important part of this process is the <em>objective function</em>, which is a function that takes an N-dimensional
vector (in our case, an estimated 2D position) and calculates a &ldquo;score&rdquo; of how accurate the value is.</p>
<p><img alt="Objective function" src="../images/objective_function.png" /> <br />
<em>Figure 5: map of objective function for a robot placed at the centre of the field. White pixels indicate high accuracy areas</em>
<em>and black pixels indicate less accurate areas. This takes up to 30 seconds to calculate for all 44,226 positions (this shows</em>
<em>why a brute force search would not work in competition).</em></p>
<p><img alt="Objective function 3D visualisation" src="../images/field_heightmap3.png" />  <br />
<em>Figure 6: another visualisation of the objective function, instead shown by treating it as a heightmap in Blender.</em></p>
<p>The known line layout/geometry of the RoboCup field is encoded into a &ldquo;field file&rdquo;. This is a binary Protocol Buffer
file written to disk, and is essentially a bitmap where 1 pixel = 1cm, white if line and black if no line. The reason
for recording it as a Protobuf instead of a bitmap is because the format is theoretically capable of storing grids of
any resolution, such as 0.5cm if more accuracy was desired. We use a 1cm grid, which stores 44,226 cells and is 42 KiB
on disk. This takes about 2 seconds to generate on a fast desktop computer using a Python script that can be modified
to support any field geometry such as SuperTeam or our regional Australian field.</p>
<p>Unfortunately, due to the expensive raycasting, calculating this objective function is very expensive. However, with
smart use of caching and memoisation, we were able to achieve enormous performance improvements (in the magnitude of
something like 100x faster). This works by having Omicam store the results of each set of raycasts it does to the static
field file to an in-memory cache. Since the field file never changes, when the optimisation algorithm needs to look up
that grid cell in future, it can simply pull the results from the cache instead of needlessly recalculating them. This
means the only raycasting we do is on the incoming camera frame, saving a heap of time. Since the robot is often still,
we have extremely low cache miss rates most of the time. In theory, it would also be possible to generate this in-memory
cache completely offline for more performance, although we currently do not.</p>
<p><img alt="Field file" src="../images/field_file.png" />  <br />
<em>Figure 7: visualisation of the field data component of the field file. In the original 243x182 image, 1 pixel = 1cm</em></p>
<p>In theory, our optimisation problem could actually be represented as a non-linear least squares problem (rather than
a non-linear derivative free problem), and could thus be solved using standard algorithms like the Gauss-Newton or
Levenberg-Marquardt algorithms. However, due to a lack of mathematical knowledge I decided to just use the simpler
derivative-free algorithm instead.</p>
<p>With the robot&rsquo;s position now determined to the highest accuracy possible, the final step in the localisation process
is to smooth and interpolate this coordinate. The optimisation algorithm can produce unstable results, especially in
environments where the line is not very visible or mistaken for other objects on the field. Smoothing is currently 
accomplished by using a simple moving average. A moving median was also tried but led to more unstable results. 
In future, we would like to investigate more complex digital signal processing such as a low-pass filter or Kalman filter
to improve our accuracy.</p>
<h3 id="robot-detection">Robot detection</h3>
<p>We also experimented with a novel robot detection algorithm, although it was not completed in time for release.
Initially testing shows it could be pretty accurate, especially with LiDAR instead of video.</p>
<p>The algorithm works under the assumption that since most robots are white, they will in turn be picked up by our line
detector. We can analyse all the lines we detect and detect outliers, or &ldquo;suspicious&rdquo; rays, which will belong to robots.
We do this using the standard method of checking if a ray lies 1.5x outside the interquartile range (IQR). Once that is
done, we cluster groups of suspicious rays one after the other into unbroken blobs that are robots. With this, we
essentially have a triangle of where the robot could be in (the back face of the triangle is set to be along the nearest
line). To check if a robot does in fact lie in this triangle, we check to see if a circle the size of the largest legal
robot diameter could fit into the triangle. If the circle fits anywhere, we have a detection - otherwise, it&rsquo;s a false
positive. Although it&rsquo;s impossible to know exactly where the robot is located in this triangle, we assume that it will
be located the closest possible distance to the robot without clipping outside the bounds (we essentially &ldquo;inscribe&rdquo;
the circle within the triangle this way). With that done, we have successfully detected the (x,y) position of a potential
opposition robot!</p>
<p><strong>TODO image</strong></p>
<p>This algorithm has its limitations, because it makes many assumptions that are not necessarily true such as robots being
white or fitting in the inscribed circle. Nonetheless, we believe it would be useful enough in competition to be another
helpful feature of Omicam.</p>
<h3 id="limitations-and-refinements">Limitations and refinements</h3>
<p>Although a lot of effort has been spent improving the performance of our approach, some issues with it remain:</p>
<ul>
<li>Accuracy can still be improved. Although 1.5cm is good, it&rsquo;s still a margin of error that may make some more precise
movements difficult.</li>
<li>The algorithm can be unstable, especially on internationals fields where lines are harder to see. The initial estimate
stage and optimiser bounds helps to correct this, but a better algorithm for detecting lines (perhaps using a Sobel filter)
should be looked into.</li>
<li>On a related note, the optimiser probably needs too many lines visible to generate a stable position. In fact, this
limitation is what led us to develop the hybrid sensor-fusion/optimisation algorithm instead of just optimising
directly. In the real world, just optimising was too unstable when lines weren&rsquo;t visible, compared to idealistic
simulation renders.</li>
<li>The optimisation stage could be solved more intelligently by representing it as a least-squares problem instead of
a derivative-free problem, as mentioned above.</li>
<li>The algorithm is difficult to debug. Despite programming tools for Omicontrol, it&rsquo;s still difficult to determine exactly
why the algorithm won&rsquo;t localise sometimes, which could be a problem at competitions. Although this is expected from a 
complex approach, more debugging tools should be programmed to assist with this.</li>
<li>To generate a better final position, a low pass filter or even Kalman filter could be used instead of a simple moving
average.</li>
</ul>
<h2 id="extramiscellaneous-features">Extra/miscellaneous features</h2>
<p>Omicam also has many additional features outside of vision!</p>
<h3 id="interfacing-with-omicontrol">Interfacing with Omicontrol</h3>
<p>To interface with our remote management application Omicontrol, Omicam starts a TCP server on port 42708. This server sends
Protocol Buffer packets containing JPEG encoded frames, zlib compressed threshold data as well as other information such as
the temperature of the SBC. The TCP listen thread receives messages from the Omicontrol client, decodes them and executes
the specified action ID (for example, save data to disk) if one is specified. The TCP send thread receives data from
the vision and localisation pipelines, and encodes them an sends them over network.</p>
<p>One important feature of the Omicam and Omicontrol connection is its ability to operate at high framerates, while not
using a ridiculous amount of bandwidth (even though it&rsquo;s meant to be connected locally, we wanted to keep bandwidth
under 2 MB/s). To achieve this, we use the SIMD optimised <code>libjpeg-turbo</code> library to efficiently encode camera frames to
JPEG, so as to not waste performance on the remote debugger (which is disabled during competition). Instead of
compressing threshold frames with JPEG, because they are 1-bit masks, we found that zlib could compress them
more efficiently (around about 460,800x reduction in size compared to uncompressed data, likely due to this being a 
great use case for zlib&rsquo;s run-length encoding). </p>
<h3 id="configuration">Configuration</h3>
<p>Usually, we embed configuration in a &ldquo;defines.h&rdquo; file. Config includes information like the bounding box of the crop
rectangle, the radius of the mirror and the dewarp function.</p>
<p>Because this is embedded in a header, the project would have to be recompiled and relaunched every time a setting is
updated which is not ideal. For Omicam, we used an INI file stored on the SBCs disk that is parsed and loaded on every
startup. In addition, the config file can also be dynamically reloaded by an Omicontrol command, making even relaunching
Omicam un-necessary. Because of this, we have much more flexibility and faster prototyping abilities when tuning to
different venues.</p>
<h3 id="video-recording">Video recording</h3>
<p>Omicam can record its vision output to disk as an MP4 file. When this is enabled in the configuration, a thread is started
that writes video frames to an OpenCV <code>VideoWriter</code> at the correct framerate to produce a stable 30 fps video. These
generated videos can be loaded as Omicam input (instead of using a live camera), which is useful for offline testing.
In this way, games can be recorded for later analysis of Omicam&rsquo;s performance and accuracy, or for entertainment value.</p>
<h3 id="replay-system">Replay system</h3>
<p>Similar to the above, we also added a &ldquo;replay&rdquo; system (although it wasn&rsquo;t fully finished). When enabled, Omicam records
the data it calculates about the robot&rsquo;s position and orientation to a binary Protobuf file on disk at a rate of 30 Hz.
This format is compact and fault tolerant. The file is periodically re-written every 5 seconds (in case of the app crashing),
and we roughly estimate it should be able to encode ~45 minutes of gameplay data in only 16MB. 
See our <a href="../omicontrol/">Omicontrol</a> section for more details on how this would have looked.</p>
<h3 id="debugging-and-diagnostics">Debugging and diagnostics</h3>
<p>Low-level compiled languages such as C and C++ can be difficult to debug when memory corruption or undefined behaviour
issues occur. In addition, many latent bugs can go undetected in code written in these languages. In order to improve
the stability of Omicam and fix bugs, we used Google&rsquo;s Address Sanitizer (ASan) and Undefined Behaviour Sanitizer
(UBSan) to easily find and trace a variety of bugs such as buffer overflows, memory leaks and more. This (usually) works
in parallel with gdb.</p>
<p>To assist in performance evaluation, we used the Linux tool OProfile to determine the slowest method calls in the application.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This year, Team Omicron presents a high-performance, custom vision and localisation application called Omicam. It is 
written mainly in C, and runs on a LattePanda Delta 432. It is capable of processing vision at orders of magnitude faster
than our previous solution, the OpenMV H7. We also present a novel approach to robot localisation, based on sensor-fusion
and 2D non-linear optimisation. This approach is significantly more accurate and robust than previous methods. Finally,
we introduce support for communication to our visualisation and management application, Omicontrol (covered separately)
using Protocol Buffers. In addition to all this, we also include many interesting features (though, untested) features
such as replays, dynamic config reloading and video recording.</p>
<p>Thank you for reading this! If you have any questions, please don&rsquo;t hesitate to contact us.</p>
<h2 id="references">References</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>C. Grana, D. Borghesani, and R. Cucchiara, “Optimized Block-Based Connected Components Labeling With Decision Trees,” IEEE Trans. Image Process., vol. 19, no. 6, pp. 1596–1609, 2010, doi: 10.1109/TIP.2010.2044963.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Lauer, M., Lange, S. and Riedmiller, M., 2006. Calculating the Perfect Match: An Efficient and Accurate Approach for Robot Self-localization. RoboCup 2005: Robot Soccer World Cup IX, pp.142-153.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>H. Lu, X. Li, H. Zhang, M. Hu, and Z. Zheng, “Robust and real-time self-localization based on omnidirectional vision for soccer robots,” Adv. Robot., vol. 27, no. 10, pp. 799–811, Jul. 2013, doi: 10.1080/01691864.2013.785473.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>T. H. Rowan, “Functional stability analysis of numerical algorithms,” Unpuplished Diss., p. 218, 1990.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>Steven G. Johnson, The NLopt nonlinear-optimization package, http://github.com/stevengj/nlopt&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
</ol>
</div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../open_source_release/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Our open-source release!
              </div>
            </div>
          </a>
        
        
          <a href="../omicontrol/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Omicontrol
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright (c) 2020 Team Omicron. Documentation is under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.0ac82a11.min.js"></script>
      <script src="../assets/javascripts/bundle.f81dfb4d.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>